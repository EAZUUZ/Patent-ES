{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We want to import patent_people\n",
    "Drop applicant_authority\n",
    "If applicant_organization empty but assignee_organization\n",
    "has something, then copy assignee_organization\n",
    "over to applicant_organization\n",
    "Change applicant_organization to patent_applicant\n",
    "Now if patent_applicant is empty, then copy from applicant_full_name. to patent_applicant.\n",
    "\n",
    "Then save to new csv patent_applicant with the cols patent_id, patent_applicant,\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "folder = \"/home/eadlzarabi/Desktop/datasets/Patents/\"\n",
    "file = 'patent_people.csv'\n",
    "output_file = folder + \"processed_patent_applicants.csv\"\n",
    "\n",
    "# Columns to keep\n",
    "cols_to_keep = ['patent_id', 'applicant_organization', 'assignee_organization', 'applicant_full_name']\n",
    "\n",
    "# Process in chunks for large files\n",
    "chunksize = 100000  # Adjust based on your system's memory capacity\n",
    "first_chunk = True\n",
    "\n",
    "for chunk in pd.read_csv(folder + file, chunksize=chunksize, sep=',', engine='python', usecols=cols_to_keep):\n",
    "    # Fill empty applicant_organization with assignee_organization\n",
    "    chunk['applicant_organization'].fillna(chunk['assignee_organization'], inplace=True)\n",
    "    \n",
    "    # Rename to patent_applicant\n",
    "    chunk.rename(columns={'applicant_organization': 'patent_applicant'}, inplace=True)\n",
    "    \n",
    "    # Fill empty patent_applicant with applicant_full_name\n",
    "    chunk['patent_applicant'].fillna(chunk['applicant_full_name'], inplace=True)\n",
    "    \n",
    "    # Keep only required columns\n",
    "    chunk = chunk[['patent_id', 'patent_applicant']]\n",
    "    \n",
    "    # Remove duplicates based on 'patent_id'\n",
    "    chunk.drop_duplicates(subset='patent_id', keep='first', inplace=True)\n",
    "    \n",
    "    # Append to CSV file\n",
    "    chunk.to_csv(output_file, mode='w' if first_chunk else 'a', index=False, header=first_chunk)\n",
    "    first_chunk = False\n",
    "    print(f\"Processed and saved a chunk with {len(chunk)} records.\")\n",
    "\n",
    "print(f\"Processed file saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Now I want to load patent_data.csv to extract the fields: patent_id, patent_date, patent_abstract\n",
    "Also, extract cpc_section and cpc_class and patent_id from the patent_classes.csv\n",
    "Then all 3 files should be merged together\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # File paths\n",
    "# folder = \"/home/eadlzarabi/Desktop/datasets/Patents/\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        folder = \"/home/eadlzarabi/Desktop/datasets/Patents/\"\n",
    "# patent_data_file = 'patent_data.csv'\n",
    "# patent_classes_file = 'patent_classes.csv'\n",
    "\n",
    "# # Step 1: Function to process each chunk of patent_data\n",
    "# def process_chunk(patent_data_chunk, patent_classes_df):\n",
    "#     # Ensure both 'patent_id' columns are of type str for merging\n",
    "#     patent_data_chunk['patent_id'] = patent_data_chunk['patent_id'].astype(str)\n",
    "#     patent_classes_df['patent_id'] = patent_classes_df['patent_id'].astype(str)\n",
    "    \n",
    "#     # Merge the chunk with the classes DataFrame on 'patent_id'\n",
    "#     merged_chunk = pd.merge(patent_data_chunk, patent_classes_df, on='patent_id', how='inner')\n",
    "    \n",
    "#     return merged_chunk\n",
    "\n",
    "# # Step 2: Load patent_classes.csv once (since it's relatively smaller)\n",
    "# patent_classes_df = pd.read_csv(folder + patent_classes_file)\n",
    "# patent_classes_df = patent_classes_df[['patent_id', 'cpc_section', 'cpc_class']]\n",
    "\n",
    "# # Step 3: Initialize an empty list to store the results\n",
    "# all_chunks = []\n",
    "\n",
    "# # Step 4: Process the patent_data.csv in chunks\n",
    "# chunksize = 100000  # Adjust the chunk size based on your system's memory capacity\n",
    "# patent_data_iter = pd.read_csv(folder + patent_data_file, chunksize=chunksize)\n",
    "\n",
    "# # Step 5: Process each chunk\n",
    "# for chunk in patent_data_iter:\n",
    "#     # Extract the necessary columns from the patent_data chunk\n",
    "#     patent_data_chunk = chunk[['patent_id', 'patent_date', 'patent_abstract']]\n",
    "    \n",
    "#     # Process and merge the chunk with patent_classes\n",
    "#     merged_chunk = process_chunk(patent_data_chunk, patent_classes_df)\n",
    "    \n",
    "#     # Add the processed chunk to the list\n",
    "#     all_chunks.append(merged_chunk)\n",
    "    \n",
    "#     print(f\"Processed a chunk with {len(merged_chunk)} records.\")\n",
    "\n",
    "# # Step 6: Concatenate all chunks and save to CSV\n",
    "# final_df = pd.concat(all_chunks, ignore_index=True)\n",
    "# final_df.to_csv(folder + 'merged_patent_data.csv', index=False)\n",
    "\n",
    "# print(\"All chunks processed and saved successfully to 'merged_patent_data.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # File paths\n",
    "# folder = \"/home/eadlzarabi/Desktop/datasets/Patents/\"\n",
    "# patent_data_file = 'patent_data.csv'\n",
    "# patent_classes_file = 'patent_classes.csv'\n",
    "\n",
    "# # Step 1: Function to process each chunk of patent_data\n",
    "# def process_chunk(patent_data_chunk, patent_classes_df):\n",
    "#     # Copy the chunk to avoid modifying a slice\n",
    "#     patent_data_chunk = patent_data_chunk.copy()\n",
    "\n",
    "#     # Ensure both 'patent_id' columns are of type str for merging\n",
    "#     patent_data_chunk['patent_id'] = patent_data_chunk['patent_id'].astype(str)\n",
    "#     patent_classes_df['patent_id'] = patent_classes_df['patent_id'].astype(str)\n",
    "    \n",
    "#     # Merge the chunk with the classes DataFrame on 'patent_id'\n",
    "#     merged_chunk = pd.merge(patent_data_chunk, patent_classes_df, on='patent_id', how='inner')\n",
    "    \n",
    "#     return merged_chunk\n",
    "\n",
    "# # Step 2: Load patent_classes.csv once (since it's relatively smaller)\n",
    "# patent_classes_df = pd.read_csv(folder + patent_classes_file, usecols=['patent_id', 'cpc_section', 'cpc_class'])\n",
    "\n",
    "# # Step 3: Initialize an empty list to store the results\n",
    "# all_chunks = []\n",
    "\n",
    "# # Step 4: Process the patent_data.csv in chunks\n",
    "# chunksize = 100000  # Adjust the chunk size based on your system's memory capacity\n",
    "# patent_data_iter = pd.read_csv(folder + patent_data_file, chunksize=chunksize, usecols=['patent_id', 'patent_date', 'patent_abstract'])\n",
    "\n",
    "# # Step 5: Process each chunk\n",
    "# for chunk in patent_data_iter:\n",
    "#     # Process and merge the chunk with patent_classes\n",
    "#     merged_chunk = process_chunk(chunk, patent_classes_df)\n",
    "    \n",
    "#     # Add the processed chunk to the list\n",
    "#     all_chunks.append(merged_chunk)\n",
    "    \n",
    "#     print(f\"Processed a chunk with {len(merged_chunk)} records.\")\n",
    "\n",
    "# # Step 6: Concatenate all chunks and save to CSV\n",
    "# final_df = pd.concat(all_chunks, ignore_index=True)\n",
    "# final_df.to_csv(folder + 'merged_patent_data.csv', index=False)\n",
    "\n",
    "# print(\"All chunks processed and saved successfully to 'merged_patent_data.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved a chunk with 99666 records.\n",
      "Processed and saved a chunk with 99650 records.\n",
      "Processed and saved a chunk with 99590 records.\n",
      "Processed and saved a chunk with 99583 records.\n",
      "Processed and saved a chunk with 99564 records.\n",
      "Processed and saved a chunk with 99508 records.\n",
      "Processed and saved a chunk with 99502 records.\n",
      "Processed and saved a chunk with 99501 records.\n",
      "Processed and saved a chunk with 99541 records.\n",
      "Processed and saved a chunk with 99501 records.\n",
      "Processed and saved a chunk with 99401 records.\n",
      "Processed and saved a chunk with 99461 records.\n",
      "Processed and saved a chunk with 99473 records.\n",
      "Processed and saved a chunk with 99526 records.\n",
      "Processed and saved a chunk with 99412 records.\n",
      "Processed and saved a chunk with 99486 records.\n",
      "Processed and saved a chunk with 99464 records.\n",
      "Processed and saved a chunk with 99452 records.\n",
      "Processed and saved a chunk with 99460 records.\n",
      "Processed and saved a chunk with 99376 records.\n",
      "Processed and saved a chunk with 99666 records.\n",
      "Processed and saved a chunk with 99970 records.\n",
      "Processed and saved a chunk with 99987 records.\n",
      "Processed and saved a chunk with 99985 records.\n",
      "Processed and saved a chunk with 99994 records.\n",
      "Processed and saved a chunk with 99981 records.\n",
      "Processed and saved a chunk with 99976 records.\n",
      "Processed and saved a chunk with 99988 records.\n",
      "Processed and saved a chunk with 99978 records.\n",
      "Processed and saved a chunk with 99966 records.\n",
      "Processed and saved a chunk with 99958 records.\n",
      "Processed and saved a chunk with 99924 records.\n",
      "Processed and saved a chunk with 99959 records.\n",
      "Processed and saved a chunk with 99949 records.\n",
      "Processed and saved a chunk with 99980 records.\n",
      "Processed and saved a chunk with 99987 records.\n",
      "Processed and saved a chunk with 99984 records.\n",
      "Processed and saved a chunk with 99975 records.\n",
      "Processed and saved a chunk with 99961 records.\n",
      "Processed and saved a chunk with 99950 records.\n",
      "Processed and saved a chunk with 99936 records.\n",
      "Processed and saved a chunk with 99908 records.\n",
      "Processed and saved a chunk with 99936 records.\n",
      "Processed and saved a chunk with 99927 records.\n",
      "Processed and saved a chunk with 99915 records.\n",
      "Processed and saved a chunk with 99975 records.\n",
      "Processed and saved a chunk with 99953 records.\n",
      "Processed and saved a chunk with 99971 records.\n",
      "Processed and saved a chunk with 99949 records.\n",
      "Processed and saved a chunk with 99932 records.\n",
      "Processed and saved a chunk with 99950 records.\n",
      "Processed and saved a chunk with 99950 records.\n",
      "Processed and saved a chunk with 99980 records.\n",
      "Processed and saved a chunk with 99965 records.\n",
      "Processed and saved a chunk with 99984 records.\n",
      "Processed and saved a chunk with 99989 records.\n",
      "Processed and saved a chunk with 99954 records.\n",
      "Processed and saved a chunk with 99905 records.\n",
      "Processed and saved a chunk with 99925 records.\n",
      "Processed and saved a chunk with 99861 records.\n",
      "Processed and saved a chunk with 99833 records.\n",
      "Processed and saved a chunk with 99845 records.\n",
      "Processed and saved a chunk with 99837 records.\n",
      "Processed and saved a chunk with 99804 records.\n",
      "Processed and saved a chunk with 99788 records.\n",
      "Processed and saved a chunk with 99771 records.\n",
      "Processed and saved a chunk with 99735 records.\n",
      "Processed and saved a chunk with 99706 records.\n",
      "Processed and saved a chunk with 99689 records.\n",
      "Processed and saved a chunk with 99717 records.\n",
      "Processed and saved a chunk with 99636 records.\n",
      "Processed and saved a chunk with 99680 records.\n",
      "Processed and saved a chunk with 99692 records.\n",
      "Processed and saved a chunk with 99618 records.\n",
      "Processed and saved a chunk with 99666 records.\n",
      "Processed and saved a chunk with 99673 records.\n",
      "Processed and saved a chunk with 99597 records.\n",
      "Processed and saved a chunk with 99627 records.\n",
      "Processed and saved a chunk with 99649 records.\n",
      "Processed and saved a chunk with 99646 records.\n",
      "Processed and saved a chunk with 99629 records.\n",
      "Processed and saved a chunk with 23297 records.\n",
      "Processed and saved a chunk with 0 records.\n",
      "Processed and saved a chunk with 0 records.\n",
      "Processed and saved a chunk with 0 records.\n",
      "Processed and saved a chunk with 0 records.\n",
      "Processed and saved a chunk with 0 records.\n",
      "Processed and saved a chunk with 0 records.\n",
      "Processed and saved a chunk with 0 records.\n",
      "Processed and saved a chunk with 0 records.\n",
      "All chunks processed and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "folder = \"/home/eadlzarabi/Desktop/datasets/Patents/\"\n",
    "patent_data_file = 'patent_data.csv'\n",
    "patent_classes_file = 'patent_classes.csv'\n",
    "output_file = 'merged_patent_data.csv'\n",
    "\n",
    "# Function to process each chunk of patent_data\n",
    "def process_chunk(patent_data_chunk, patent_classes_df):\n",
    "    # Ensure 'patent_id' is of type str for merging\n",
    "    patent_data_chunk['patent_id'] = patent_data_chunk['patent_id'].astype(str)\n",
    "    # Merge the chunk with the classes DataFrame on 'patent_id' with validation\n",
    "    merged_chunk = pd.merge(\n",
    "        patent_data_chunk,\n",
    "        patent_classes_df,\n",
    "        on='patent_id',\n",
    "        how='inner',\n",
    "        validate='many_to_one'  # Expects patent_classes_df to have unique patent_id values\n",
    "    )\n",
    "    # Remove any duplicate rows that may have been introduced during the merge\n",
    "    merged_chunk = merged_chunk.drop_duplicates()\n",
    "    return merged_chunk\n",
    "\n",
    "# Load patent_classes.csv once\n",
    "patent_classes_df = pd.read_csv(\n",
    "    folder + patent_classes_file,\n",
    "    usecols=['patent_id', 'cpc_section', 'cpc_class'],\n",
    "    dtype={'patent_id': str}  # Ensure 'patent_id' is read as string\n",
    ")\n",
    "\n",
    "# Remove duplicates from patent_classes_df to satisfy the merge condition\n",
    "patent_classes_df = patent_classes_df.drop_duplicates(subset=['patent_id'])\n",
    "\n",
    "# Process patent_data.csv in chunks\n",
    "chunksize = 100000\n",
    "first_chunk = True\n",
    "\n",
    "with pd.read_csv(\n",
    "    folder + patent_data_file,\n",
    "    chunksize=chunksize,\n",
    "    usecols=['patent_id', 'patent_date', 'patent_abstract'],\n",
    "    dtype={'patent_id': str}  # Ensure 'patent_id' is read as string\n",
    ") as reader:\n",
    "    for chunk in reader:\n",
    "        merged_chunk = process_chunk(chunk, patent_classes_df)\n",
    "        # Append to CSV file\n",
    "        merged_chunk.to_csv(\n",
    "            folder + output_file,\n",
    "            mode='w' if first_chunk else 'a',\n",
    "            index=False,\n",
    "            header=first_chunk\n",
    "        )\n",
    "        first_chunk = False\n",
    "        print(f\"Processed and saved a chunk with {len(merged_chunk)} records.\")\n",
    "\n",
    "print(\"All chunks processed and saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Now we need to merge merged_patent_data.csv and processed_patent_applicants.csv \n",
    "Merge on patent_id\n",
    "save to csv called patents.csv\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 100000 records.\n",
      "Processed a chunk with 5235 records.\n",
      "All chunks processed and saved successfully to '/home/eadlzarabi/Desktop/datasets/Patents/patents.csv'.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "folder = \"/home/eadlzarabi/Desktop/datasets/Patents/\"\n",
    "merged_patent_data_file = 'merged_patent_data.csv'\n",
    "processed_patent_applicants_file = 'processed_patent_applicants.csv'\n",
    "output_file = folder + 'patents.csv'\n",
    "\n",
    "# Load processed_patent_applicants.csv once (since it's relatively smaller)\n",
    "processed_patent_applicants_df = pd.read_csv(\n",
    "    folder + processed_patent_applicants_file, \n",
    "    on_bad_lines='skip', \n",
    "    skipinitialspace=True\n",
    ")\n",
    "\n",
    "# Drop duplicate patent_id entries in the applicants dataset (keeping all its fields)\n",
    "processed_patent_applicants_df = processed_patent_applicants_df.drop_duplicates(subset=['patent_id'])\n",
    "\n",
    "# Initialize an empty list to store results\n",
    "all_chunks = []\n",
    "\n",
    "# Process merged_patent_data.csv in chunks\n",
    "chunksize = 100000\n",
    "merged_patent_data_iter = pd.read_csv(\n",
    "    folder + merged_patent_data_file, \n",
    "    chunksize=chunksize, \n",
    "    on_bad_lines='skip', \n",
    "    skipinitialspace=True\n",
    ")\n",
    "\n",
    "# Process each chunk\n",
    "for chunk in merged_patent_data_iter:\n",
    "    # Drop duplicate patent_id rows, keeping all fields from merged_patent_data\n",
    "    merged_patent_data_chunk = chunk.drop_duplicates(subset=['patent_id'])\n",
    "    \n",
    "    # Ensure patent_id is of type str for safe merging in both DataFrames\n",
    "    merged_patent_data_chunk['patent_id'] = merged_patent_data_chunk['patent_id'].astype(str)\n",
    "    processed_patent_applicants_df['patent_id'] = processed_patent_applicants_df['patent_id'].astype(str)\n",
    "    \n",
    "    # Merge the chunk with processed_patent_applicants_df using a LEFT JOIN so all fields from both files are kept\n",
    "    merged_chunk = pd.merge(\n",
    "        merged_patent_data_chunk, \n",
    "        processed_patent_applicants_df, \n",
    "        on='patent_id', \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Append the processed chunk to the list\n",
    "    all_chunks.append(merged_chunk)\n",
    "    print(f\"Processed a chunk with {len(merged_chunk)} records.\")\n",
    "\n",
    "# Concatenate all chunks and save to CSV\n",
    "final_df = pd.concat(all_chunks, ignore_index=True)\n",
    "final_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"All chunks processed and saved successfully to '{output_file}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# import tempfile\n",
    "\n",
    "# # File paths\n",
    "# folder = \"/home/eadlzarabi/Desktop/datasets/Patents/\"\n",
    "# patents_file = 'patents.csv'\n",
    "# patent_classes_file = 'patent_classes.csv'\n",
    "# output_file = 'patents0.csv'\n",
    "\n",
    "# chunksize = 100000\n",
    "# temp_files = []  # Store temporary file paths\n",
    "\n",
    "# # Process patents.csv in chunks\n",
    "# with pd.read_csv(folder + patents_file, chunksize=chunksize, on_bad_lines='skip', skipinitialspace=True) as patents_iter:\n",
    "#     for i, patents_chunk in enumerate(patents_iter):\n",
    "#         patents_chunk['patent_id'] = patents_chunk['patent_id'].astype(str)\n",
    "\n",
    "#         # Create a temporary file\n",
    "#         temp_fd, temp_path = tempfile.mkstemp(suffix=\".csv\", dir=folder)\n",
    "#         os.close(temp_fd)  # Close the file descriptor, Pandas will handle writing\n",
    "\n",
    "#         # Process patent_classes.csv in chunks and merge\n",
    "#         with pd.read_csv(folder + patent_classes_file, usecols=['patent_id', 'cpc_section', 'cpc_class'], \n",
    "#                          chunksize=chunksize, on_bad_lines='skip') as patent_classes_iter:\n",
    "#             for patent_classes_chunk in patent_classes_iter:\n",
    "#                 patent_classes_chunk['patent_id'] = patent_classes_chunk['patent_id'].astype(str)\n",
    "\n",
    "#                 merged_chunk = pd.merge(patents_chunk, patent_classes_chunk, on='patent_id', how='left')\n",
    "\n",
    "#                 # Save to temporary file\n",
    "#                 merged_chunk.to_csv(temp_path, mode='a', index=False, header=not os.path.exists(temp_path))\n",
    "        \n",
    "#         temp_files.append(temp_path)  # Store temp file path\n",
    "#         print(f\"Processed chunk {i+1}, saved to {temp_path}\")\n",
    "\n",
    "# # Combine all temporary files into the final output file\n",
    "# with open(folder + output_file, 'w') as outfile:\n",
    "#     for i, temp_path in enumerate(temp_files):\n",
    "#         with open(temp_path, 'r') as infile:\n",
    "#             if i > 0:\n",
    "#                 infile.readline()  # Skip header except for the first file\n",
    "#             outfile.write(infile.read())\n",
    "\n",
    "# # Delete temporary files\n",
    "# for temp_path in temp_files:\n",
    "#     os.remove(temp_path)\n",
    "#     print(f\"Deleted temp file: {temp_path}\")\n",
    "\n",
    "# print(f\"Final merged file saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will save output to: /home/eadlzarabi/Desktop/datasets/Patents/merged_patent_citations.csv\n",
      "Starting patent citation processing...\n",
      "Reading citation file headers...\n",
      "Available columns in citation file: ['patent_id', 'US_citation_citation_sequence', 'US_citation_citation_patent_id', 'US_citation_citation_date']\n",
      "Column mapping: {'patent_id': 'patent_id', 'US_citation_citation_sequence': 'US_citation_sequence', 'US_citation_citation_patent_id': 'US_citation_patent_id', 'US_citation_citation_date': 'US_citation_date'}\n",
      "Beginning merge operation...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 49513 matching citations\n",
      "Found 49574 matching citations\n",
      "Found 48615 matching citations\n",
      "Found 49817 matching citations\n",
      "Found 49666 matching citations\n",
      "Found 49750 matching citations\n",
      "Found 49896 matching citations\n",
      "Found 49910 matching citations\n",
      "Found 49840 matching citations\n",
      "Found 49464 matching citations\n",
      "Found 49979 matching citations\n",
      "Found 49371 matching citations\n",
      "Found 49938 matching citations\n",
      "Found 49781 matching citations\n",
      "Found 49460 matching citations\n",
      "Found 49862 matching citations\n",
      "Found 49365 matching citations\n",
      "Found 49855 matching citations\n",
      "Found 24539 matching citations\n",
      "Combined 918195 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 925031 rows\n",
      "File exists with size: 694118285 bytes\n",
      "Processed 50000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 25317 matching citations\n",
      "Found 49989 matching citations\n",
      "Found 49624 matching citations\n",
      "Found 49877 matching citations\n",
      "Found 49615 matching citations\n",
      "Found 49671 matching citations\n",
      "Found 49985 matching citations\n",
      "Found 49857 matching citations\n",
      "Found 49780 matching citations\n",
      "Found 49814 matching citations\n",
      "Found 49875 matching citations\n",
      "Found 49942 matching citations\n",
      "Found 49960 matching citations\n",
      "Found 49336 matching citations\n",
      "Found 49918 matching citations\n",
      "Found 49680 matching citations\n",
      "Found 49793 matching citations\n",
      "Found 49477 matching citations\n",
      "Found 49369 matching citations\n",
      "Found 49920 matching citations\n",
      "Found 1176 matching citations\n",
      "Combined 971975 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 978867 rows\n",
      "File exists with size: 1440870699 bytes\n",
      "Processed 100000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 48429 matching citations\n",
      "Found 49455 matching citations\n",
      "Found 49966 matching citations\n",
      "Found 49758 matching citations\n",
      "Found 49638 matching citations\n",
      "Found 49934 matching citations\n",
      "Found 49497 matching citations\n",
      "Found 49877 matching citations\n",
      "Found 49822 matching citations\n",
      "Found 49305 matching citations\n",
      "Found 49988 matching citations\n",
      "Found 49936 matching citations\n",
      "Found 49774 matching citations\n",
      "Found 49398 matching citations\n",
      "Found 49855 matching citations\n",
      "Found 49878 matching citations\n",
      "Found 49413 matching citations\n",
      "Found 49991 matching citations\n",
      "Found 14467 matching citations\n",
      "Combined 908381 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 915538 rows\n",
      "File exists with size: 2133457776 bytes\n",
      "Processed 150000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 34874 matching citations\n",
      "Found 49373 matching citations\n",
      "Found 49946 matching citations\n",
      "Found 49912 matching citations\n",
      "Found 49758 matching citations\n",
      "Found 49985 matching citations\n",
      "Found 49896 matching citations\n",
      "Found 49958 matching citations\n",
      "Found 49796 matching citations\n",
      "Found 49926 matching citations\n",
      "Found 49764 matching citations\n",
      "Found 49789 matching citations\n",
      "Found 49973 matching citations\n",
      "Found 49913 matching citations\n",
      "Found 49928 matching citations\n",
      "Found 49656 matching citations\n",
      "Found 49772 matching citations\n",
      "Found 49793 matching citations\n",
      "Found 49895 matching citations\n",
      "Found 18189 matching citations\n",
      "Combined 950096 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 957283 rows\n",
      "File exists with size: 2859202031 bytes\n",
      "Processed 200000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 31449 matching citations\n",
      "Found 49898 matching citations\n",
      "Found 49981 matching citations\n",
      "Found 49719 matching citations\n",
      "Found 49191 matching citations\n",
      "Found 49865 matching citations\n",
      "Found 49886 matching citations\n",
      "Found 49934 matching citations\n",
      "Found 49658 matching citations\n",
      "Found 49901 matching citations\n",
      "Found 49858 matching citations\n",
      "Found 49423 matching citations\n",
      "Found 49947 matching citations\n",
      "Found 49496 matching citations\n",
      "Found 49927 matching citations\n",
      "Found 49394 matching citations\n",
      "Found 49226 matching citations\n",
      "Found 49894 matching citations\n",
      "Found 49964 matching citations\n",
      "Found 49913 matching citations\n",
      "Found 49713 matching citations\n",
      "Found 2752 matching citations\n",
      "Combined 1028989 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 1035975 rows\n",
      "File exists with size: 3665423900 bytes\n",
      "Processed 250000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 47007 matching citations\n",
      "Found 48708 matching citations\n",
      "Found 49882 matching citations\n",
      "Found 48975 matching citations\n",
      "Found 49888 matching citations\n",
      "Found 49967 matching citations\n",
      "Found 49572 matching citations\n",
      "Found 49700 matching citations\n",
      "Found 49932 matching citations\n",
      "Found 49052 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49927 matching citations\n",
      "Found 49841 matching citations\n",
      "Found 49999 matching citations\n",
      "Found 49774 matching citations\n",
      "Found 49450 matching citations\n",
      "Found 49869 matching citations\n",
      "Found 49846 matching citations\n",
      "Found 49779 matching citations\n",
      "Found 49813 matching citations\n",
      "Found 25213 matching citations\n",
      "Combined 1016194 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 1023512 rows\n",
      "File exists with size: 4455984450 bytes\n",
      "Processed 300000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 24601 matching citations\n",
      "Found 49940 matching citations\n",
      "Found 49867 matching citations\n",
      "Found 49886 matching citations\n",
      "Found 49987 matching citations\n",
      "Found 49922 matching citations\n",
      "Found 49865 matching citations\n",
      "Found 49849 matching citations\n",
      "Found 49915 matching citations\n",
      "Found 49174 matching citations\n",
      "Found 49781 matching citations\n",
      "Found 49953 matching citations\n",
      "Found 49817 matching citations\n",
      "Found 49972 matching citations\n",
      "Found 49377 matching citations\n",
      "Found 49717 matching citations\n",
      "Found 49989 matching citations\n",
      "Found 49886 matching citations\n",
      "Found 49592 matching citations\n",
      "Found 49693 matching citations\n",
      "Found 34374 matching citations\n",
      "Combined 1005157 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 1012772 rows\n",
      "File exists with size: 5227156665 bytes\n",
      "Processed 350000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 15469 matching citations\n",
      "Found 49515 matching citations\n",
      "Found 49540 matching citations\n",
      "Found 48350 matching citations\n",
      "Found 49898 matching citations\n",
      "Found 49776 matching citations\n",
      "Found 49659 matching citations\n",
      "Found 49676 matching citations\n",
      "Found 49764 matching citations\n",
      "Found 49650 matching citations\n",
      "Found 49902 matching citations\n",
      "Found 49410 matching citations\n",
      "Found 49965 matching citations\n",
      "Found 49804 matching citations\n",
      "Found 49588 matching citations\n",
      "Found 49947 matching citations\n",
      "Found 49854 matching citations\n",
      "Found 49576 matching citations\n",
      "Found 49732 matching citations\n",
      "Found 49976 matching citations\n",
      "Found 15898 matching citations\n",
      "Combined 974949 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 982406 rows\n",
      "File exists with size: 5974594049 bytes\n",
      "Processed 400000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 33963 matching citations\n",
      "Found 49639 matching citations\n",
      "Found 49699 matching citations\n",
      "Found 49948 matching citations\n",
      "Found 49421 matching citations\n",
      "Found 49195 matching citations\n",
      "Found 49998 matching citations\n",
      "Found 49690 matching citations\n",
      "Found 49024 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49605 matching citations\n",
      "Found 49419 matching citations\n",
      "Found 49984 matching citations\n",
      "Found 49889 matching citations\n",
      "Found 49698 matching citations\n",
      "Found 49907 matching citations\n",
      "Found 49787 matching citations\n",
      "Found 49931 matching citations\n",
      "Found 49717 matching citations\n",
      "Found 49953 matching citations\n",
      "Found 49765 matching citations\n",
      "Found 14347 matching citations\n",
      "Combined 1042579 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 1050102 rows\n",
      "File exists with size: 6770566963 bytes\n",
      "Processed 450000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 35564 matching citations\n",
      "Found 49961 matching citations\n",
      "Found 49308 matching citations\n",
      "Found 49859 matching citations\n",
      "Found 49857 matching citations\n",
      "Found 49713 matching citations\n",
      "Found 49909 matching citations\n",
      "Found 48569 matching citations\n",
      "Found 49743 matching citations\n",
      "Found 49442 matching citations\n",
      "Found 49985 matching citations\n",
      "Found 48631 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49699 matching citations\n",
      "Found 49804 matching citations\n",
      "Found 49674 matching citations\n",
      "Found 49154 matching citations\n",
      "Found 49048 matching citations\n",
      "Found 49628 matching citations\n",
      "Found 49097 matching citations\n",
      "Found 9636 matching citations\n",
      "Combined 986281 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 993813 rows\n",
      "File exists with size: 7530912887 bytes\n",
      "Processed 500000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 40064 matching citations\n",
      "Found 49483 matching citations\n",
      "Found 49851 matching citations\n",
      "Found 49692 matching citations\n",
      "Found 49876 matching citations\n",
      "Found 49982 matching citations\n",
      "Found 49370 matching citations\n",
      "Found 49069 matching citations\n",
      "Found 49972 matching citations\n",
      "Found 49394 matching citations\n",
      "Found 49059 matching citations\n",
      "Found 49401 matching citations\n",
      "Found 49322 matching citations\n",
      "Found 49896 matching citations\n",
      "Found 49595 matching citations\n",
      "Found 49860 matching citations\n",
      "Found 49821 matching citations\n",
      "Found 49907 matching citations\n",
      "Found 49395 matching citations\n",
      "Found 49360 matching citations\n",
      "Found 34492 matching citations\n",
      "Combined 1016861 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 1024412 rows\n",
      "File exists with size: 8318350562 bytes\n",
      "Processed 550000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 15351 matching citations\n",
      "Found 49728 matching citations\n",
      "Found 49805 matching citations\n",
      "Found 49552 matching citations\n",
      "Found 49870 matching citations\n",
      "Found 49344 matching citations\n",
      "Found 49665 matching citations\n",
      "Found 49804 matching citations\n",
      "Found 49800 matching citations\n",
      "Found 49825 matching citations\n",
      "Found 49702 matching citations\n",
      "Found 49713 matching citations\n",
      "Found 49583 matching citations\n",
      "Found 49864 matching citations\n",
      "Found 49845 matching citations\n",
      "Found 49024 matching citations\n",
      "Found 49962 matching citations\n",
      "Found 49362 matching citations\n",
      "Found 49945 matching citations\n",
      "Found 49960 matching citations\n",
      "Found 23064 matching citations\n",
      "Combined 982768 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 990243 rows\n",
      "File exists with size: 9079496310 bytes\n",
      "Processed 600000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 26104 matching citations\n",
      "Found 49817 matching citations\n",
      "Found 49392 matching citations\n",
      "Found 48266 matching citations\n",
      "Found 49974 matching citations\n",
      "Found 49947 matching citations\n",
      "Found 49185 matching citations\n",
      "Found 49681 matching citations\n",
      "Found 49676 matching citations\n",
      "Found 49144 matching citations\n",
      "Found 49953 matching citations\n",
      "Found 48999 matching citations\n",
      "Found 49873 matching citations\n",
      "Found 49764 matching citations\n",
      "Found 49941 matching citations\n",
      "Found 49674 matching citations\n",
      "Found 49955 matching citations\n",
      "Found 49607 matching citations\n",
      "Found 49673 matching citations\n",
      "Found 45265 matching citations\n",
      "Combined 963890 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 971780 rows\n",
      "File exists with size: 9831540660 bytes\n",
      "Processed 650000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 4084 matching citations\n",
      "Found 49673 matching citations\n",
      "Found 49980 matching citations\n",
      "Found 49553 matching citations\n",
      "Found 49995 matching citations\n",
      "Found 49893 matching citations\n",
      "Found 49441 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49827 matching citations\n",
      "Found 49766 matching citations\n",
      "Found 49945 matching citations\n",
      "Found 49939 matching citations\n",
      "Found 49868 matching citations\n",
      "Found 49900 matching citations\n",
      "Found 49841 matching citations\n",
      "Found 49899 matching citations\n",
      "Found 49974 matching citations\n",
      "Found 49698 matching citations\n",
      "Found 49813 matching citations\n",
      "Found 49528 matching citations\n",
      "Found 49973 matching citations\n",
      "Found 27172 matching citations\n",
      "Combined 1027762 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 1035645 rows\n",
      "File exists with size: 10628779792 bytes\n",
      "Processed 700000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 22022 matching citations\n",
      "Found 49658 matching citations\n",
      "Found 49402 matching citations\n",
      "Found 49813 matching citations\n",
      "Found 49852 matching citations\n",
      "Found 49859 matching citations\n",
      "Found 49739 matching citations\n",
      "Found 49762 matching citations\n",
      "Found 49911 matching citations\n",
      "Found 49763 matching citations\n",
      "Found 49878 matching citations\n",
      "Found 49748 matching citations\n",
      "Found 49977 matching citations\n",
      "Found 49324 matching citations\n",
      "Found 49729 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49757 matching citations\n",
      "Found 49716 matching citations\n",
      "Found 49988 matching citations\n",
      "Found 49932 matching citations\n",
      "Found 49410 matching citations\n",
      "Found 49881 matching citations\n",
      "Found 13531 matching citations\n",
      "Combined 1080652 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 1088502 rows\n",
      "File exists with size: 11466700807 bytes\n",
      "Processed 750000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 36300 matching citations\n",
      "Found 48895 matching citations\n",
      "Found 49999 matching citations\n",
      "Found 49452 matching citations\n",
      "Found 49813 matching citations\n",
      "Found 49782 matching citations\n",
      "Found 49366 matching citations\n",
      "Found 49944 matching citations\n",
      "Found 49950 matching citations\n",
      "Found 49674 matching citations\n",
      "Found 49229 matching citations\n",
      "Found 49784 matching citations\n",
      "Found 49724 matching citations\n",
      "Found 49893 matching citations\n",
      "Found 49730 matching citations\n",
      "Found 49733 matching citations\n",
      "Found 49385 matching citations\n",
      "Found 49903 matching citations\n",
      "Found 49789 matching citations\n",
      "Found 27612 matching citations\n",
      "Combined 957957 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 965870 rows\n",
      "File exists with size: 12207167912 bytes\n",
      "Processed 800000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 22173 matching citations\n",
      "Found 49738 matching citations\n",
      "Found 49989 matching citations\n",
      "Found 49125 matching citations\n",
      "Found 49599 matching citations\n",
      "Found 49750 matching citations\n",
      "Found 49749 matching citations\n",
      "Found 49986 matching citations\n",
      "Found 49660 matching citations\n",
      "Found 49946 matching citations\n",
      "Found 49998 matching citations\n",
      "Found 49713 matching citations\n",
      "Found 49632 matching citations\n",
      "Found 49763 matching citations\n",
      "Found 49797 matching citations\n",
      "Found 49896 matching citations\n",
      "Found 49562 matching citations\n",
      "Found 49796 matching citations\n",
      "Found 49940 matching citations\n",
      "Found 49383 matching citations\n",
      "Found 27782 matching citations\n",
      "Combined 994977 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 1002872 rows\n",
      "File exists with size: 12970371842 bytes\n",
      "Processed 850000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 22007 matching citations\n",
      "Found 49628 matching citations\n",
      "Found 49589 matching citations\n",
      "Found 49940 matching citations\n",
      "Found 49793 matching citations\n",
      "Found 49802 matching citations\n",
      "Found 49601 matching citations\n",
      "Found 49691 matching citations\n",
      "Found 49926 matching citations\n",
      "Found 49672 matching citations\n",
      "Found 49654 matching citations\n",
      "Found 49931 matching citations\n",
      "Found 49477 matching citations\n",
      "Found 49892 matching citations\n",
      "Found 49618 matching citations\n",
      "Found 49736 matching citations\n",
      "Found 49495 matching citations\n",
      "Found 49431 matching citations\n",
      "Found 49765 matching citations\n",
      "Found 49666 matching citations\n",
      "Found 48366 matching citations\n",
      "Found 12674 matching citations\n",
      "Combined 1027354 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 1035509 rows\n",
      "File exists with size: 13763513269 bytes\n",
      "Processed 900000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 36006 matching citations\n",
      "Found 49677 matching citations\n",
      "Found 49832 matching citations\n",
      "Found 49979 matching citations\n",
      "Found 49766 matching citations\n",
      "Found 49737 matching citations\n",
      "Found 49392 matching citations\n",
      "Found 49477 matching citations\n",
      "Found 49612 matching citations\n",
      "Found 49578 matching citations\n",
      "Found 49638 matching citations\n",
      "Found 49882 matching citations\n",
      "Found 48461 matching citations\n",
      "Found 49751 matching citations\n",
      "Found 49495 matching citations\n",
      "Found 49712 matching citations\n",
      "Found 49998 matching citations\n",
      "Found 49533 matching citations\n",
      "Found 49480 matching citations\n",
      "Found 49943 matching citations\n",
      "Found 17060 matching citations\n",
      "Combined 996009 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 1004127 rows\n",
      "File exists with size: 14520976432 bytes\n",
      "Processed 950000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 32834 matching citations\n",
      "Found 49387 matching citations\n",
      "Found 49933 matching citations\n",
      "Found 48746 matching citations\n",
      "Found 49967 matching citations\n",
      "Found 49686 matching citations\n",
      "Found 49378 matching citations\n",
      "Found 49991 matching citations\n",
      "Found 49338 matching citations\n",
      "Found 49751 matching citations\n",
      "Found 49586 matching citations\n",
      "Found 49832 matching citations\n",
      "Found 49658 matching citations\n",
      "Found 49940 matching citations\n",
      "Found 49764 matching citations\n",
      "Found 49975 matching citations\n",
      "Found 49596 matching citations\n",
      "Found 49254 matching citations\n",
      "Found 49830 matching citations\n",
      "Found 45810 matching citations\n",
      "Combined 972256 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 980533 rows\n",
      "File exists with size: 15260001456 bytes\n",
      "Processed 1000000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 3817 matching citations\n",
      "Found 49713 matching citations\n",
      "Found 49870 matching citations\n",
      "Found 49743 matching citations\n",
      "Found 49991 matching citations\n",
      "Found 49710 matching citations\n",
      "Found 49727 matching citations\n",
      "Found 47480 matching citations\n",
      "Found 49363 matching citations\n",
      "Found 48920 matching citations\n",
      "Found 49662 matching citations\n",
      "Found 49660 matching citations\n",
      "Found 49628 matching citations\n",
      "Found 49613 matching citations\n",
      "Found 48636 matching citations\n",
      "Found 49558 matching citations\n",
      "Found 49956 matching citations\n",
      "Found 49842 matching citations\n",
      "Found 49454 matching citations\n",
      "Found 49999 matching citations\n",
      "Found 49964 matching citations\n",
      "Found 37580 matching citations\n",
      "Combined 1031886 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 1039871 rows\n",
      "File exists with size: 16047753917 bytes\n",
      "Processed 1050000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 12067 matching citations\n",
      "Found 49929 matching citations\n",
      "Found 49313 matching citations\n",
      "Found 49722 matching citations\n",
      "Found 49621 matching citations\n",
      "Found 49782 matching citations\n",
      "Found 49933 matching citations\n",
      "Found 49073 matching citations\n",
      "Found 49934 matching citations\n",
      "Found 48626 matching citations\n",
      "Found 49499 matching citations\n",
      "Found 49801 matching citations\n",
      "Found 47135 matching citations\n",
      "Found 49427 matching citations\n",
      "Found 49549 matching citations\n",
      "Found 49681 matching citations\n",
      "Found 49816 matching citations\n",
      "Found 49772 matching citations\n",
      "Found 49914 matching citations\n",
      "Found 49992 matching citations\n",
      "Found 30502 matching citations\n",
      "Combined 983088 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 991602 rows\n",
      "File exists with size: 16803011327 bytes\n",
      "Processed 1100000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 19110 matching citations\n",
      "Found 49246 matching citations\n",
      "Found 49397 matching citations\n",
      "Found 49512 matching citations\n",
      "Found 49723 matching citations\n",
      "Found 49327 matching citations\n",
      "Found 49999 matching citations\n",
      "Found 49409 matching citations\n",
      "Found 49934 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49745 matching citations\n",
      "Found 49476 matching citations\n",
      "Found 49702 matching citations\n",
      "Found 49635 matching citations\n",
      "Found 48863 matching citations\n",
      "Found 49621 matching citations\n",
      "Found 49950 matching citations\n",
      "Found 49723 matching citations\n",
      "Found 49273 matching citations\n",
      "Found 49447 matching citations\n",
      "Found 40116 matching citations\n",
      "Combined 1001208 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 1009758 rows\n",
      "File exists with size: 17568531821 bytes\n",
      "Processed 1150000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 9359 matching citations\n",
      "Found 49663 matching citations\n",
      "Found 49332 matching citations\n",
      "Found 49222 matching citations\n",
      "Found 49756 matching citations\n",
      "Found 49351 matching citations\n",
      "Found 49940 matching citations\n",
      "Found 49574 matching citations\n",
      "Found 49808 matching citations\n",
      "Found 49745 matching citations\n",
      "Found 49478 matching citations\n",
      "Found 49676 matching citations\n",
      "Found 48842 matching citations\n",
      "Found 49755 matching citations\n",
      "Found 49821 matching citations\n",
      "Found 49754 matching citations\n",
      "Found 49918 matching citations\n",
      "Found 49988 matching citations\n",
      "Found 49947 matching citations\n",
      "Found 49802 matching citations\n",
      "Found 7813 matching citations\n",
      "Combined 960544 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 969163 rows\n",
      "File exists with size: 18289178535 bytes\n",
      "Processed 1200000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 40940 matching citations\n",
      "Found 49910 matching citations\n",
      "Found 48940 matching citations\n",
      "Found 49457 matching citations\n",
      "Found 49686 matching citations\n",
      "Found 49727 matching citations\n",
      "Found 49763 matching citations\n",
      "Found 49951 matching citations\n",
      "Found 49887 matching citations\n",
      "Found 49732 matching citations\n",
      "Found 49783 matching citations\n",
      "Found 49871 matching citations\n",
      "Found 49997 matching citations\n",
      "Found 49296 matching citations\n",
      "Found 49759 matching citations\n",
      "Found 49899 matching citations\n",
      "Found 49594 matching citations\n",
      "Found 49698 matching citations\n",
      "Found 49322 matching citations\n",
      "Found 38471 matching citations\n",
      "Combined 973683 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 982692 rows\n",
      "File exists with size: 19036658217 bytes\n",
      "Processed 1250000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 10523 matching citations\n",
      "Found 49867 matching citations\n",
      "Found 49585 matching citations\n",
      "Found 49682 matching citations\n",
      "Found 49458 matching citations\n",
      "Found 49598 matching citations\n",
      "Found 49861 matching citations\n",
      "Found 49726 matching citations\n",
      "Found 49998 matching citations\n",
      "Found 49867 matching citations\n",
      "Found 49691 matching citations\n",
      "Found 49935 matching citations\n",
      "Found 49391 matching citations\n",
      "Found 47914 matching citations\n",
      "Found 49996 matching citations\n",
      "Found 49875 matching citations\n",
      "Found 49718 matching citations\n",
      "Found 49830 matching citations\n",
      "Found 49257 matching citations\n",
      "Found 49315 matching citations\n",
      "Found 49244 matching citations\n",
      "Found 22702 matching citations\n",
      "Combined 1025033 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 1033852 rows\n",
      "File exists with size: 19829760929 bytes\n",
      "Processed 1300000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 26173 matching citations\n",
      "Found 49946 matching citations\n",
      "Found 49644 matching citations\n",
      "Found 49826 matching citations\n",
      "Found 49614 matching citations\n",
      "Found 48548 matching citations\n",
      "Found 49470 matching citations\n",
      "Found 49334 matching citations\n",
      "Found 49330 matching citations\n",
      "Found 49612 matching citations\n",
      "Found 49963 matching citations\n",
      "Found 48714 matching citations\n",
      "Found 49663 matching citations\n",
      "Found 49988 matching citations\n",
      "Found 49553 matching citations\n",
      "Found 49184 matching citations\n",
      "Found 49779 matching citations\n",
      "Found 49711 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 23848 matching citations\n",
      "Combined 941900 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 951068 rows\n",
      "File exists with size: 20547474966 bytes\n",
      "Processed 1350000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 25693 matching citations\n",
      "Found 49931 matching citations\n",
      "Found 49617 matching citations\n",
      "Found 49882 matching citations\n",
      "Found 49232 matching citations\n",
      "Found 49671 matching citations\n",
      "Found 49555 matching citations\n",
      "Found 49093 matching citations\n",
      "Found 49360 matching citations\n",
      "Found 49645 matching citations\n",
      "Found 49597 matching citations\n",
      "Found 49991 matching citations\n",
      "Found 49555 matching citations\n",
      "Found 49583 matching citations\n",
      "Found 49990 matching citations\n",
      "Found 49830 matching citations\n",
      "Found 48986 matching citations\n",
      "Found 49194 matching citations\n",
      "Found 41413 matching citations\n",
      "Combined 909818 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 922293 rows\n",
      "File exists with size: 21271997431 bytes\n",
      "Processed 1400000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 7706 matching citations\n",
      "Found 49801 matching citations\n",
      "Found 49472 matching citations\n",
      "Found 49757 matching citations\n",
      "Found 49714 matching citations\n",
      "Found 49546 matching citations\n",
      "Found 49254 matching citations\n",
      "Found 49859 matching citations\n",
      "Found 49700 matching citations\n",
      "Found 49788 matching citations\n",
      "Found 49427 matching citations\n",
      "Found 49709 matching citations\n",
      "Found 49273 matching citations\n",
      "Found 48152 matching citations\n",
      "Found 49838 matching citations\n",
      "Found 49631 matching citations\n",
      "Found 49337 matching citations\n",
      "Found 48593 matching citations\n",
      "Found 49927 matching citations\n",
      "Found 35828 matching citations\n",
      "Combined 934312 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 943466 rows\n",
      "File exists with size: 21984149832 bytes\n",
      "Processed 1450000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 13921 matching citations\n",
      "Found 47700 matching citations\n",
      "Found 49610 matching citations\n",
      "Found 49128 matching citations\n",
      "Found 49849 matching citations\n",
      "Found 48881 matching citations\n",
      "Found 49772 matching citations\n",
      "Found 49217 matching citations\n",
      "Found 49891 matching citations\n",
      "Found 49538 matching citations\n",
      "Found 49899 matching citations\n",
      "Found 49072 matching citations\n",
      "Found 49546 matching citations\n",
      "Found 49746 matching citations\n",
      "Found 49855 matching citations\n",
      "Found 49761 matching citations\n",
      "Found 49682 matching citations\n",
      "Found 49634 matching citations\n",
      "Found 49329 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 39615 matching citations\n",
      "Combined 993646 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 1002957 rows\n",
      "File exists with size: 22750500982 bytes\n",
      "Processed 1500000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 10252 matching citations\n",
      "Found 49577 matching citations\n",
      "Found 49584 matching citations\n",
      "Found 49855 matching citations\n",
      "Found 49901 matching citations\n",
      "Found 49189 matching citations\n",
      "Found 49971 matching citations\n",
      "Found 49855 matching citations\n",
      "Found 49901 matching citations\n",
      "Found 49881 matching citations\n",
      "Found 49437 matching citations\n",
      "Found 49956 matching citations\n",
      "Found 49131 matching citations\n",
      "Found 49882 matching citations\n",
      "Found 49482 matching citations\n",
      "Found 49604 matching citations\n",
      "Found 49881 matching citations\n",
      "Found 49448 matching citations\n",
      "Found 49998 matching citations\n",
      "Found 49712 matching citations\n",
      "Found 8770 matching citations\n",
      "Combined 963267 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 972693 rows\n",
      "File exists with size: 23502530927 bytes\n",
      "Processed 1550000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 41085 matching citations\n",
      "Found 49704 matching citations\n",
      "Found 49515 matching citations\n",
      "Found 49918 matching citations\n",
      "Found 49092 matching citations\n",
      "Found 49869 matching citations\n",
      "Found 49275 matching citations\n",
      "Found 49454 matching citations\n",
      "Found 49750 matching citations\n",
      "Found 48955 matching citations\n",
      "Found 49724 matching citations\n",
      "Found 49600 matching citations\n",
      "Found 49605 matching citations\n",
      "Found 47563 matching citations\n",
      "Found 49841 matching citations\n",
      "Found 49890 matching citations\n",
      "Found 49554 matching citations\n",
      "Found 49062 matching citations\n",
      "Found 49189 matching citations\n",
      "Found 49799 matching citations\n",
      "Found 49564 matching citations\n",
      "Found 2705 matching citations\n",
      "Combined 1032713 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 1041669 rows\n",
      "File exists with size: 24301316528 bytes\n",
      "Processed 1600000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 47059 matching citations\n",
      "Found 49582 matching citations\n",
      "Found 49777 matching citations\n",
      "Found 49661 matching citations\n",
      "Found 49269 matching citations\n",
      "Found 49690 matching citations\n",
      "Found 49248 matching citations\n",
      "Found 49877 matching citations\n",
      "Found 49858 matching citations\n",
      "Found 49864 matching citations\n",
      "Found 49171 matching citations\n",
      "Found 49852 matching citations\n",
      "Found 49927 matching citations\n",
      "Found 49391 matching citations\n",
      "Found 49963 matching citations\n",
      "Found 49783 matching citations\n",
      "Found 49478 matching citations\n",
      "Found 49984 matching citations\n",
      "Found 49162 matching citations\n",
      "Found 49800 matching citations\n",
      "Found 49450 matching citations\n",
      "Found 23783 matching citations\n",
      "Combined 1063629 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 1072643 rows\n",
      "File exists with size: 25118999563 bytes\n",
      "Processed 1650000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 25891 matching citations\n",
      "Found 49887 matching citations\n",
      "Found 49816 matching citations\n",
      "Found 49567 matching citations\n",
      "Found 49960 matching citations\n",
      "Found 49785 matching citations\n",
      "Found 49978 matching citations\n",
      "Found 49596 matching citations\n",
      "Found 49232 matching citations\n",
      "Found 49921 matching citations\n",
      "Found 48859 matching citations\n",
      "Found 49630 matching citations\n",
      "Found 49612 matching citations\n",
      "Found 49267 matching citations\n",
      "Found 49848 matching citations\n",
      "Found 49447 matching citations\n",
      "Found 49956 matching citations\n",
      "Found 49567 matching citations\n",
      "Found 49689 matching citations\n",
      "Found 49813 matching citations\n",
      "Found 49907 matching citations\n",
      "Found 49938 matching citations\n",
      "Found 46231 matching citations\n",
      "Combined 1115397 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 1124174 rows\n",
      "File exists with size: 25956927606 bytes\n",
      "Processed 1700000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 3329 matching citations\n",
      "Found 49892 matching citations\n",
      "Found 49923 matching citations\n",
      "Found 49753 matching citations\n",
      "Found 49525 matching citations\n",
      "Found 48528 matching citations\n",
      "Found 49381 matching citations\n",
      "Found 49090 matching citations\n",
      "Found 49665 matching citations\n",
      "Found 49668 matching citations\n",
      "Found 49948 matching citations\n",
      "Found 49944 matching citations\n",
      "Found 49915 matching citations\n",
      "Found 49909 matching citations\n",
      "Found 49839 matching citations\n",
      "Found 49440 matching citations\n",
      "Found 49859 matching citations\n",
      "Found 49964 matching citations\n",
      "Found 49646 matching citations\n",
      "Found 49808 matching citations\n",
      "Found 49875 matching citations\n",
      "Found 49948 matching citations\n",
      "Found 41634 matching citations\n",
      "Combined 1088483 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 1097431 rows\n",
      "File exists with size: 26777189379 bytes\n",
      "Processed 1750000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 8258 matching citations\n",
      "Found 49599 matching citations\n",
      "Found 49581 matching citations\n",
      "Found 49803 matching citations\n",
      "Found 49553 matching citations\n",
      "Found 49392 matching citations\n",
      "Found 49828 matching citations\n",
      "Found 48984 matching citations\n",
      "Found 49428 matching citations\n",
      "Found 49944 matching citations\n",
      "Found 49903 matching citations\n",
      "Found 49736 matching citations\n",
      "Found 49903 matching citations\n",
      "Found 49879 matching citations\n",
      "Found 49969 matching citations\n",
      "Found 49915 matching citations\n",
      "Found 49518 matching citations\n",
      "Found 49874 matching citations\n",
      "Found 49986 matching citations\n",
      "Found 49886 matching citations\n",
      "Found 49799 matching citations\n",
      "Found 41354 matching citations\n",
      "Combined 1044092 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 1053208 rows\n",
      "File exists with size: 27565352497 bytes\n",
      "Processed 1800000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 8240 matching citations\n",
      "Found 49758 matching citations\n",
      "Found 49473 matching citations\n",
      "Found 49915 matching citations\n",
      "Found 49678 matching citations\n",
      "Found 49855 matching citations\n",
      "Found 49942 matching citations\n",
      "Found 49900 matching citations\n",
      "Found 49264 matching citations\n",
      "Found 49984 matching citations\n",
      "Found 49878 matching citations\n",
      "Found 49250 matching citations\n",
      "Found 49995 matching citations\n",
      "Found 49782 matching citations\n",
      "Found 49666 matching citations\n",
      "Found 49925 matching citations\n",
      "Found 49942 matching citations\n",
      "Found 49373 matching citations\n",
      "Found 49979 matching citations\n",
      "Found 49377 matching citations\n",
      "Found 49176 matching citations\n",
      "Found 49727 matching citations\n",
      "Found 27626 matching citations\n",
      "Combined 1079705 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 1089030 rows\n",
      "File exists with size: 28367246514 bytes\n",
      "Processed 1850000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 21915 matching citations\n",
      "Found 49252 matching citations\n",
      "Found 49684 matching citations\n",
      "Found 49921 matching citations\n",
      "Found 49815 matching citations\n",
      "Found 49820 matching citations\n",
      "Found 49584 matching citations\n",
      "Found 49991 matching citations\n",
      "Found 49509 matching citations\n",
      "Found 49661 matching citations\n",
      "Found 49692 matching citations\n",
      "Found 49709 matching citations\n",
      "Found 49619 matching citations\n",
      "Found 49773 matching citations\n",
      "Found 49694 matching citations\n",
      "Found 49339 matching citations\n",
      "Found 49435 matching citations\n",
      "Found 49722 matching citations\n",
      "Found 49448 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49840 matching citations\n",
      "Found 49508 matching citations\n",
      "Found 16090 matching citations\n",
      "Combined 1081021 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 1090683 rows\n",
      "File exists with size: 29178367610 bytes\n",
      "Processed 1900000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 33544 matching citations\n",
      "Found 49872 matching citations\n",
      "Found 49579 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49015 matching citations\n",
      "Found 49167 matching citations\n",
      "Found 49991 matching citations\n",
      "Found 49258 matching citations\n",
      "Found 49770 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49765 matching citations\n",
      "Found 49450 matching citations\n",
      "Found 49937 matching citations\n",
      "Found 49552 matching citations\n",
      "Found 49872 matching citations\n",
      "Found 49337 matching citations\n",
      "Found 49455 matching citations\n",
      "Found 49821 matching citations\n",
      "Found 49527 matching citations\n",
      "Found 49914 matching citations\n",
      "Found 49816 matching citations\n",
      "Found 49961 matching citations\n",
      "Found 30077 matching citations\n",
      "Combined 1106680 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 1116599 rows\n",
      "File exists with size: 30009320790 bytes\n",
      "Processed 1950000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 19755 matching citations\n",
      "Found 49558 matching citations\n",
      "Found 49645 matching citations\n",
      "Found 49865 matching citations\n",
      "Found 49894 matching citations\n",
      "Found 49239 matching citations\n",
      "Found 49989 matching citations\n",
      "Found 49797 matching citations\n",
      "Found 49427 matching citations\n",
      "Found 49723 matching citations\n",
      "Found 49651 matching citations\n",
      "Found 49706 matching citations\n",
      "Found 49403 matching citations\n",
      "Found 49356 matching citations\n",
      "Found 49780 matching citations\n",
      "Found 49600 matching citations\n",
      "Found 49506 matching citations\n",
      "Found 49828 matching citations\n",
      "Found 49750 matching citations\n",
      "Found 49889 matching citations\n",
      "Found 38760 matching citations\n",
      "Combined 1002121 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 1011810 rows\n",
      "File exists with size: 30774506666 bytes\n",
      "Processed 2000000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 10737 matching citations\n",
      "Found 49953 matching citations\n",
      "Found 49707 matching citations\n",
      "Found 48739 matching citations\n",
      "Found 49775 matching citations\n",
      "Found 49722 matching citations\n",
      "Found 49315 matching citations\n",
      "Found 49998 matching citations\n",
      "Found 49595 matching citations\n",
      "Found 49616 matching citations\n",
      "Found 49783 matching citations\n",
      "Found 49957 matching citations\n",
      "Found 49457 matching citations\n",
      "Found 49744 matching citations\n",
      "Found 49917 matching citations\n",
      "Found 49984 matching citations\n",
      "Found 49641 matching citations\n",
      "Found 49923 matching citations\n",
      "Found 49555 matching citations\n",
      "Found 49911 matching citations\n",
      "Found 49769 matching citations\n",
      "Found 794 matching citations\n",
      "Combined 1005592 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 1015335 rows\n",
      "File exists with size: 31533906114 bytes\n",
      "Processed 2050000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 48024 matching citations\n",
      "Found 49466 matching citations\n",
      "Found 49992 matching citations\n",
      "Found 49988 matching citations\n",
      "Found 49259 matching citations\n",
      "Found 49850 matching citations\n",
      "Found 49976 matching citations\n",
      "Found 43847 matching citations\n",
      "Found 49463 matching citations\n",
      "Found 49996 matching citations\n",
      "Found 25947 matching citations\n",
      "Combined 515808 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 521406 rows\n",
      "File exists with size: 31930028013 bytes\n",
      "Processed 2100000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 24046 matching citations\n",
      "Found 49992 matching citations\n",
      "Found 49986 matching citations\n",
      "Found 49995 matching citations\n",
      "Found 49995 matching citations\n",
      "Found 23622 matching citations\n",
      "Combined 247636 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 249007 rows\n",
      "File exists with size: 32121133406 bytes\n",
      "Processed 2150000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 26378 matching citations\n",
      "Found 49999 matching citations\n",
      "Found 49998 matching citations\n",
      "Found 49981 matching citations\n",
      "Found 49986 matching citations\n",
      "Found 27825 matching citations\n",
      "Combined 254167 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 255226 rows\n",
      "File exists with size: 32317590013 bytes\n",
      "Processed 2200000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 22166 matching citations\n",
      "Found 49993 matching citations\n",
      "Found 49999 matching citations\n",
      "Found 49993 matching citations\n",
      "Found 49996 matching citations\n",
      "Found 33010 matching citations\n",
      "Combined 255157 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 256163 rows\n",
      "File exists with size: 32515440111 bytes\n",
      "Processed 2250000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 16988 matching citations\n",
      "Found 49998 matching citations\n",
      "Found 49997 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 40639 matching citations\n",
      "Combined 257622 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 258646 rows\n",
      "File exists with size: 32714354845 bytes\n",
      "Processed 2300000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 9361 matching citations\n",
      "Found 49989 matching citations\n",
      "Found 49999 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49991 matching citations\n",
      "Found 49991 matching citations\n",
      "Found 13663 matching citations\n",
      "Combined 272994 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 273964 rows\n",
      "File exists with size: 32927013581 bytes\n",
      "Processed 2350000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 36337 matching citations\n",
      "Found 49997 matching citations\n",
      "Found 49997 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49969 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 251 matching citations\n",
      "Combined 286551 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 287493 rows\n",
      "File exists with size: 33150641838 bytes\n",
      "Processed 2400000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 49749 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49993 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 47391 matching citations\n",
      "Combined 297133 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 298082 rows\n",
      "File exists with size: 33383023732 bytes\n",
      "Processed 2450000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 2609 matching citations\n",
      "Found 49995 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49988 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49200 matching citations\n",
      "Combined 301792 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 302736 rows\n",
      "File exists with size: 33620282365 bytes\n",
      "Processed 2500000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 800 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49991 matching citations\n",
      "Found 49999 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49987 matching citations\n",
      "Found 5349 matching citations\n",
      "Combined 306126 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 306997 rows\n",
      "File exists with size: 33860670328 bytes\n",
      "Processed 2550000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 44648 matching citations\n",
      "Found 49985 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49940 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49988 matching citations\n",
      "Found 17818 matching citations\n",
      "Combined 312379 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 313365 rows\n",
      "File exists with size: 34106490261 bytes\n",
      "Processed 2600000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 32172 matching citations\n",
      "Found 49995 matching citations\n",
      "Found 49973 matching citations\n",
      "Found 49964 matching citations\n",
      "Found 49984 matching citations\n",
      "Found 49994 matching citations\n",
      "Found 36292 matching citations\n",
      "Combined 318374 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 319399 rows\n",
      "File exists with size: 34358636540 bytes\n",
      "Processed 2650000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 13691 matching citations\n",
      "Found 49993 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49973 matching citations\n",
      "Found 49986 matching citations\n",
      "Found 49996 matching citations\n",
      "Found 15321 matching citations\n",
      "Combined 328960 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 329958 rows\n",
      "File exists with size: 34618163232 bytes\n",
      "Processed 2700000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 34677 matching citations\n",
      "Found 49991 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49995 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49337 matching citations\n",
      "Combined 334000 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 334980 rows\n",
      "File exists with size: 34882576737 bytes\n",
      "Processed 2750000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 650 matching citations\n",
      "Found 49995 matching citations\n",
      "Found 49986 matching citations\n",
      "Found 49992 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49994 matching citations\n",
      "Found 49997 matching citations\n",
      "Found 36982 matching citations\n",
      "Combined 337596 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 338526 rows\n",
      "File exists with size: 35151831767 bytes\n",
      "Processed 2800000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 12995 matching citations\n",
      "Found 49992 matching citations\n",
      "Found 49999 matching citations\n",
      "Found 49923 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49981 matching citations\n",
      "Found 49986 matching citations\n",
      "Found 35775 matching citations\n",
      "Combined 348651 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 349660 rows\n",
      "File exists with size: 35430240308 bytes\n",
      "Processed 2850000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 14215 matching citations\n",
      "Found 49995 matching citations\n",
      "Found 49962 matching citations\n",
      "Found 49961 matching citations\n",
      "Found 49983 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49975 matching citations\n",
      "Found 40841 matching citations\n",
      "Combined 354932 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 355919 rows\n",
      "File exists with size: 35712670278 bytes\n",
      "Processed 2900000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 9142 matching citations\n",
      "Found 49998 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49992 matching citations\n",
      "Found 49995 matching citations\n",
      "Found 49999 matching citations\n",
      "Found 49997 matching citations\n",
      "Found 49992 matching citations\n",
      "Found 5788 matching citations\n",
      "Combined 364903 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 365899 rows\n",
      "File exists with size: 36003644474 bytes\n",
      "Processed 2950000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 44192 matching citations\n",
      "Found 49947 matching citations\n",
      "Found 49959 matching citations\n",
      "Found 49958 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49996 matching citations\n",
      "Found 49997 matching citations\n",
      "Found 33789 matching citations\n",
      "Combined 377838 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 378828 rows\n",
      "File exists with size: 36306005744 bytes\n",
      "Processed 3000000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 16211 matching citations\n",
      "Found 49974 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49997 matching citations\n",
      "Found 49963 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49946 matching citations\n",
      "Found 13408 matching citations\n",
      "Combined 379499 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 380570 rows\n",
      "File exists with size: 36608584023 bytes\n",
      "Processed 3050000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 36556 matching citations\n",
      "Found 49935 matching citations\n",
      "Found 49964 matching citations\n",
      "Found 49912 matching citations\n",
      "Found 49967 matching citations\n",
      "Found 49954 matching citations\n",
      "Found 49983 matching citations\n",
      "Found 42934 matching citations\n",
      "Combined 379205 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 380426 rows\n",
      "File exists with size: 36912814198 bytes\n",
      "Processed 3100000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 7054 matching citations\n",
      "Found 49913 matching citations\n",
      "Found 49936 matching citations\n",
      "Found 49965 matching citations\n",
      "Found 49909 matching citations\n",
      "Found 49978 matching citations\n",
      "Found 49953 matching citations\n",
      "Found 49978 matching citations\n",
      "Found 19107 matching citations\n",
      "Combined 375793 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 377078 rows\n",
      "File exists with size: 37211697170 bytes\n",
      "Processed 3150000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 30860 matching citations\n",
      "Found 49991 matching citations\n",
      "Found 49994 matching citations\n",
      "Found 49955 matching citations\n",
      "Found 49980 matching citations\n",
      "Found 49945 matching citations\n",
      "Found 49973 matching citations\n",
      "Found 49988 matching citations\n",
      "Found 5188 matching citations\n",
      "Combined 385874 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 387016 rows\n",
      "File exists with size: 37521085298 bytes\n",
      "Processed 3200000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 44761 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49977 matching citations\n",
      "Found 49963 matching citations\n",
      "Found 49981 matching citations\n",
      "Found 49991 matching citations\n",
      "Found 49969 matching citations\n",
      "Found 47517 matching citations\n",
      "Combined 392159 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 393358 rows\n",
      "File exists with size: 37836470674 bytes\n",
      "Processed 3250000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 2477 matching citations\n",
      "Found 49971 matching citations\n",
      "Found 49964 matching citations\n",
      "Found 49993 matching citations\n",
      "Found 49954 matching citations\n",
      "Found 49985 matching citations\n",
      "Found 49954 matching citations\n",
      "Found 49945 matching citations\n",
      "Found 49998 matching citations\n",
      "Found 83 matching citations\n",
      "Combined 402324 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 403479 rows\n",
      "File exists with size: 38163012460 bytes\n",
      "Processed 3300000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 49908 matching citations\n",
      "Found 49993 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49984 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49911 matching citations\n",
      "Found 49839 matching citations\n",
      "Found 49888 matching citations\n",
      "Found 8697 matching citations\n",
      "Combined 408220 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 409428 rows\n",
      "File exists with size: 38493983351 bytes\n",
      "Processed 3350000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 41303 matching citations\n",
      "Found 49999 matching citations\n",
      "Found 49996 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49999 matching citations\n",
      "Found 49992 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 27412 matching citations\n",
      "Combined 418701 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 419907 rows\n",
      "File exists with size: 38835418558 bytes\n",
      "Processed 3400000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 22588 matching citations\n",
      "Found 49995 matching citations\n",
      "Found 49998 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49973 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49991 matching citations\n",
      "Found 49995 matching citations\n",
      "Found 49994 matching citations\n",
      "Found 4280 matching citations\n",
      "Combined 426814 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 427890 rows\n",
      "File exists with size: 39184951449 bytes\n",
      "Processed 3450000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 45720 matching citations\n",
      "Found 49998 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49942 matching citations\n",
      "Found 49958 matching citations\n",
      "Found 49961 matching citations\n",
      "Found 49988 matching citations\n",
      "Found 49979 matching citations\n",
      "Found 49987 matching citations\n",
      "Found 3742 matching citations\n",
      "Combined 449275 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 450335 rows\n",
      "File exists with size: 39557414204 bytes\n",
      "Processed 3500000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 46253 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49984 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49978 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49995 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 20360 matching citations\n",
      "Combined 466570 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 467511 rows\n",
      "File exists with size: 39944509960 bytes\n",
      "Processed 3550000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 29640 matching citations\n",
      "Found 49862 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49995 matching citations\n",
      "Found 49981 matching citations\n",
      "Found 45448 matching citations\n",
      "Combined 474926 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 475934 rows\n",
      "File exists with size: 40345654636 bytes\n",
      "Processed 3600000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 4552 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49997 matching citations\n",
      "Found 49999 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49978 matching citations\n",
      "Found 49996 matching citations\n",
      "Found 49986 matching citations\n",
      "Found 49993 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 22512 matching citations\n",
      "Combined 477013 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 478096 rows\n",
      "File exists with size: 40749406543 bytes\n",
      "Processed 3650000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 27488 matching citations\n",
      "Found 49969 matching citations\n",
      "Found 49975 matching citations\n",
      "Found 49968 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49988 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49994 matching citations\n",
      "Found 19410 matching citations\n",
      "Combined 496792 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 497941 rows\n",
      "File exists with size: 41171595058 bytes\n",
      "Processed 3700000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 30590 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49990 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49987 matching citations\n",
      "Found 49988 matching citations\n",
      "Found 28554 matching citations\n",
      "Combined 509109 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 510391 rows\n",
      "File exists with size: 41605872865 bytes\n",
      "Processed 3750000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 21432 matching citations\n",
      "Found 49979 matching citations\n",
      "Found 49930 matching citations\n",
      "Found 49996 matching citations\n",
      "Found 49984 matching citations\n",
      "Found 49971 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49960 matching citations\n",
      "Found 49988 matching citations\n",
      "Found 49997 matching citations\n",
      "Found 48501 matching citations\n",
      "Combined 519738 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 521167 rows\n",
      "File exists with size: 42051544604 bytes\n",
      "Processed 3800000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 1499 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49991 matching citations\n",
      "Found 49996 matching citations\n",
      "Found 49948 matching citations\n",
      "Found 49994 matching citations\n",
      "Found 49986 matching citations\n",
      "Found 49725 matching citations\n",
      "Found 49943 matching citations\n",
      "Found 49831 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 17863 matching citations\n",
      "Combined 518776 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 520268 rows\n",
      "File exists with size: 42496775605 bytes\n",
      "Processed 3850000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 32094 matching citations\n",
      "Found 49956 matching citations\n",
      "Found 49984 matching citations\n",
      "Found 49952 matching citations\n",
      "Found 49999 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49921 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49994 matching citations\n",
      "Found 33798 matching citations\n",
      "Combined 515698 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 517113 rows\n",
      "File exists with size: 42942367870 bytes\n",
      "Processed 3900000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 16145 matching citations\n",
      "Found 49976 matching citations\n",
      "Found 49990 matching citations\n",
      "Found 49981 matching citations\n",
      "Found 49992 matching citations\n",
      "Found 49977 matching citations\n",
      "Found 49981 matching citations\n",
      "Found 49974 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49973 matching citations\n",
      "Found 49998 matching citations\n",
      "Found 16265 matching citations\n",
      "Combined 532252 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 533605 rows\n",
      "File exists with size: 43414383572 bytes\n",
      "Processed 3950000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 33655 matching citations\n",
      "Found 49961 matching citations\n",
      "Found 49897 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49982 matching citations\n",
      "Found 49981 matching citations\n",
      "Found 49995 matching citations\n",
      "Found 49926 matching citations\n",
      "Found 49988 matching citations\n",
      "Found 49979 matching citations\n",
      "Found 49924 matching citations\n",
      "Found 2930 matching citations\n",
      "Combined 536218 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 537891 rows\n",
      "File exists with size: 43876949243 bytes\n",
      "Processed 4000000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 47010 matching citations\n",
      "Found 49889 matching citations\n",
      "Found 49965 matching citations\n",
      "Found 49805 matching citations\n",
      "Found 49922 matching citations\n",
      "Found 49978 matching citations\n",
      "Found 49976 matching citations\n",
      "Found 49900 matching citations\n",
      "Found 49911 matching citations\n",
      "Found 49990 matching citations\n",
      "Found 39463 matching citations\n",
      "Combined 535809 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 537202 rows\n",
      "File exists with size: 44342025853 bytes\n",
      "Processed 4050000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 10510 matching citations\n",
      "Found 49943 matching citations\n",
      "Found 49980 matching citations\n",
      "Found 49994 matching citations\n",
      "Found 49991 matching citations\n",
      "Found 49985 matching citations\n",
      "Found 49991 matching citations\n",
      "Found 49998 matching citations\n",
      "Found 49986 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49944 matching citations\n",
      "Found 31522 matching citations\n",
      "Combined 541844 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 543200 rows\n",
      "File exists with size: 44810948447 bytes\n",
      "Processed 4100000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 18432 matching citations\n",
      "Found 49914 matching citations\n",
      "Found 49967 matching citations\n",
      "Found 49973 matching citations\n",
      "Found 49876 matching citations\n",
      "Found 49998 matching citations\n",
      "Found 49987 matching citations\n",
      "Found 49892 matching citations\n",
      "Found 49987 matching citations\n",
      "Found 49958 matching citations\n",
      "Found 49963 matching citations\n",
      "Found 33324 matching citations\n",
      "Combined 551271 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 552944 rows\n",
      "File exists with size: 45289033754 bytes\n",
      "Processed 4150000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 16400 matching citations\n",
      "Found 49940 matching citations\n",
      "Found 49887 matching citations\n",
      "Found 49899 matching citations\n",
      "Found 49951 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49970 matching citations\n",
      "Found 49912 matching citations\n",
      "Found 49998 matching citations\n",
      "Found 49870 matching citations\n",
      "Found 49962 matching citations\n",
      "Found 40129 matching citations\n",
      "Combined 555918 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 557183 rows\n",
      "File exists with size: 45774145469 bytes\n",
      "Processed 4200000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 9838 matching citations\n",
      "Found 49953 matching citations\n",
      "Found 49994 matching citations\n",
      "Found 49992 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49900 matching citations\n",
      "Found 49915 matching citations\n",
      "Found 49988 matching citations\n",
      "Found 49637 matching citations\n",
      "Found 49997 matching citations\n",
      "Found 49999 matching citations\n",
      "Found 49989 matching citations\n",
      "Found 16994 matching citations\n",
      "Combined 576196 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 577444 rows\n",
      "File exists with size: 46276430334 bytes\n",
      "Processed 4250000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 32941 matching citations\n",
      "Found 49986 matching citations\n",
      "Found 49982 matching citations\n",
      "Found 49999 matching citations\n",
      "Found 49919 matching citations\n",
      "Found 49987 matching citations\n",
      "Found 49996 matching citations\n",
      "Found 49963 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49993 matching citations\n",
      "Found 49993 matching citations\n",
      "Found 49984 matching citations\n",
      "Found 92 matching citations\n",
      "Combined 582835 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 584087 rows\n",
      "File exists with size: 46785556902 bytes\n",
      "Processed 4300000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 49877 matching citations\n",
      "Found 49931 matching citations\n",
      "Found 49929 matching citations\n",
      "Found 49975 matching citations\n",
      "Found 49979 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49931 matching citations\n",
      "Found 49937 matching citations\n",
      "Found 49914 matching citations\n",
      "Found 49989 matching citations\n",
      "Found 49920 matching citations\n",
      "Found 29252 matching citations\n",
      "Combined 578634 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 579829 rows\n",
      "File exists with size: 47283947126 bytes\n",
      "Processed 4350000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 20724 matching citations\n",
      "Found 49851 matching citations\n",
      "Found 49943 matching citations\n",
      "Found 49989 matching citations\n",
      "Found 49928 matching citations\n",
      "Found 49955 matching citations\n",
      "Found 49992 matching citations\n",
      "Found 49928 matching citations\n",
      "Found 49966 matching citations\n",
      "Found 49952 matching citations\n",
      "Found 49995 matching citations\n",
      "Found 49991 matching citations\n",
      "Found 32054 matching citations\n",
      "Combined 602268 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 603512 rows\n",
      "File exists with size: 47804328261 bytes\n",
      "Processed 4400000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 17935 matching citations\n",
      "Found 49961 matching citations\n",
      "Found 49984 matching citations\n",
      "Found 49944 matching citations\n",
      "Found 49981 matching citations\n",
      "Found 49946 matching citations\n",
      "Found 49940 matching citations\n",
      "Found 49977 matching citations\n",
      "Found 49909 matching citations\n",
      "Found 49967 matching citations\n",
      "Found 49962 matching citations\n",
      "Found 49929 matching citations\n",
      "Found 35297 matching citations\n",
      "Combined 602732 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 603968 rows\n",
      "File exists with size: 48324528520 bytes\n",
      "Processed 4450000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 14551 matching citations\n",
      "Found 49935 matching citations\n",
      "Found 49925 matching citations\n",
      "Found 49716 matching citations\n",
      "Found 49899 matching citations\n",
      "Found 49636 matching citations\n",
      "Found 49828 matching citations\n",
      "Found 49888 matching citations\n",
      "Found 49930 matching citations\n",
      "Found 49685 matching citations\n",
      "Found 49948 matching citations\n",
      "Found 49962 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 41295 matching citations\n",
      "Combined 704198 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 705441 rows\n",
      "File exists with size: 48927499200 bytes\n",
      "Processed 4500000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 8699 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49959 matching citations\n",
      "Found 49960 matching citations\n",
      "Found 49996 matching citations\n",
      "Found 49993 matching citations\n",
      "Found 49887 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49402 matching citations\n",
      "Found 49916 matching citations\n",
      "Found 3320 matching citations\n",
      "Combined 611132 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 612236 rows\n",
      "File exists with size: 49446955113 bytes\n",
      "Processed 4550000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 46680 matching citations\n",
      "Found 49984 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49828 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49990 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49939 matching citations\n",
      "Found 49997 matching citations\n",
      "Found 49969 matching citations\n",
      "Found 27383 matching citations\n",
      "Combined 623770 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 624885 rows\n",
      "File exists with size: 49967612497 bytes\n",
      "Processed 4600000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 22607 matching citations\n",
      "Found 49987 matching citations\n",
      "Found 49995 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49895 matching citations\n",
      "Found 49971 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49986 matching citations\n",
      "Found 49995 matching citations\n",
      "Found 49994 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49951 matching citations\n",
      "Found 49979 matching citations\n",
      "Found 17683 matching citations\n",
      "Combined 640043 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 641125 rows\n",
      "File exists with size: 50492527407 bytes\n",
      "Processed 4650000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 32305 matching citations\n",
      "Found 49958 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49998 matching citations\n",
      "Found 49999 matching citations\n",
      "Found 49952 matching citations\n",
      "Found 49985 matching citations\n",
      "Found 49928 matching citations\n",
      "Found 49960 matching citations\n",
      "Found 49968 matching citations\n",
      "Found 49992 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 28888 matching citations\n",
      "Combined 660933 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 661938 rows\n",
      "File exists with size: 51028985423 bytes\n",
      "Processed 4700000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 21062 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49980 matching citations\n",
      "Found 49999 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 16918 matching citations\n",
      "Combined 687959 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 688816 rows\n",
      "File exists with size: 51583682781 bytes\n",
      "Processed 4750000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 33082 matching citations\n",
      "Found 49988 matching citations\n",
      "Found 49997 matching citations\n",
      "Found 49967 matching citations\n",
      "Found 49998 matching citations\n",
      "Found 49975 matching citations\n",
      "Found 49680 matching citations\n",
      "Found 49957 matching citations\n",
      "Found 49972 matching citations\n",
      "Found 49967 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49996 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 41002 matching citations\n",
      "Combined 673581 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 674487 rows\n",
      "File exists with size: 52123396051 bytes\n",
      "Processed 4800000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 8998 matching citations\n",
      "Found 49991 matching citations\n",
      "Found 49974 matching citations\n",
      "Found 49978 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49991 matching citations\n",
      "Found 49978 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49997 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49992 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49974 matching citations\n",
      "Found 1520 matching citations\n",
      "Combined 710393 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 711232 rows\n",
      "File exists with size: 52688934769 bytes\n",
      "Processed 4850000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 48369 matching citations\n",
      "Found 49983 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49938 matching citations\n",
      "Found 49955 matching citations\n",
      "Found 49918 matching citations\n",
      "Found 49977 matching citations\n",
      "Found 49985 matching citations\n",
      "Found 49988 matching citations\n",
      "Found 49917 matching citations\n",
      "Found 49708 matching citations\n",
      "Found 49940 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49992 matching citations\n",
      "Found 13278 matching citations\n",
      "Combined 710948 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 711707 rows\n",
      "File exists with size: 53254824182 bytes\n",
      "Processed 4900000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 36708 matching citations\n",
      "Found 49976 matching citations\n",
      "Found 49812 matching citations\n",
      "Found 49992 matching citations\n",
      "Found 49989 matching citations\n",
      "Found 49987 matching citations\n",
      "Found 49901 matching citations\n",
      "Found 49962 matching citations\n",
      "Found 49664 matching citations\n",
      "Found 49948 matching citations\n",
      "Found 49960 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49971 matching citations\n",
      "Found 49977 matching citations\n",
      "Found 3238 matching citations\n",
      "Combined 689085 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 689887 rows\n",
      "File exists with size: 53800301342 bytes\n",
      "Processed 4950000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 46710 matching citations\n",
      "Found 49976 matching citations\n",
      "Found 49829 matching citations\n",
      "Found 49974 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49999 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49940 matching citations\n",
      "Found 49764 matching citations\n",
      "Found 49975 matching citations\n",
      "Found 49842 matching citations\n",
      "Found 49945 matching citations\n",
      "Found 49982 matching citations\n",
      "Found 49902 matching citations\n",
      "Found 32588 matching citations\n",
      "Combined 728426 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 729320 rows\n",
      "File exists with size: 54369081753 bytes\n",
      "Processed 5000000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 17403 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49981 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49997 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49937 matching citations\n",
      "Found 49949 matching citations\n",
      "Found 49987 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49990 matching citations\n",
      "Found 49961 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 34870 matching citations\n",
      "Combined 752075 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 752994 rows\n",
      "File exists with size: 54955807184 bytes\n",
      "Processed 5050000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 15055 matching citations\n",
      "Found 49992 matching citations\n",
      "Found 49926 matching citations\n",
      "Found 49999 matching citations\n",
      "Found 49992 matching citations\n",
      "Found 49826 matching citations\n",
      "Found 49966 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49948 matching citations\n",
      "Found 49888 matching citations\n",
      "Found 49839 matching citations\n",
      "Found 49952 matching citations\n",
      "Found 49963 matching citations\n",
      "Found 49995 matching citations\n",
      "Found 49928 matching citations\n",
      "Found 49925 matching citations\n",
      "Found 855 matching citations\n",
      "Combined 765049 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 765962 rows\n",
      "File exists with size: 55556591331 bytes\n",
      "Processed 5100000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 49140 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49926 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49991 matching citations\n",
      "Found 49987 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49991 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49937 matching citations\n",
      "Found 3889 matching citations\n",
      "Combined 802861 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 803756 rows\n",
      "File exists with size: 56178247774 bytes\n",
      "Processed 5150000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 46108 matching citations\n",
      "Found 49989 matching citations\n",
      "Found 49853 matching citations\n",
      "Found 49987 matching citations\n",
      "Found 49978 matching citations\n",
      "Found 49911 matching citations\n",
      "Found 49979 matching citations\n",
      "Found 49966 matching citations\n",
      "Found 49990 matching citations\n",
      "Found 49987 matching citations\n",
      "Found 49888 matching citations\n",
      "Found 49993 matching citations\n",
      "Found 49942 matching citations\n",
      "Found 49780 matching citations\n",
      "Found 49910 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 3393 matching citations\n",
      "Combined 798654 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 799851 rows\n",
      "File exists with size: 56792997487 bytes\n",
      "Processed 5200000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 46607 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49970 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49999 matching citations\n",
      "Found 49989 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 43839 matching citations\n",
      "Combined 840404 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 841581 rows\n",
      "File exists with size: 57435888431 bytes\n",
      "Processed 5250000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 6161 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49998 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49952 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49996 matching citations\n",
      "Found 49962 matching citations\n",
      "Found 49979 matching citations\n",
      "Found 49965 matching citations\n",
      "Found 49999 matching citations\n",
      "Found 49941 matching citations\n",
      "Found 49924 matching citations\n",
      "Found 11554 matching citations\n",
      "Combined 817431 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 818637 rows\n",
      "File exists with size: 58055955632 bytes\n",
      "Processed 5300000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 38433 matching citations\n",
      "Found 49991 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49981 matching citations\n",
      "Found 49718 matching citations\n",
      "Found 49805 matching citations\n",
      "Found 49994 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49727 matching citations\n",
      "Found 49998 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 35343 matching citations\n",
      "Combined 822990 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 824320 rows\n",
      "File exists with size: 58680401629 bytes\n",
      "Processed 5350000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 14657 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49970 matching citations\n",
      "Found 49950 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49992 matching citations\n",
      "Found 1494 matching citations\n",
      "Combined 816063 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 817515 rows\n",
      "File exists with size: 59302025202 bytes\n",
      "Processed 5400000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 48506 matching citations\n",
      "Found 49952 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49972 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49924 matching citations\n",
      "Found 49956 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49991 matching citations\n",
      "Found 49902 matching citations\n",
      "Found 49970 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 46788 matching citations\n",
      "Combined 844961 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 846461 rows\n",
      "File exists with size: 59937588588 bytes\n",
      "Processed 5450000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 3212 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49998 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49760 matching citations\n",
      "Found 49930 matching citations\n",
      "Found 19068 matching citations\n",
      "Combined 821968 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 823501 rows\n",
      "File exists with size: 60555976754 bytes\n",
      "Processed 5500000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 30892 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 3306 matching citations\n",
      "Combined 834198 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 835846 rows\n",
      "File exists with size: 61184838491 bytes\n",
      "Processed 5550000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 46694 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49716 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49974 matching citations\n",
      "Found 49997 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49970 matching citations\n",
      "Found 2478 matching citations\n",
      "Combined 848829 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 850463 rows\n",
      "File exists with size: 61822365116 bytes\n",
      "Processed 5600000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 47387 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49873 matching citations\n",
      "Found 49918 matching citations\n",
      "Found 49277 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49933 matching citations\n",
      "Found 49993 matching citations\n",
      "Found 49904 matching citations\n",
      "Found 49671 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49963 matching citations\n",
      "Found 49977 matching citations\n",
      "Found 49969 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 39064 matching citations\n",
      "Combined 884929 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 886765 rows\n",
      "File exists with size: 62484410295 bytes\n",
      "Processed 5650000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 10936 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49976 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49975 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49701 matching citations\n",
      "Found 49858 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49982 matching citations\n",
      "Found 20664 matching citations\n",
      "Combined 881092 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 883004 rows\n",
      "File exists with size: 63144616540 bytes\n",
      "Processed 5700000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 29335 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49905 matching citations\n",
      "Found 49985 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49982 matching citations\n",
      "Found 49805 matching citations\n",
      "Found 49996 matching citations\n",
      "Found 49988 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49721 matching citations\n",
      "Found 49728 matching citations\n",
      "Found 49940 matching citations\n",
      "Found 49973 matching citations\n",
      "Found 49758 matching citations\n",
      "Found 49944 matching citations\n",
      "Found 49888 matching citations\n",
      "Found 1602 matching citations\n",
      "Combined 929550 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 931566 rows\n",
      "File exists with size: 63839480498 bytes\n",
      "Processed 5750000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 48121 matching citations\n",
      "Found 49940 matching citations\n",
      "Found 49637 matching citations\n",
      "Found 49941 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49973 matching citations\n",
      "Found 49967 matching citations\n",
      "Found 49945 matching citations\n",
      "Found 49947 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49993 matching citations\n",
      "Found 49917 matching citations\n",
      "Found 49744 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49996 matching citations\n",
      "Found 49877 matching citations\n",
      "Found 49982 matching citations\n",
      "Found 49988 matching citations\n",
      "Found 33767 matching citations\n",
      "Combined 930735 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 932842 rows\n",
      "File exists with size: 64532957690 bytes\n",
      "Processed 5800000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 16203 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49898 matching citations\n",
      "Found 49760 matching citations\n",
      "Found 49908 matching citations\n",
      "Found 49986 matching citations\n",
      "Found 49934 matching citations\n",
      "Found 49618 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49996 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 9796 matching citations\n",
      "Combined 925099 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 927221 rows\n",
      "File exists with size: 65215170803 bytes\n",
      "Processed 5850000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 40188 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49991 matching citations\n",
      "Found 49937 matching citations\n",
      "Found 49631 matching citations\n",
      "Found 49499 matching citations\n",
      "Found 49874 matching citations\n",
      "Found 49869 matching citations\n",
      "Found 49968 matching citations\n",
      "Found 49997 matching citations\n",
      "Found 49774 matching citations\n",
      "Found 49914 matching citations\n",
      "Found 49981 matching citations\n",
      "Found 49967 matching citations\n",
      "Found 49964 matching citations\n",
      "Found 49989 matching citations\n",
      "Found 49921 matching citations\n",
      "Found 49633 matching citations\n",
      "Found 41803 matching citations\n",
      "Combined 979900 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 982066 rows\n",
      "File exists with size: 65943270381 bytes\n",
      "Processed 5900000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 8124 matching citations\n",
      "Found 49931 matching citations\n",
      "Found 49995 matching citations\n",
      "Found 49867 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49855 matching citations\n",
      "Found 49889 matching citations\n",
      "Found 49954 matching citations\n",
      "Found 49896 matching citations\n",
      "Found 49889 matching citations\n",
      "Found 49806 matching citations\n",
      "Found 49968 matching citations\n",
      "Found 49992 matching citations\n",
      "Found 49535 matching citations\n",
      "Found 49913 matching citations\n",
      "Found 49852 matching citations\n",
      "Found 49971 matching citations\n",
      "Found 49907 matching citations\n",
      "Found 49837 matching citations\n",
      "Found 49830 matching citations\n",
      "Found 27679 matching citations\n",
      "Combined 983690 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 985995 rows\n",
      "File exists with size: 66687671247 bytes\n",
      "Processed 5950000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 21863 matching citations\n",
      "Found 49876 matching citations\n",
      "Found 49933 matching citations\n",
      "Found 49992 matching citations\n",
      "Found 49654 matching citations\n",
      "Found 49872 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49997 matching citations\n",
      "Found 49884 matching citations\n",
      "Found 49935 matching citations\n",
      "Found 49872 matching citations\n",
      "Found 49772 matching citations\n",
      "Found 49896 matching citations\n",
      "Found 49994 matching citations\n",
      "Found 49982 matching citations\n",
      "Found 49979 matching citations\n",
      "Found 49862 matching citations\n",
      "Found 49839 matching citations\n",
      "Found 49972 matching citations\n",
      "Found 49897 matching citations\n",
      "Found 32174 matching citations\n",
      "Combined 1002245 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 1004636 rows\n",
      "File exists with size: 67442946713 bytes\n",
      "Processed 6000000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 17171 matching citations\n",
      "Found 49970 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49983 matching citations\n",
      "Found 49138 matching citations\n",
      "Found 49979 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49941 matching citations\n",
      "Found 49770 matching citations\n",
      "Found 49927 matching citations\n",
      "Found 49679 matching citations\n",
      "Found 49375 matching citations\n",
      "Found 49607 matching citations\n",
      "Found 49712 matching citations\n",
      "Found 49978 matching citations\n",
      "Found 49993 matching citations\n",
      "Found 49868 matching citations\n",
      "Found 49302 matching citations\n",
      "Found 49989 matching citations\n",
      "Found 49903 matching citations\n",
      "Found 49992 matching citations\n",
      "Found 9791 matching citations\n",
      "Combined 1023068 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 1025545 rows\n",
      "File exists with size: 68206651448 bytes\n",
      "Processed 6050000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 40181 matching citations\n",
      "Found 48937 matching citations\n",
      "Found 49598 matching citations\n",
      "Found 48277 matching citations\n",
      "Found 49862 matching citations\n",
      "Found 49926 matching citations\n",
      "Found 49758 matching citations\n",
      "Found 49528 matching citations\n",
      "Found 49956 matching citations\n",
      "Found 49780 matching citations\n",
      "Found 49965 matching citations\n",
      "Found 49948 matching citations\n",
      "Found 49963 matching citations\n",
      "Found 49474 matching citations\n",
      "Found 49834 matching citations\n",
      "Found 49699 matching citations\n",
      "Found 49580 matching citations\n",
      "Found 49534 matching citations\n",
      "Found 49572 matching citations\n",
      "Found 49873 matching citations\n",
      "Found 28014 matching citations\n",
      "Combined 1011259 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 1013852 rows\n",
      "File exists with size: 68956972154 bytes\n",
      "Processed 6100000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 21900 matching citations\n",
      "Found 49993 matching citations\n",
      "Found 49799 matching citations\n",
      "Found 49983 matching citations\n",
      "Found 49657 matching citations\n",
      "Found 49895 matching citations\n",
      "Found 49969 matching citations\n",
      "Found 49851 matching citations\n",
      "Found 49641 matching citations\n",
      "Found 49949 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49838 matching citations\n",
      "Found 49922 matching citations\n",
      "Found 49974 matching citations\n",
      "Found 49653 matching citations\n",
      "Found 49968 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49609 matching citations\n",
      "Found 49896 matching citations\n",
      "Found 49976 matching citations\n",
      "Found 29359 matching citations\n",
      "Combined 998832 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 1001677 rows\n",
      "File exists with size: 69691936234 bytes\n",
      "Processed 6150000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 20629 matching citations\n",
      "Found 49959 matching citations\n",
      "Found 49927 matching citations\n",
      "Found 49906 matching citations\n",
      "Found 49964 matching citations\n",
      "Found 49927 matching citations\n",
      "Found 49989 matching citations\n",
      "Found 49941 matching citations\n",
      "Found 49813 matching citations\n",
      "Found 49967 matching citations\n",
      "Found 49869 matching citations\n",
      "Found 49925 matching citations\n",
      "Found 49968 matching citations\n",
      "Found 49974 matching citations\n",
      "Found 49120 matching citations\n",
      "Found 49825 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49987 matching citations\n",
      "Found 49992 matching citations\n",
      "Found 49829 matching citations\n",
      "Found 49558 matching citations\n",
      "Found 13451 matching citations\n",
      "Combined 1031520 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 1034398 rows\n",
      "File exists with size: 70454955900 bytes\n",
      "Processed 6200000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 36326 matching citations\n",
      "Found 49699 matching citations\n",
      "Found 49924 matching citations\n",
      "Found 49865 matching citations\n",
      "Found 49977 matching citations\n",
      "Found 49889 matching citations\n",
      "Found 49780 matching citations\n",
      "Found 49985 matching citations\n",
      "Found 49990 matching citations\n",
      "Found 49921 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49810 matching citations\n",
      "Found 49988 matching citations\n",
      "Found 49740 matching citations\n",
      "Found 49977 matching citations\n",
      "Found 49908 matching citations\n",
      "Found 49958 matching citations\n",
      "Found 49930 matching citations\n",
      "Found 49467 matching citations\n",
      "Found 49241 matching citations\n",
      "Found 16223 matching citations\n",
      "Combined 999598 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 1002581 rows\n",
      "File exists with size: 71194161524 bytes\n",
      "Processed 6250000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 33774 matching citations\n",
      "Found 49947 matching citations\n",
      "Found 49571 matching citations\n",
      "Found 49924 matching citations\n",
      "Found 49989 matching citations\n",
      "Found 49975 matching citations\n",
      "Found 49908 matching citations\n",
      "Found 49722 matching citations\n",
      "Found 49872 matching citations\n",
      "Found 49769 matching citations\n",
      "Found 49775 matching citations\n",
      "Found 49977 matching citations\n",
      "Found 49954 matching citations\n",
      "Found 48349 matching citations\n",
      "Found 49946 matching citations\n",
      "Found 49270 matching citations\n",
      "Found 49715 matching citations\n",
      "Found 49956 matching citations\n",
      "Found 49763 matching citations\n",
      "Found 49975 matching citations\n",
      "Found 1127 matching citations\n",
      "Combined 980258 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 983317 rows\n",
      "File exists with size: 71918885416 bytes\n",
      "Processed 6300000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 48859 matching citations\n",
      "Found 49897 matching citations\n",
      "Found 49901 matching citations\n",
      "Found 49836 matching citations\n",
      "Found 49863 matching citations\n",
      "Found 49816 matching citations\n",
      "Found 49914 matching citations\n",
      "Found 48387 matching citations\n",
      "Found 49882 matching citations\n",
      "Found 49877 matching citations\n",
      "Found 49993 matching citations\n",
      "Found 49869 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49543 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49950 matching citations\n",
      "Found 49867 matching citations\n",
      "Found 49671 matching citations\n",
      "Found 49771 matching citations\n",
      "Found 46784 matching citations\n",
      "Combined 991680 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 995132 rows\n",
      "File exists with size: 72655952207 bytes\n",
      "Processed 6350000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 3200 matching citations\n",
      "Found 49488 matching citations\n",
      "Found 49947 matching citations\n",
      "Found 49815 matching citations\n",
      "Found 49796 matching citations\n",
      "Found 49542 matching citations\n",
      "Found 49942 matching citations\n",
      "Found 49922 matching citations\n",
      "Found 49947 matching citations\n",
      "Found 49956 matching citations\n",
      "Found 49990 matching citations\n",
      "Found 49987 matching citations\n",
      "Found 49977 matching citations\n",
      "Found 49964 matching citations\n",
      "Found 49987 matching citations\n",
      "Found 48497 matching citations\n",
      "Found 49975 matching citations\n",
      "Found 49202 matching citations\n",
      "Found 49946 matching citations\n",
      "Found 48992 matching citations\n",
      "Found 40555 matching citations\n",
      "Combined 988627 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 992324 rows\n",
      "File exists with size: 73385319027 bytes\n",
      "Processed 6400000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 9027 matching citations\n",
      "Found 49957 matching citations\n",
      "Found 49980 matching citations\n",
      "Found 49914 matching citations\n",
      "Found 49819 matching citations\n",
      "Found 49926 matching citations\n",
      "Found 49986 matching citations\n",
      "Found 49983 matching citations\n",
      "Found 49946 matching citations\n",
      "Found 49533 matching citations\n",
      "Found 49950 matching citations\n",
      "Found 49973 matching citations\n",
      "Found 49739 matching citations\n",
      "Found 49954 matching citations\n",
      "Found 49781 matching citations\n",
      "Found 49737 matching citations\n",
      "Found 49705 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49934 matching citations\n",
      "Found 49992 matching citations\n",
      "Found 24157 matching citations\n",
      "Combined 980993 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 984622 rows\n",
      "File exists with size: 74107609720 bytes\n",
      "Processed 6450000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 25838 matching citations\n",
      "Found 49918 matching citations\n",
      "Found 49631 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49821 matching citations\n",
      "Found 49347 matching citations\n",
      "Found 49881 matching citations\n",
      "Found 49504 matching citations\n",
      "Found 49986 matching citations\n",
      "Found 49958 matching citations\n",
      "Found 49939 matching citations\n",
      "Found 49949 matching citations\n",
      "Found 49275 matching citations\n",
      "Found 49894 matching citations\n",
      "Found 49490 matching citations\n",
      "Found 49876 matching citations\n",
      "Found 49909 matching citations\n",
      "Found 49950 matching citations\n",
      "Found 49854 matching citations\n",
      "Found 49920 matching citations\n",
      "Found 11965 matching citations\n",
      "Combined 983905 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 987714 rows\n",
      "File exists with size: 74839683665 bytes\n",
      "Processed 6500000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 38027 matching citations\n",
      "Found 49863 matching citations\n",
      "Found 49974 matching citations\n",
      "Found 49629 matching citations\n",
      "Found 49990 matching citations\n",
      "Found 49953 matching citations\n",
      "Found 49954 matching citations\n",
      "Found 49581 matching citations\n",
      "Found 49858 matching citations\n",
      "Found 49925 matching citations\n",
      "Found 49886 matching citations\n",
      "Found 49913 matching citations\n",
      "Found 49870 matching citations\n",
      "Found 49975 matching citations\n",
      "Found 49566 matching citations\n",
      "Found 49921 matching citations\n",
      "Found 49119 matching citations\n",
      "Found 49790 matching citations\n",
      "Found 49730 matching citations\n",
      "Found 37015 matching citations\n",
      "Combined 971539 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 975565 rows\n",
      "File exists with size: 75581375122 bytes\n",
      "Processed 6550000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 12866 matching citations\n",
      "Found 49929 matching citations\n",
      "Found 49784 matching citations\n",
      "Found 49911 matching citations\n",
      "Found 49958 matching citations\n",
      "Found 49868 matching citations\n",
      "Found 49931 matching citations\n",
      "Found 49823 matching citations\n",
      "Found 49793 matching citations\n",
      "Found 49841 matching citations\n",
      "Found 49224 matching citations\n",
      "Found 49962 matching citations\n",
      "Found 49944 matching citations\n",
      "Found 49880 matching citations\n",
      "Found 49548 matching citations\n",
      "Found 49723 matching citations\n",
      "Found 49404 matching citations\n",
      "Found 49744 matching citations\n",
      "Found 49925 matching citations\n",
      "Found 49273 matching citations\n",
      "Found 43225 matching citations\n",
      "Combined 1001556 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 1005461 rows\n",
      "File exists with size: 76334188290 bytes\n",
      "Processed 6600000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 6740 matching citations\n",
      "Found 48758 matching citations\n",
      "Found 49746 matching citations\n",
      "Found 49782 matching citations\n",
      "Found 49770 matching citations\n",
      "Found 49808 matching citations\n",
      "Found 49975 matching citations\n",
      "Found 49949 matching citations\n",
      "Found 49972 matching citations\n",
      "Found 49548 matching citations\n",
      "Found 49722 matching citations\n",
      "Found 49828 matching citations\n",
      "Found 49811 matching citations\n",
      "Found 49970 matching citations\n",
      "Found 49950 matching citations\n",
      "Found 49959 matching citations\n",
      "Found 49939 matching citations\n",
      "Found 49966 matching citations\n",
      "Found 49642 matching citations\n",
      "Found 48639 matching citations\n",
      "Found 30983 matching citations\n",
      "Combined 982457 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 986490 rows\n",
      "File exists with size: 77073770116 bytes\n",
      "Processed 6650000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 18937 matching citations\n",
      "Found 49839 matching citations\n",
      "Found 49967 matching citations\n",
      "Found 49926 matching citations\n",
      "Found 49991 matching citations\n",
      "Found 49653 matching citations\n",
      "Found 49751 matching citations\n",
      "Found 49588 matching citations\n",
      "Found 49961 matching citations\n",
      "Found 49798 matching citations\n",
      "Found 49993 matching citations\n",
      "Found 49909 matching citations\n",
      "Found 49956 matching citations\n",
      "Found 49852 matching citations\n",
      "Found 49897 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49873 matching citations\n",
      "Found 49949 matching citations\n",
      "Found 49487 matching citations\n",
      "Found 49435 matching citations\n",
      "Found 49828 matching citations\n",
      "Found 26160 matching citations\n",
      "Combined 1041750 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 1045902 rows\n",
      "File exists with size: 77860973190 bytes\n",
      "Processed 6700000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 23724 matching citations\n",
      "Found 49943 matching citations\n",
      "Found 49658 matching citations\n",
      "Found 49791 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49873 matching citations\n",
      "Found 49951 matching citations\n",
      "Found 49837 matching citations\n",
      "Found 49960 matching citations\n",
      "Found 49938 matching citations\n",
      "Found 49885 matching citations\n",
      "Found 49969 matching citations\n",
      "Found 49863 matching citations\n",
      "Found 49923 matching citations\n",
      "Found 49691 matching citations\n",
      "Found 49297 matching citations\n",
      "Found 49735 matching citations\n",
      "Found 49826 matching citations\n",
      "Found 49656 matching citations\n",
      "Found 49925 matching citations\n",
      "Found 49793 matching citations\n",
      "Found 17062 matching citations\n",
      "Combined 1037300 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 1041400 rows\n",
      "File exists with size: 78653305152 bytes\n",
      "Processed 6750000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 32478 matching citations\n",
      "Found 49733 matching citations\n",
      "Found 49697 matching citations\n",
      "Found 49781 matching citations\n",
      "Found 49956 matching citations\n",
      "Found 49895 matching citations\n",
      "Found 49881 matching citations\n",
      "Found 49974 matching citations\n",
      "Found 49632 matching citations\n",
      "Found 49882 matching citations\n",
      "Found 49424 matching citations\n",
      "Found 49676 matching citations\n",
      "Found 49399 matching citations\n",
      "Found 49880 matching citations\n",
      "Found 48986 matching citations\n",
      "Found 49361 matching citations\n",
      "Found 49798 matching citations\n",
      "Found 49856 matching citations\n",
      "Found 49800 matching citations\n",
      "Found 49931 matching citations\n",
      "Found 47452 matching citations\n",
      "Combined 1024472 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 1028616 rows\n",
      "File exists with size: 79432578961 bytes\n",
      "Processed 6800000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 2157 matching citations\n",
      "Found 49966 matching citations\n",
      "Found 49679 matching citations\n",
      "Found 49788 matching citations\n",
      "Found 49741 matching citations\n",
      "Found 49616 matching citations\n",
      "Found 49974 matching citations\n",
      "Found 49785 matching citations\n",
      "Found 49008 matching citations\n",
      "Found 49971 matching citations\n",
      "Found 49907 matching citations\n",
      "Found 49990 matching citations\n",
      "Found 49967 matching citations\n",
      "Found 49745 matching citations\n",
      "Found 49951 matching citations\n",
      "Found 49190 matching citations\n",
      "Found 49975 matching citations\n",
      "Found 49930 matching citations\n",
      "Found 49952 matching citations\n",
      "Found 49985 matching citations\n",
      "Found 49898 matching citations\n",
      "Found 10379 matching citations\n",
      "Combined 1008554 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 1012890 rows\n",
      "File exists with size: 80185037534 bytes\n",
      "Processed 6850000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 39500 matching citations\n",
      "Found 49925 matching citations\n",
      "Found 49925 matching citations\n",
      "Found 49584 matching citations\n",
      "Found 49972 matching citations\n",
      "Found 49962 matching citations\n",
      "Found 49953 matching citations\n",
      "Found 49118 matching citations\n",
      "Found 49927 matching citations\n",
      "Found 49902 matching citations\n",
      "Found 49815 matching citations\n",
      "Found 49819 matching citations\n",
      "Found 49165 matching citations\n",
      "Found 49949 matching citations\n",
      "Found 49943 matching citations\n",
      "Found 49959 matching citations\n",
      "Found 49783 matching citations\n",
      "Found 49309 matching citations\n",
      "Found 49968 matching citations\n",
      "Found 49916 matching citations\n",
      "Found 38516 matching citations\n",
      "Combined 1023910 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 1028367 rows\n",
      "File exists with size: 80957440400 bytes\n",
      "Processed 6900000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 11115 matching citations\n",
      "Found 49622 matching citations\n",
      "Found 49829 matching citations\n",
      "Found 49946 matching citations\n",
      "Found 49917 matching citations\n",
      "Found 49978 matching citations\n",
      "Found 49888 matching citations\n",
      "Found 49970 matching citations\n",
      "Found 49961 matching citations\n",
      "Found 49712 matching citations\n",
      "Found 49781 matching citations\n",
      "Found 49882 matching citations\n",
      "Found 48398 matching citations\n",
      "Found 49856 matching citations\n",
      "Found 49946 matching citations\n",
      "Found 49930 matching citations\n",
      "Found 49957 matching citations\n",
      "Found 49637 matching citations\n",
      "Found 49812 matching citations\n",
      "Found 49936 matching citations\n",
      "Found 26579 matching citations\n",
      "Combined 983652 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 988352 rows\n",
      "File exists with size: 81701229251 bytes\n",
      "Processed 6950000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 23388 matching citations\n",
      "Found 49900 matching citations\n",
      "Found 49904 matching citations\n",
      "Found 49988 matching citations\n",
      "Found 49878 matching citations\n",
      "Found 49291 matching citations\n",
      "Found 49773 matching citations\n",
      "Found 49644 matching citations\n",
      "Found 49967 matching citations\n",
      "Found 43648 matching citations\n",
      "Found 49913 matching citations\n",
      "Found 49685 matching citations\n",
      "Found 49826 matching citations\n",
      "Found 49856 matching citations\n",
      "Found 49884 matching citations\n",
      "Found 49959 matching citations\n",
      "Found 47734 matching citations\n",
      "Found 49554 matching citations\n",
      "Found 48299 matching citations\n",
      "Found 49849 matching citations\n",
      "Found 15384 matching citations\n",
      "Combined 975324 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 980004 rows\n",
      "File exists with size: 82437863839 bytes\n",
      "Processed 7000000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 31347 matching citations\n",
      "Found 48965 matching citations\n",
      "Found 49995 matching citations\n",
      "Found 44582 matching citations\n",
      "Found 49728 matching citations\n",
      "Found 49189 matching citations\n",
      "Found 49894 matching citations\n",
      "Found 49665 matching citations\n",
      "Found 49910 matching citations\n",
      "Found 49970 matching citations\n",
      "Found 49941 matching citations\n",
      "Found 49906 matching citations\n",
      "Found 49681 matching citations\n",
      "Found 49882 matching citations\n",
      "Found 49706 matching citations\n",
      "Found 49946 matching citations\n",
      "Found 49881 matching citations\n",
      "Found 49924 matching citations\n",
      "Found 49691 matching citations\n",
      "Found 49824 matching citations\n",
      "Found 46482 matching citations\n",
      "Combined 1018109 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 1023006 rows\n",
      "File exists with size: 83205249211 bytes\n",
      "Processed 7050000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 3461 matching citations\n",
      "Found 49979 matching citations\n",
      "Found 49769 matching citations\n",
      "Found 49864 matching citations\n",
      "Found 49813 matching citations\n",
      "Found 49579 matching citations\n",
      "Found 49915 matching citations\n",
      "Found 49882 matching citations\n",
      "Found 49759 matching citations\n",
      "Found 49895 matching citations\n",
      "Found 49627 matching citations\n",
      "Found 49987 matching citations\n",
      "Found 49884 matching citations\n",
      "Found 49688 matching citations\n",
      "Found 49540 matching citations\n",
      "Found 49701 matching citations\n",
      "Found 49980 matching citations\n",
      "Found 49490 matching citations\n",
      "Found 49920 matching citations\n",
      "Found 49649 matching citations\n",
      "Found 10050 matching citations\n",
      "Combined 959432 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 964460 rows\n",
      "File exists with size: 83930925779 bytes\n",
      "Processed 7100000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 39630 matching citations\n",
      "Found 49122 matching citations\n",
      "Found 49230 matching citations\n",
      "Found 49702 matching citations\n",
      "Found 49863 matching citations\n",
      "Found 49921 matching citations\n",
      "Found 49939 matching citations\n",
      "Found 49896 matching citations\n",
      "Found 49770 matching citations\n",
      "Found 49546 matching citations\n",
      "Found 49350 matching citations\n",
      "Found 48110 matching citations\n",
      "Found 49745 matching citations\n",
      "Found 49991 matching citations\n",
      "Found 48016 matching citations\n",
      "Found 49935 matching citations\n",
      "Found 49404 matching citations\n",
      "Found 49898 matching citations\n",
      "Found 49637 matching citations\n",
      "Found 49610 matching citations\n",
      "Found 5513 matching citations\n",
      "Combined 985828 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 990954 rows\n",
      "File exists with size: 84672646897 bytes\n",
      "Processed 7150000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 42877 matching citations\n",
      "Found 49941 matching citations\n",
      "Found 49927 matching citations\n",
      "Found 49880 matching citations\n",
      "Found 48348 matching citations\n",
      "Found 48140 matching citations\n",
      "Found 49763 matching citations\n",
      "Found 49809 matching citations\n",
      "Found 49952 matching citations\n",
      "Found 47498 matching citations\n",
      "Found 49957 matching citations\n",
      "Found 49874 matching citations\n",
      "Found 49703 matching citations\n",
      "Found 49811 matching citations\n",
      "Found 49921 matching citations\n",
      "Found 49863 matching citations\n",
      "Found 49665 matching citations\n",
      "Found 49895 matching citations\n",
      "Found 46611 matching citations\n",
      "Found 34842 matching citations\n",
      "Combined 966277 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 971496 rows\n",
      "File exists with size: 85395625351 bytes\n",
      "Processed 7200000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 15137 matching citations\n",
      "Found 48332 matching citations\n",
      "Found 49835 matching citations\n",
      "Found 49889 matching citations\n",
      "Found 49934 matching citations\n",
      "Found 49948 matching citations\n",
      "Found 48343 matching citations\n",
      "Found 49830 matching citations\n",
      "Found 48200 matching citations\n",
      "Found 48420 matching citations\n",
      "Found 46276 matching citations\n",
      "Found 47188 matching citations\n",
      "Found 49958 matching citations\n",
      "Found 49937 matching citations\n",
      "Found 49881 matching citations\n",
      "Found 48269 matching citations\n",
      "Found 49859 matching citations\n",
      "Found 49987 matching citations\n",
      "Found 49581 matching citations\n",
      "Found 49895 matching citations\n",
      "Found 14423 matching citations\n",
      "Combined 963122 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 968545 rows\n",
      "File exists with size: 86117434702 bytes\n",
      "Processed 7250000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 35262 matching citations\n",
      "Found 49862 matching citations\n",
      "Found 49856 matching citations\n",
      "Found 49851 matching citations\n",
      "Found 49804 matching citations\n",
      "Found 49662 matching citations\n",
      "Found 49904 matching citations\n",
      "Found 46662 matching citations\n",
      "Found 49448 matching citations\n",
      "Found 49774 matching citations\n",
      "Found 49553 matching citations\n",
      "Found 48084 matching citations\n",
      "Found 49529 matching citations\n",
      "Found 49291 matching citations\n",
      "Found 49919 matching citations\n",
      "Found 49615 matching citations\n",
      "Found 49972 matching citations\n",
      "Found 49969 matching citations\n",
      "Found 49737 matching citations\n",
      "Found 21254 matching citations\n",
      "Combined 947008 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 952603 rows\n",
      "File exists with size: 86823136339 bytes\n",
      "Processed 7300000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 28720 matching citations\n",
      "Found 49831 matching citations\n",
      "Found 49405 matching citations\n",
      "Found 49808 matching citations\n",
      "Found 49585 matching citations\n",
      "Found 49288 matching citations\n",
      "Found 49709 matching citations\n",
      "Found 49695 matching citations\n",
      "Found 49653 matching citations\n",
      "Found 49892 matching citations\n",
      "Found 49681 matching citations\n",
      "Found 49905 matching citations\n",
      "Found 49850 matching citations\n",
      "Found 49978 matching citations\n",
      "Found 49749 matching citations\n",
      "Found 49914 matching citations\n",
      "Found 49780 matching citations\n",
      "Found 48916 matching citations\n",
      "Found 49834 matching citations\n",
      "Found 25145 matching citations\n",
      "Combined 948338 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 954205 rows\n",
      "File exists with size: 87541397616 bytes\n",
      "Processed 7350000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 24118 matching citations\n",
      "Found 49856 matching citations\n",
      "Found 49784 matching citations\n",
      "Found 49824 matching citations\n",
      "Found 49805 matching citations\n",
      "Found 49700 matching citations\n",
      "Found 49727 matching citations\n",
      "Found 49699 matching citations\n",
      "Found 49992 matching citations\n",
      "Found 49697 matching citations\n",
      "Found 49934 matching citations\n",
      "Found 49884 matching citations\n",
      "Found 49813 matching citations\n",
      "Found 49792 matching citations\n",
      "Found 49837 matching citations\n",
      "Found 49842 matching citations\n",
      "Found 49686 matching citations\n",
      "Found 49848 matching citations\n",
      "Found 49809 matching citations\n",
      "Found 15901 matching citations\n",
      "Combined 936548 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 942615 rows\n",
      "File exists with size: 88246108450 bytes\n",
      "Processed 7400000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 33991 matching citations\n",
      "Found 49858 matching citations\n",
      "Found 49688 matching citations\n",
      "Found 49961 matching citations\n",
      "Found 49661 matching citations\n",
      "Found 49891 matching citations\n",
      "Found 49955 matching citations\n",
      "Found 49718 matching citations\n",
      "Found 49996 matching citations\n",
      "Found 49900 matching citations\n",
      "Found 49818 matching citations\n",
      "Found 49753 matching citations\n",
      "Found 49936 matching citations\n",
      "Found 49691 matching citations\n",
      "Found 49466 matching citations\n",
      "Found 49816 matching citations\n",
      "Found 49862 matching citations\n",
      "Found 49980 matching citations\n",
      "Found 49868 matching citations\n",
      "Found 12894 matching citations\n",
      "Combined 943703 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 949506 rows\n",
      "File exists with size: 88959024136 bytes\n",
      "Processed 7450000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 36747 matching citations\n",
      "Found 49871 matching citations\n",
      "Found 49910 matching citations\n",
      "Found 49487 matching citations\n",
      "Found 49571 matching citations\n",
      "Found 49770 matching citations\n",
      "Found 49861 matching citations\n",
      "Found 49905 matching citations\n",
      "Found 49854 matching citations\n",
      "Found 49845 matching citations\n",
      "Found 49959 matching citations\n",
      "Found 49954 matching citations\n",
      "Found 49967 matching citations\n",
      "Found 48837 matching citations\n",
      "Found 49886 matching citations\n",
      "Found 49739 matching citations\n",
      "Found 49795 matching citations\n",
      "Found 49895 matching citations\n",
      "Found 49919 matching citations\n",
      "Found 20924 matching citations\n",
      "Combined 953696 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 959776 rows\n",
      "File exists with size: 89675705660 bytes\n",
      "Processed 7500000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 28843 matching citations\n",
      "Found 49842 matching citations\n",
      "Found 49655 matching citations\n",
      "Found 49939 matching citations\n",
      "Found 49540 matching citations\n",
      "Found 49788 matching citations\n",
      "Found 49727 matching citations\n",
      "Found 49922 matching citations\n",
      "Found 49878 matching citations\n",
      "Found 49896 matching citations\n",
      "Found 49939 matching citations\n",
      "Found 49930 matching citations\n",
      "Found 49410 matching citations\n",
      "Found 49642 matching citations\n",
      "Found 49516 matching citations\n",
      "Found 49968 matching citations\n",
      "Found 49899 matching citations\n",
      "Found 49957 matching citations\n",
      "Found 24834 matching citations\n",
      "Combined 900125 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 906252 rows\n",
      "File exists with size: 90354398110 bytes\n",
      "Processed 7550000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 25107 matching citations\n",
      "Found 49959 matching citations\n",
      "Found 49622 matching citations\n",
      "Found 49508 matching citations\n",
      "Found 49852 matching citations\n",
      "Found 49208 matching citations\n",
      "Found 49769 matching citations\n",
      "Found 49623 matching citations\n",
      "Found 49085 matching citations\n",
      "Found 49912 matching citations\n",
      "Found 49805 matching citations\n",
      "Found 49497 matching citations\n",
      "Found 49235 matching citations\n",
      "Found 48916 matching citations\n",
      "Found 49634 matching citations\n",
      "Found 49809 matching citations\n",
      "Found 49921 matching citations\n",
      "Found 49943 matching citations\n",
      "Found 32006 matching citations\n",
      "Combined 900411 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 906561 rows\n",
      "File exists with size: 91036584000 bytes\n",
      "Processed 7600000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 17519 matching citations\n",
      "Found 49777 matching citations\n",
      "Found 49552 matching citations\n",
      "Found 49742 matching citations\n",
      "Found 49543 matching citations\n",
      "Found 49698 matching citations\n",
      "Found 49766 matching citations\n",
      "Found 49796 matching citations\n",
      "Found 49786 matching citations\n",
      "Found 49774 matching citations\n",
      "Found 49466 matching citations\n",
      "Found 49575 matching citations\n",
      "Found 49880 matching citations\n",
      "Found 49489 matching citations\n",
      "Found 49437 matching citations\n",
      "Found 49734 matching citations\n",
      "Found 49878 matching citations\n",
      "Found 49460 matching citations\n",
      "Found 49449 matching citations\n",
      "Found 41277 matching citations\n",
      "Combined 952598 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 958982 rows\n",
      "File exists with size: 91761318286 bytes\n",
      "Processed 7650000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 8557 matching citations\n",
      "Found 49505 matching citations\n",
      "Found 49697 matching citations\n",
      "Found 49239 matching citations\n",
      "Found 49960 matching citations\n",
      "Found 49105 matching citations\n",
      "Found 49926 matching citations\n",
      "Found 49889 matching citations\n",
      "Found 49253 matching citations\n",
      "Found 49267 matching citations\n",
      "Found 49858 matching citations\n",
      "Found 48370 matching citations\n",
      "Found 49303 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49733 matching citations\n",
      "Found 49309 matching citations\n",
      "Found 49685 matching citations\n",
      "Found 49727 matching citations\n",
      "Found 49917 matching citations\n",
      "Found 41458 matching citations\n",
      "Combined 941758 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 947993 rows\n",
      "File exists with size: 92475162316 bytes\n",
      "Processed 7700000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 7612 matching citations\n",
      "Found 49964 matching citations\n",
      "Found 49982 matching citations\n",
      "Found 49733 matching citations\n",
      "Found 49751 matching citations\n",
      "Found 49591 matching citations\n",
      "Found 49919 matching citations\n",
      "Found 49829 matching citations\n",
      "Found 49964 matching citations\n",
      "Found 49877 matching citations\n",
      "Found 49859 matching citations\n",
      "Found 49555 matching citations\n",
      "Found 49734 matching citations\n",
      "Found 49919 matching citations\n",
      "Found 49326 matching citations\n",
      "Found 49800 matching citations\n",
      "Found 49627 matching citations\n",
      "Found 49937 matching citations\n",
      "Found 49323 matching citations\n",
      "Found 25467 matching citations\n",
      "Combined 928769 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 935324 rows\n",
      "File exists with size: 93177618535 bytes\n",
      "Processed 7750000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 24014 matching citations\n",
      "Found 49091 matching citations\n",
      "Found 49755 matching citations\n",
      "Found 49902 matching citations\n",
      "Found 49368 matching citations\n",
      "Found 49918 matching citations\n",
      "Found 49676 matching citations\n",
      "Found 49024 matching citations\n",
      "Found 49979 matching citations\n",
      "Found 49440 matching citations\n",
      "Found 49699 matching citations\n",
      "Found 49892 matching citations\n",
      "Found 49657 matching citations\n",
      "Found 49975 matching citations\n",
      "Found 49226 matching citations\n",
      "Found 49757 matching citations\n",
      "Found 49131 matching citations\n",
      "Found 49737 matching citations\n",
      "Found 49593 matching citations\n",
      "Found 25819 matching citations\n",
      "Combined 942653 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 948999 rows\n",
      "File exists with size: 93890473494 bytes\n",
      "Processed 7800000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 24077 matching citations\n",
      "Found 49999 matching citations\n",
      "Found 49709 matching citations\n",
      "Found 49958 matching citations\n",
      "Found 49885 matching citations\n",
      "Found 49691 matching citations\n",
      "Found 49805 matching citations\n",
      "Found 49854 matching citations\n",
      "Found 49688 matching citations\n",
      "Found 49940 matching citations\n",
      "Found 49945 matching citations\n",
      "Found 49267 matching citations\n",
      "Found 49914 matching citations\n",
      "Found 49613 matching citations\n",
      "Found 49479 matching citations\n",
      "Found 49934 matching citations\n",
      "Found 49964 matching citations\n",
      "Found 49904 matching citations\n",
      "Found 49960 matching citations\n",
      "Found 38941 matching citations\n",
      "Combined 959527 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 966215 rows\n",
      "File exists with size: 94618038142 bytes\n",
      "Processed 7850000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 10096 matching citations\n",
      "Found 49990 matching citations\n",
      "Found 49718 matching citations\n",
      "Found 49648 matching citations\n",
      "Found 49942 matching citations\n",
      "Found 49829 matching citations\n",
      "Found 49958 matching citations\n",
      "Found 49957 matching citations\n",
      "Found 49809 matching citations\n",
      "Found 49780 matching citations\n",
      "Found 49583 matching citations\n",
      "Found 49968 matching citations\n",
      "Found 49827 matching citations\n",
      "Found 49841 matching citations\n",
      "Found 49940 matching citations\n",
      "Found 49520 matching citations\n",
      "Found 49955 matching citations\n",
      "Found 49615 matching citations\n",
      "Found 49687 matching citations\n",
      "Found 49608 matching citations\n",
      "Found 22095 matching citations\n",
      "Combined 978366 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 984935 rows\n",
      "File exists with size: 95354126087 bytes\n",
      "Processed 7900000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 27453 matching citations\n",
      "Found 49354 matching citations\n",
      "Found 49933 matching citations\n",
      "Found 49703 matching citations\n",
      "Found 50000 matching citations\n",
      "Found 49292 matching citations\n",
      "Found 49876 matching citations\n",
      "Found 49584 matching citations\n",
      "Found 49531 matching citations\n",
      "Found 49983 matching citations\n",
      "Found 49977 matching citations\n",
      "Found 49397 matching citations\n",
      "Found 49619 matching citations\n",
      "Found 49840 matching citations\n",
      "Found 49924 matching citations\n",
      "Found 49551 matching citations\n",
      "Found 49991 matching citations\n",
      "Found 49707 matching citations\n",
      "Found 49931 matching citations\n",
      "Found 49725 matching citations\n",
      "Found 17118 matching citations\n",
      "Combined 989489 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 996244 rows\n",
      "File exists with size: 96100668748 bytes\n",
      "Processed 7950000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 32703 matching citations\n",
      "Found 49922 matching citations\n",
      "Found 49964 matching citations\n",
      "Found 49955 matching citations\n",
      "Found 49852 matching citations\n",
      "Found 49917 matching citations\n",
      "Found 49750 matching citations\n",
      "Found 49998 matching citations\n",
      "Found 49801 matching citations\n",
      "Found 49637 matching citations\n",
      "Found 49949 matching citations\n",
      "Found 49736 matching citations\n",
      "Found 49933 matching citations\n",
      "Found 49325 matching citations\n",
      "Found 49992 matching citations\n",
      "Found 49448 matching citations\n",
      "Found 49753 matching citations\n",
      "Found 49615 matching citations\n",
      "Found 49959 matching citations\n",
      "Found 49764 matching citations\n",
      "Found 3755 matching citations\n",
      "Combined 982728 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 989605 rows\n",
      "File exists with size: 96846734913 bytes\n",
      "Processed 8000000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 46143 matching citations\n",
      "Found 49683 matching citations\n",
      "Found 49299 matching citations\n",
      "Found 49823 matching citations\n",
      "Found 49218 matching citations\n",
      "Found 49919 matching citations\n",
      "Found 49248 matching citations\n",
      "Found 49802 matching citations\n",
      "Found 49927 matching citations\n",
      "Found 49974 matching citations\n",
      "Found 49869 matching citations\n",
      "Found 49599 matching citations\n",
      "Found 49380 matching citations\n",
      "Found 49712 matching citations\n",
      "Found 49958 matching citations\n",
      "Found 49726 matching citations\n",
      "Found 49871 matching citations\n",
      "Found 49931 matching citations\n",
      "Found 49868 matching citations\n",
      "Found 7672 matching citations\n",
      "Combined 948622 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 955397 rows\n",
      "File exists with size: 97559550633 bytes\n",
      "Processed 8050000 patent records so far...\n",
      "Processing patent chunk with 50000 records\n",
      "Found 41825 matching citations\n",
      "Found 49536 matching citations\n",
      "Found 49875 matching citations\n",
      "Found 48537 matching citations\n",
      "Found 49959 matching citations\n",
      "Found 49839 matching citations\n",
      "Found 49643 matching citations\n",
      "Found 49744 matching citations\n",
      "Found 49729 matching citations\n",
      "Found 49567 matching citations\n",
      "Found 49738 matching citations\n",
      "Found 49903 matching citations\n",
      "Found 49519 matching citations\n",
      "Found 49578 matching citations\n",
      "Found 49712 matching citations\n",
      "Found 49216 matching citations\n",
      "Found 49594 matching citations\n",
      "Found 49879 matching citations\n",
      "Found 49860 matching citations\n",
      "Found 49593 matching citations\n",
      "Found 47552 matching citations\n",
      "Combined 1032398 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 1039353 rows\n",
      "File exists with size: 98342518949 bytes\n",
      "Processed 8100000 patent records so far...\n",
      "Processing patent chunk with 5235 records\n",
      "Found 2270 matching citations\n",
      "Found 49868 matching citations\n",
      "Found 39082 matching citations\n",
      "Combined 91220 matching citations\n",
      "Performing merge operation...\n",
      "Merged dataframe has 91948 rows\n",
      "File exists with size: 98413626460 bytes\n",
      "Processed 8105235 patent records so far...\n",
      "All chunks processed and saved successfully to '/home/eadlzarabi/Desktop/datasets/Patents/merged_patent_citations.csv'.\n",
      "Final file size: 98413626460 bytes\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# import gc\n",
    "\n",
    "# # File paths\n",
    "# folder = \"/home/eadlzarabi/Desktop/datasets/Patents/\"\n",
    "# merged_patent_data_file = folder + 'patents.csv'\n",
    "# g_us_patent_citation_file = folder + 'g_us_patent_citation.csv'\n",
    "# output_file = folder + 'merged_patent_citations.csv'\n",
    "\n",
    "# # Print absolute path for verification\n",
    "# print(f\"Will save output to: {os.path.abspath(output_file)}\")\n",
    "\n",
    "# # Process in smaller chunks to reduce memory usage\n",
    "# chunk_size = 50000\n",
    "\n",
    "# # Function to free memory\n",
    "# def free_memory():\n",
    "#     gc.collect()\n",
    "    \n",
    "# print(\"Starting patent citation processing...\")\n",
    "\n",
    "# try:\n",
    "#     # Define citation columns with corrected names (without double \"citation\")\n",
    "#     citation_columns = [\n",
    "#         'patent_id',\n",
    "#         'US_citation_sequence',       # Changed from US_citation_citation_sequence\n",
    "#         'US_citation_patent_id',      # Changed from US_citation_citation_patent_id\n",
    "#         'US_citation_date'            # Changed from US_citation_citation_date\n",
    "#     ]\n",
    "    \n",
    "#     # Original column names in the file\n",
    "#     original_columns = [\n",
    "#         'patent_id',\n",
    "#         'US_citation_citation_sequence',\n",
    "#         'US_citation_citation_patent_id',\n",
    "#         'US_citation_citation_date'\n",
    "#     ]\n",
    "    \n",
    "#     # Create a mapping from original to new column names\n",
    "#     column_mapping = dict(zip(original_columns, citation_columns))\n",
    "    \n",
    "#     print(\"Reading citation file headers...\")\n",
    "#     # Check which columns actually exist in the citation file\n",
    "#     temp_df = pd.read_csv(g_us_patent_citation_file, nrows=5)\n",
    "#     available_original_columns = [col for col in original_columns if col in temp_df.columns]\n",
    "    \n",
    "#     print(f\"Available columns in citation file: {available_original_columns}\")\n",
    "#     print(f\"Column mapping: {column_mapping}\")\n",
    "    \n",
    "#     del temp_df  # Free memory\n",
    "    \n",
    "#     # Check if output file exists and remove it\n",
    "#     if os.path.exists(output_file):\n",
    "#         print(f\"Removing existing output file at {output_file}\")\n",
    "#         os.remove(output_file)\n",
    "    \n",
    "#     # Set up CSV file writers for merged data\n",
    "#     first_chunk = True\n",
    "#     processed_count = 0\n",
    "    \n",
    "#     # Perform the merge in chunks\n",
    "#     print(\"Beginning merge operation...\")\n",
    "    \n",
    "#     # Create a reader for the patent data file\n",
    "#     patent_reader = pd.read_csv(\n",
    "#         merged_patent_data_file,\n",
    "#         chunksize=chunk_size,\n",
    "#         engine='python',\n",
    "#         on_bad_lines='skip',\n",
    "#         dtype={'patent_id': str}\n",
    "#     )\n",
    "    \n",
    "#     for patent_chunk in patent_reader:\n",
    "#         try:\n",
    "#             print(f\"Processing patent chunk with {len(patent_chunk)} records\")\n",
    "            \n",
    "#             # Read citation data that could match this chunk of patents\n",
    "#             patent_ids_list = patent_chunk['patent_id'].tolist()\n",
    "            \n",
    "#             # Read citation data in chunks and filter for matches\n",
    "#             citation_chunks = []\n",
    "#             citation_reader = pd.read_csv(\n",
    "#                 g_us_patent_citation_file,\n",
    "#                 chunksize=chunk_size,\n",
    "#                 engine='python',\n",
    "#                 on_bad_lines='skip',\n",
    "#                 dtype={'patent_id': str},\n",
    "#                 usecols=available_original_columns\n",
    "#             )\n",
    "            \n",
    "#             for citation_chunk in citation_reader:\n",
    "#                 # Rename columns to new format\n",
    "#                 citation_chunk = citation_chunk.rename(columns=column_mapping)\n",
    "                \n",
    "#                 # Filter for patents in our current chunk\n",
    "#                 matched_citations = citation_chunk[citation_chunk['patent_id'].isin(patent_ids_list)]\n",
    "                \n",
    "#                 if not matched_citations.empty:\n",
    "#                     citation_chunks.append(matched_citations)\n",
    "#                     print(f\"Found {len(matched_citations)} matching citations\")\n",
    "            \n",
    "#             # Combine all citation chunks\n",
    "#             if citation_chunks:\n",
    "#                 citations_df = pd.concat(citation_chunks)\n",
    "#                 print(f\"Combined {len(citations_df)} matching citations\")\n",
    "#             else:\n",
    "#                 citations_df = pd.DataFrame(columns=citation_columns)\n",
    "#                 print(\"No matching citations found\")\n",
    "            \n",
    "#             # Perform the merge operation\n",
    "#             print(\"Performing merge operation...\")\n",
    "#             merged_df = pd.merge(\n",
    "#                 patent_chunk,\n",
    "#                 citations_df,\n",
    "#                 on='patent_id',\n",
    "#                 how='left'  # Keep all patents even if they don't have citations\n",
    "#             )\n",
    "            \n",
    "#             print(f\"Merged dataframe has {len(merged_df)} rows\")\n",
    "            \n",
    "#             # Write to output file\n",
    "#             if first_chunk:\n",
    "#                 merged_df.to_csv(output_file, index=False, mode='w')\n",
    "#                 first_chunk = False\n",
    "#             else:\n",
    "#                 merged_df.to_csv(output_file, index=False, mode='a', header=False)\n",
    "            \n",
    "#             # Verify file was written\n",
    "#             if os.path.exists(output_file):\n",
    "#                 print(f\"File exists with size: {os.path.getsize(output_file)} bytes\")\n",
    "#             else:\n",
    "#                 print(f\"Warning: Output file was not created after write operation!\")\n",
    "            \n",
    "#             processed_count += len(patent_chunk)\n",
    "#             print(f\"Processed {processed_count} patent records so far...\")\n",
    "            \n",
    "#             # Free memory\n",
    "#             del patent_chunk\n",
    "#             del citations_df\n",
    "#             del merged_df\n",
    "#             if citation_chunks:\n",
    "#                 del citation_chunks\n",
    "#             free_memory()\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing chunk: {e}\")\n",
    "#             import traceback\n",
    "#             traceback.print_exc()\n",
    "#             continue\n",
    "    \n",
    "#     # Final verification\n",
    "#     if os.path.exists(output_file):\n",
    "#         print(f\"All chunks processed and saved successfully to '{output_file}'.\")\n",
    "#         print(f\"Final file size: {os.path.getsize(output_file)} bytes\")\n",
    "#     else:\n",
    "#         print(f\"Error: Output file does not exist at '{output_file}' after processing!\")\n",
    "    \n",
    "# except Exception as e:\n",
    "#     print(f\"Error in processing: {e}\")\n",
    "#     import traceback\n",
    "#     traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "\n",
    "# File paths\n",
    "folder = \"/home/eadlzarabi/Desktop/datasets/Patents/\"\n",
    "merged_patent_data_file = folder + 'patents.csv'\n",
    "g_us_patent_citation_file = folder + 'g_us_patent_citation.csv'\n",
    "output_file = folder + 'merged_patent_citations.csv'\n",
    "\n",
    "# Print absolute path for verification\n",
    "print(f\"Will save output to: {os.path.abspath(output_file)}\")\n",
    "\n",
    "# Process in smaller chunks to reduce memory usage\n",
    "chunk_size = 50000\n",
    "\n",
    "# Function to free memory\n",
    "def free_memory():\n",
    "    gc.collect()\n",
    "    \n",
    "print(\"Starting patent citation processing...\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Define citation columns with corrected names (without double \"citation\")\n",
    "    citation_columns = [\n",
    "        'patent_id',\n",
    "        'US_citation_sequence',       # Changed from US_citation_citation_sequence\n",
    "        'US_citation_patent_id',      # Changed from US_citation_citation_patent_id\n",
    "        'US_citation_date'            # Changed from US_citation_citation_date\n",
    "    ]\n",
    "    \n",
    "    # Original column names in the file\n",
    "    original_columns = [\n",
    "        'patent_id',\n",
    "        'US_citation_citation_sequence',\n",
    "        'US_citation_citation_patent_id',\n",
    "        'US_citation_citation_date'\n",
    "    ]\n",
    "    \n",
    "    # Create a mapping from original to new column names\n",
    "    column_mapping = dict(zip(original_columns, citation_columns))\n",
    "    \n",
    "    print(\"Reading citation file headers...\")\n",
    "    # Check which columns actually exist in the citation file\n",
    "    temp_df = pd.read_csv(g_us_patent_citation_file, nrows=5)\n",
    "    available_original_columns = [col for col in original_columns if col in temp_df.columns]\n",
    "    \n",
    "    print(f\"Available columns in citation file: {available_original_columns}\")\n",
    "    print(f\"Column mapping: {column_mapping}\")\n",
    "    \n",
    "    del temp_df  # Free memory\n",
    "    \n",
    "    # Check if output file exists and remove it\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"Removing existing output file at {output_file}\")\n",
    "        os.remove(output_file)\n",
    "    \n",
    "    # Set up CSV file writers for merged data\n",
    "    first_chunk = True\n",
    "    processed_count = 0\n",
    "    \n",
    "    # Track processed patent IDs to avoid duplicates\n",
    "    processed_patent_ids = set()\n",
    "    \n",
    "    # Perform the merge in chunks\n",
    "    print(\"Beginning merge operation...\")\n",
    "    \n",
    "    # Create a reader for the patent data file\n",
    "    patent_reader = pd.read_csv(\n",
    "        merged_patent_data_file,\n",
    "        chunksize=chunk_size,\n",
    "        engine='python',\n",
    "        on_bad_lines='skip',\n",
    "        dtype={'patent_id': str}\n",
    "    )\n",
    "    \n",
    "    for chunk_num, patent_chunk in enumerate(patent_reader, 1):\n",
    "        chunk_start_time = time.time()\n",
    "        try:\n",
    "            print(f\"Processing patent chunk {chunk_num} with {len(patent_chunk)} records\")\n",
    "            \n",
    "            # Filter out patents we've already processed to avoid duplicates\n",
    "            patent_chunk = patent_chunk[~patent_chunk['patent_id'].isin(processed_patent_ids)]\n",
    "            \n",
    "            if patent_chunk.empty:\n",
    "                print(\"All patents in this chunk have already been processed. Skipping.\")\n",
    "                continue\n",
    "                \n",
    "            # Get the list of new patent IDs to process\n",
    "            patent_ids_list = patent_chunk['patent_id'].tolist()\n",
    "            \n",
    "            # Add these to our tracking set\n",
    "            processed_patent_ids.update(patent_ids_list)\n",
    "            \n",
    "            # Read citation data that could match this chunk of patents\n",
    "            print(f\"Searching citations for {len(patent_ids_list)} patents...\")\n",
    "            \n",
    "            # Read citation data in chunks and filter for matches\n",
    "            citation_chunks = []\n",
    "            matched_citation_count = 0\n",
    "            \n",
    "            # Create a unique key for each citation to avoid duplicates\n",
    "            seen_citations = set()\n",
    "            \n",
    "            citation_reader = pd.read_csv(\n",
    "                g_us_patent_citation_file,\n",
    "                chunksize=chunk_size,\n",
    "                engine='python',\n",
    "                on_bad_lines='skip',\n",
    "                dtype={'patent_id': str},\n",
    "                usecols=available_original_columns\n",
    "            )\n",
    "            \n",
    "            for citation_chunk_num, citation_chunk in enumerate(citation_reader, 1):\n",
    "                # Rename columns to new format\n",
    "                citation_chunk = citation_chunk.rename(columns=column_mapping)\n",
    "                \n",
    "                # Filter for patents in our current chunk\n",
    "                matched_citations = citation_chunk[citation_chunk['patent_id'].isin(patent_ids_list)]\n",
    "                \n",
    "                if not matched_citations.empty:\n",
    "                    # Create a unique key for each citation to avoid duplicates\n",
    "                    if 'US_citation_patent_id' in matched_citations.columns and 'US_citation_sequence' in matched_citations.columns:\n",
    "                        matched_citations['citation_key'] = matched_citations.apply(\n",
    "                            lambda row: f\"{row['patent_id']}_{row['US_citation_patent_id']}_{row['US_citation_sequence']}\", \n",
    "                            axis=1\n",
    "                        )\n",
    "                    else:\n",
    "                        # Fallback if columns are missing\n",
    "                        matched_citations['citation_key'] = matched_citations['patent_id'].astype(str)\n",
    "                        if 'US_citation_patent_id' in matched_citations.columns:\n",
    "                            matched_citations['citation_key'] += '_' + matched_citations['US_citation_patent_id'].astype(str)\n",
    "                    \n",
    "                    # Filter out citations we've already seen\n",
    "                    new_keys = set(matched_citations['citation_key']) - seen_citations\n",
    "                    matched_citations = matched_citations[matched_citations['citation_key'].isin(new_keys)]\n",
    "                    \n",
    "                    # Update seen citations\n",
    "                    seen_citations.update(new_keys)\n",
    "                    \n",
    "                    # Remove the temporary key column\n",
    "                    matched_citations = matched_citations.drop(columns=['citation_key'])\n",
    "                    \n",
    "                    if not matched_citations.empty:\n",
    "                        citation_chunks.append(matched_citations)\n",
    "                        matched_citation_count += len(matched_citations)\n",
    "                        print(f\"Found {len(matched_citations)} new matching citations in citation chunk {citation_chunk_num}\")\n",
    "            \n",
    "            # Combine all citation chunks\n",
    "            if citation_chunks:\n",
    "                citations_df = pd.concat(citation_chunks)\n",
    "                print(f\"Combined {len(citations_df)} matching citations for {len(patent_ids_list)} patents\")\n",
    "            else:\n",
    "                citations_df = pd.DataFrame(columns=citation_columns)\n",
    "                print(\"No matching citations found\")\n",
    "            \n",
    "            # Perform the merge operation\n",
    "            print(\"Performing merge operation...\")\n",
    "            merged_df = pd.merge(\n",
    "                patent_chunk,\n",
    "                citations_df,\n",
    "                on='patent_id',\n",
    "                how='left'  # Keep all patents even if they don't have citations\n",
    "            )\n",
    "            \n",
    "            # Double-check for duplicates before writing\n",
    "            # This is an extra safety measure\n",
    "            if 'US_citation_patent_id' in merged_df.columns and 'US_citation_sequence' in merged_df.columns:\n",
    "                # Create a composite key for deduplication\n",
    "                merged_df['unique_key'] = merged_df.apply(\n",
    "                    lambda row: f\"{row['patent_id']}_{row.get('US_citation_patent_id', 'None')}_{row.get('US_citation_sequence', 'None')}\", \n",
    "                    axis=1\n",
    "                )\n",
    "                \n",
    "                # Remove duplicates\n",
    "                initial_size = len(merged_df)\n",
    "                merged_df = merged_df.drop_duplicates(subset=['unique_key'], keep='first')\n",
    "                dups_removed = initial_size - len(merged_df)\n",
    "                \n",
    "                if dups_removed > 0:\n",
    "                    print(f\"Removed {dups_removed} duplicate records from merged dataframe\")\n",
    "                \n",
    "                # Remove the key column\n",
    "                merged_df = merged_df.drop(columns=['unique_key'])\n",
    "            \n",
    "            print(f\"Final merged dataframe has {len(merged_df)} rows\")\n",
    "            \n",
    "            # Write to output file\n",
    "            if first_chunk:\n",
    "                merged_df.to_csv(output_file, index=False, mode='w')\n",
    "                first_chunk = False\n",
    "            else:\n",
    "                merged_df.to_csv(output_file, index=False, mode='a', header=False)\n",
    "            \n",
    "            # Verify file was written\n",
    "            if os.path.exists(output_file):\n",
    "                print(f\"File exists with size: {os.path.getsize(output_file) / (1024*1024):.2f} MB\")\n",
    "            else:\n",
    "                print(f\"Warning: Output file was not created after write operation!\")\n",
    "            \n",
    "            processed_count += len(patent_chunk)\n",
    "            chunk_time = time.time() - chunk_start_time\n",
    "            print(f\"Processed {processed_count} patent records so far...\")\n",
    "            print(f\"Chunk {chunk_num} processing time: {chunk_time:.2f} seconds\")\n",
    "            \n",
    "            # Free memory\n",
    "            del patent_chunk\n",
    "            del citations_df\n",
    "            del merged_df\n",
    "            if citation_chunks:\n",
    "                del citation_chunks\n",
    "            free_memory()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    \n",
    "    # Final verification\n",
    "    total_time = time.time() - start_time\n",
    "    if os.path.exists(output_file):\n",
    "        final_size = os.path.getsize(output_file)\n",
    "        print(f\"All chunks processed and saved successfully to '{output_file}'.\")\n",
    "        print(f\"Final file size: {final_size / (1024*1024):.2f} MB\")\n",
    "        print(f\"Total processing time: {total_time:.2f} seconds\")\n",
    "        print(f\"Total patents processed: {processed_count}\")\n",
    "        print(f\"Processed {processed_count / total_time:.2f} patents per second\")\n",
    "    else:\n",
    "        print(f\"Error: Output file does not exist at '{output_file}' after processing!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in processing: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will save deduplicated data to: /home/eadlzarabi/Desktop/datasets/Patents/merged_patent_citations_deduplicated.csv\n",
      "Starting deduplication process based on patent_id...\n",
      "Input file size: 93854.55 MB\n",
      "Reading file headers...\n",
      "Found 9 columns in the file\n",
      "Removing existing output file: /home/eadlzarabi/Desktop/datasets/Patents/merged_patent_citations_deduplicated.csv\n",
      "Processing file in chunks...\n",
      "Processing chunk 1 with 100000 rows...\n",
      "Chunk 1 processed in 0.04 seconds\n",
      "Removed 95049 duplicates from this chunk\n",
      "Progress: 100,000/100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 95,049 (95.05%)\n",
      "Processing chunk 2 with 100000 rows...\n",
      "Chunk 2 processed in 0.03 seconds\n",
      "Removed 94475 duplicates from this chunk\n",
      "Progress: 200,000/200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 189,524 (94.76%)\n",
      "Processing chunk 3 with 100000 rows...\n",
      "Chunk 3 processed in 0.03 seconds\n",
      "Removed 94226 duplicates from this chunk\n",
      "Progress: 300,000/300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 283,750 (94.58%)\n",
      "Processing chunk 4 with 100000 rows...\n",
      "Chunk 4 processed in 0.03 seconds\n",
      "Removed 94234 duplicates from this chunk\n",
      "Progress: 400,000/400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 377,984 (94.50%)\n",
      "Processing chunk 5 with 100000 rows...\n",
      "Chunk 5 processed in 0.03 seconds\n",
      "Removed 93764 duplicates from this chunk\n",
      "Progress: 500,000/500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 471,748 (94.35%)\n",
      "Processing chunk 6 with 100000 rows...\n",
      "Chunk 6 processed in 0.02 seconds\n",
      "Removed 95321 duplicates from this chunk\n",
      "Progress: 600,000/600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 567,069 (94.51%)\n",
      "Processing chunk 7 with 100000 rows...\n",
      "Chunk 7 processed in 0.03 seconds\n",
      "Removed 94042 duplicates from this chunk\n",
      "Progress: 700,000/700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 661,111 (94.44%)\n",
      "Processing chunk 8 with 100000 rows...\n",
      "Chunk 8 processed in 0.03 seconds\n",
      "Removed 94956 duplicates from this chunk\n",
      "Progress: 800,000/800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 756,067 (94.51%)\n",
      "Processing chunk 9 with 100000 rows...\n",
      "Chunk 9 processed in 0.02 seconds\n",
      "Removed 95311 duplicates from this chunk\n",
      "Progress: 900,000/900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 851,378 (94.60%)\n",
      "Processing chunk 10 with 100000 rows...\n",
      "Chunk 10 processed in 0.02 seconds\n",
      "Removed 96063 duplicates from this chunk\n",
      "Progress: 1,000,000/1,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 947,441 (94.74%)\n",
      "Processing chunk 11 with 100000 rows...\n",
      "Chunk 11 processed in 0.03 seconds\n",
      "Removed 94427 duplicates from this chunk\n",
      "Progress: 1,100,000/1,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 1,041,868 (94.72%)\n",
      "Processing chunk 12 with 100000 rows...\n",
      "Chunk 12 processed in 0.03 seconds\n",
      "Removed 94255 duplicates from this chunk\n",
      "Progress: 1,200,000/1,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 1,136,123 (94.68%)\n",
      "Processing chunk 13 with 100000 rows...\n",
      "Chunk 13 processed in 0.03 seconds\n",
      "Removed 95020 duplicates from this chunk\n",
      "Progress: 1,300,000/1,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 1,231,143 (94.70%)\n",
      "Processing chunk 14 with 100000 rows...\n",
      "Chunk 14 processed in 0.02 seconds\n",
      "Removed 95043 duplicates from this chunk\n",
      "Progress: 1,400,000/1,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 1,326,186 (94.73%)\n",
      "Processing chunk 15 with 100000 rows...\n",
      "Chunk 15 processed in 0.03 seconds\n",
      "Removed 94971 duplicates from this chunk\n",
      "Progress: 1,500,000/1,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 1,421,157 (94.74%)\n",
      "Processing chunk 16 with 100000 rows...\n",
      "Chunk 16 processed in 0.03 seconds\n",
      "Removed 93995 duplicates from this chunk\n",
      "Progress: 1,600,000/1,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 1,515,152 (94.70%)\n",
      "Processing chunk 17 with 100000 rows...\n",
      "Chunk 17 processed in 0.02 seconds\n",
      "Removed 95479 duplicates from this chunk\n",
      "Progress: 1,700,000/1,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 1,610,631 (94.74%)\n",
      "Processing chunk 18 with 100000 rows...\n",
      "Chunk 18 processed in 0.03 seconds\n",
      "Removed 94569 duplicates from this chunk\n",
      "Progress: 1,800,000/1,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 1,705,200 (94.73%)\n",
      "Processing chunk 19 with 100000 rows...\n",
      "Chunk 19 processed in 0.02 seconds\n",
      "Removed 95041 duplicates from this chunk\n",
      "Progress: 1,900,000/1,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 1,800,241 (94.75%)\n",
      "Processing chunk 20 with 100000 rows...\n",
      "Chunk 20 processed in 0.03 seconds\n",
      "Removed 94714 duplicates from this chunk\n",
      "Progress: 2,000,000/2,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 1,894,955 (94.75%)\n",
      "Processing chunk 21 with 100000 rows...\n",
      "Chunk 21 processed in 0.03 seconds\n",
      "Removed 94029 duplicates from this chunk\n",
      "Progress: 2,100,000/2,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 1,988,984 (94.71%)\n",
      "Processing chunk 22 with 100000 rows...\n",
      "Chunk 22 processed in 0.03 seconds\n",
      "Removed 94428 duplicates from this chunk\n",
      "Progress: 2,200,000/2,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 2,083,412 (94.70%)\n",
      "Processing chunk 23 with 100000 rows...\n",
      "Chunk 23 processed in 0.03 seconds\n",
      "Removed 93889 duplicates from this chunk\n",
      "Progress: 2,300,000/2,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 2,177,301 (94.67%)\n",
      "Processing chunk 24 with 100000 rows...\n",
      "Chunk 24 processed in 0.03 seconds\n",
      "Removed 94020 duplicates from this chunk\n",
      "Progress: 2,400,000/2,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 2,271,321 (94.64%)\n",
      "Processing chunk 25 with 100000 rows...\n",
      "Chunk 25 processed in 0.02 seconds\n",
      "Removed 95473 duplicates from this chunk\n",
      "Progress: 2,500,000/2,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 2,366,794 (94.67%)\n",
      "Processing chunk 26 with 100000 rows...\n",
      "Chunk 26 processed in 0.02 seconds\n",
      "Removed 95513 duplicates from this chunk\n",
      "Progress: 2,600,000/2,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 2,462,307 (94.70%)\n",
      "Processing chunk 27 with 100000 rows...\n",
      "Chunk 27 processed in 0.03 seconds\n",
      "Removed 94337 duplicates from this chunk\n",
      "Progress: 2,700,000/2,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 2,556,644 (94.69%)\n",
      "Processing chunk 28 with 100000 rows...\n",
      "Chunk 28 processed in 0.03 seconds\n",
      "Removed 94682 duplicates from this chunk\n",
      "Progress: 2,800,000/2,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 2,651,326 (94.69%)\n",
      "Processing chunk 29 with 100000 rows...\n",
      "Chunk 29 processed in 0.03 seconds\n",
      "Removed 94055 duplicates from this chunk\n",
      "Progress: 2,900,000/2,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 2,745,381 (94.67%)\n",
      "Processing chunk 30 with 100000 rows...\n",
      "Chunk 30 processed in 0.02 seconds\n",
      "Removed 95089 duplicates from this chunk\n",
      "Progress: 3,000,000/3,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 2,840,470 (94.68%)\n",
      "Processing chunk 31 with 100000 rows...\n",
      "Chunk 31 processed in 0.03 seconds\n",
      "Removed 94357 duplicates from this chunk\n",
      "Progress: 3,100,000/3,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 2,934,827 (94.67%)\n",
      "Processing chunk 32 with 100000 rows...\n",
      "Chunk 32 processed in 0.03 seconds\n",
      "Removed 94350 duplicates from this chunk\n",
      "Progress: 3,200,000/3,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 3,029,177 (94.66%)\n",
      "Processing chunk 33 with 100000 rows...\n",
      "Chunk 33 processed in 0.03 seconds\n",
      "Removed 93710 duplicates from this chunk\n",
      "Progress: 3,300,000/3,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 3,122,887 (94.63%)\n",
      "Processing chunk 34 with 100000 rows...\n",
      "Chunk 34 processed in 0.02 seconds\n",
      "Removed 95913 duplicates from this chunk\n",
      "Progress: 3,400,000/3,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 3,218,800 (94.67%)\n",
      "Processing chunk 35 with 100000 rows...\n",
      "Chunk 35 processed in 0.03 seconds\n",
      "Removed 94719 duplicates from this chunk\n",
      "Progress: 3,500,000/3,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 3,313,519 (94.67%)\n",
      "Processing chunk 36 with 100000 rows...\n",
      "Chunk 36 processed in 0.03 seconds\n",
      "Removed 94211 duplicates from this chunk\n",
      "Progress: 3,600,000/3,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 3,407,730 (94.66%)\n",
      "Processing chunk 37 with 100000 rows...\n",
      "Chunk 37 processed in 0.02 seconds\n",
      "Removed 95290 duplicates from this chunk\n",
      "Progress: 3,700,000/3,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 3,503,020 (94.68%)\n",
      "Processing chunk 38 with 100000 rows...\n",
      "Chunk 38 processed in 0.02 seconds\n",
      "Removed 95588 duplicates from this chunk\n",
      "Progress: 3,800,000/3,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 3,598,608 (94.70%)\n",
      "Processing chunk 39 with 100000 rows...\n",
      "Chunk 39 processed in 0.02 seconds\n",
      "Removed 95574 duplicates from this chunk\n",
      "Progress: 3,900,000/3,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 3,694,182 (94.72%)\n",
      "Processing chunk 40 with 100000 rows...\n",
      "Chunk 40 processed in 0.03 seconds\n",
      "Removed 94810 duplicates from this chunk\n",
      "Progress: 4,000,000/4,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 3,788,992 (94.72%)\n",
      "Processing chunk 41 with 100000 rows...\n",
      "Chunk 41 processed in 0.03 seconds\n",
      "Removed 93604 duplicates from this chunk\n",
      "Progress: 4,100,000/4,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 3,882,596 (94.70%)\n",
      "Processing chunk 42 with 100000 rows...\n",
      "Chunk 42 processed in 0.02 seconds\n",
      "Removed 95676 duplicates from this chunk\n",
      "Progress: 4,200,000/4,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 3,978,272 (94.72%)\n",
      "Processing chunk 43 with 100000 rows...\n",
      "Chunk 43 processed in 0.02 seconds\n",
      "Removed 95963 duplicates from this chunk\n",
      "Progress: 4,300,000/4,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 4,074,235 (94.75%)\n",
      "Processing chunk 44 with 100000 rows...\n",
      "Chunk 44 processed in 0.02 seconds\n",
      "Removed 95309 duplicates from this chunk\n",
      "Progress: 4,400,000/4,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 4,169,544 (94.76%)\n",
      "Processing chunk 45 with 100000 rows...\n",
      "Chunk 45 processed in 0.03 seconds\n",
      "Removed 94661 duplicates from this chunk\n",
      "Progress: 4,500,000/4,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 4,264,205 (94.76%)\n",
      "Processing chunk 46 with 100000 rows...\n",
      "Chunk 46 processed in 0.02 seconds\n",
      "Removed 95453 duplicates from this chunk\n",
      "Progress: 4,600,000/4,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 4,359,658 (94.78%)\n",
      "Processing chunk 47 with 100000 rows...\n",
      "Chunk 47 processed in 0.02 seconds\n",
      "Removed 96762 duplicates from this chunk\n",
      "Progress: 4,700,000/4,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 4,456,420 (94.82%)\n",
      "Processing chunk 48 with 100000 rows...\n",
      "Chunk 48 processed in 0.03 seconds\n",
      "Removed 94237 duplicates from this chunk\n",
      "Progress: 4,800,000/4,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 4,550,657 (94.81%)\n",
      "Processing chunk 49 with 100000 rows...\n",
      "Chunk 49 processed in 0.03 seconds\n",
      "Removed 93869 duplicates from this chunk\n",
      "Progress: 4,900,000/4,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 4,644,526 (94.79%)\n",
      "Processing chunk 50 with 100000 rows...\n",
      "Chunk 50 processed in 0.02 seconds\n",
      "Removed 95952 duplicates from this chunk\n",
      "Progress: 5,000,000/5,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 4,740,478 (94.81%)\n",
      "Processing chunk 51 with 100000 rows...\n",
      "Chunk 51 processed in 0.02 seconds\n",
      "Removed 96369 duplicates from this chunk\n",
      "Progress: 5,100,000/5,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 4,836,847 (94.84%)\n",
      "Processing chunk 52 with 100000 rows...\n",
      "Chunk 52 processed in 0.03 seconds\n",
      "Removed 94305 duplicates from this chunk\n",
      "Progress: 5,200,000/5,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 4,931,152 (94.83%)\n",
      "Processing chunk 53 with 100000 rows...\n",
      "Chunk 53 processed in 0.03 seconds\n",
      "Removed 94380 duplicates from this chunk\n",
      "Progress: 5,300,000/5,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 5,025,532 (94.82%)\n",
      "Processing chunk 54 with 100000 rows...\n",
      "Chunk 54 processed in 0.02 seconds\n",
      "Removed 96505 duplicates from this chunk\n",
      "Progress: 5,400,000/5,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 5,122,037 (94.85%)\n",
      "Processing chunk 55 with 100000 rows...\n",
      "Chunk 55 processed in 0.03 seconds\n",
      "Removed 95023 duplicates from this chunk\n",
      "Progress: 5,500,000/5,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 5,217,060 (94.86%)\n",
      "Processing chunk 56 with 100000 rows...\n",
      "Chunk 56 processed in 0.03 seconds\n",
      "Removed 93585 duplicates from this chunk\n",
      "Progress: 5,600,000/5,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 5,310,645 (94.83%)\n",
      "Processing chunk 57 with 100000 rows...\n",
      "Chunk 57 processed in 0.02 seconds\n",
      "Removed 95605 duplicates from this chunk\n",
      "Progress: 5,700,000/5,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 5,406,250 (94.85%)\n",
      "Processing chunk 58 with 100000 rows...\n",
      "Chunk 58 processed in 0.02 seconds\n",
      "Removed 96208 duplicates from this chunk\n",
      "Progress: 5,800,000/5,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 5,502,458 (94.87%)\n",
      "Processing chunk 59 with 100000 rows...\n",
      "Chunk 59 processed in 0.03 seconds\n",
      "Removed 93770 duplicates from this chunk\n",
      "Progress: 5,900,000/5,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 5,596,228 (94.85%)\n",
      "Processing chunk 60 with 100000 rows...\n",
      "Chunk 60 processed in 0.03 seconds\n",
      "Removed 94791 duplicates from this chunk\n",
      "Progress: 6,000,000/6,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 5,691,019 (94.85%)\n",
      "Processing chunk 61 with 100000 rows...\n",
      "Chunk 61 processed in 0.02 seconds\n",
      "Removed 95821 duplicates from this chunk\n",
      "Progress: 6,100,000/6,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 5,786,840 (94.87%)\n",
      "Processing chunk 62 with 100000 rows...\n",
      "Chunk 62 processed in 0.03 seconds\n",
      "Removed 94669 duplicates from this chunk\n",
      "Progress: 6,200,000/6,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 5,881,509 (94.86%)\n",
      "Processing chunk 63 with 100000 rows...\n",
      "Chunk 63 processed in 0.03 seconds\n",
      "Removed 94461 duplicates from this chunk\n",
      "Progress: 6,300,000/6,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 5,975,970 (94.86%)\n",
      "Processing chunk 64 with 100000 rows...\n",
      "Chunk 64 processed in 0.02 seconds\n",
      "Removed 96066 duplicates from this chunk\n",
      "Progress: 6,400,000/6,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 6,072,036 (94.88%)\n",
      "Processing chunk 65 with 100000 rows...\n",
      "Chunk 65 processed in 0.02 seconds\n",
      "Removed 96041 duplicates from this chunk\n",
      "Progress: 6,500,000/6,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 6,168,077 (94.89%)\n",
      "Processing chunk 66 with 100000 rows...\n",
      "Chunk 66 processed in 0.03 seconds\n",
      "Removed 93871 duplicates from this chunk\n",
      "Progress: 6,600,000/6,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 6,261,948 (94.88%)\n",
      "Processing chunk 67 with 100000 rows...\n",
      "Chunk 67 processed in 0.02 seconds\n",
      "Removed 96065 duplicates from this chunk\n",
      "Progress: 6,700,000/6,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 6,358,013 (94.90%)\n",
      "Processing chunk 68 with 100000 rows...\n",
      "Chunk 68 processed in 0.02 seconds\n",
      "Removed 95255 duplicates from this chunk\n",
      "Progress: 6,800,000/6,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 6,453,268 (94.90%)\n",
      "Processing chunk 69 with 100000 rows...\n",
      "Chunk 69 processed in 0.03 seconds\n",
      "Removed 93452 duplicates from this chunk\n",
      "Progress: 6,900,000/6,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 6,546,720 (94.88%)\n",
      "Processing chunk 70 with 100000 rows...\n",
      "Chunk 70 processed in 0.02 seconds\n",
      "Removed 95817 duplicates from this chunk\n",
      "Progress: 7,000,000/7,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 6,642,537 (94.89%)\n",
      "Processing chunk 71 with 100000 rows...\n",
      "Chunk 71 processed in 0.02 seconds\n",
      "Removed 95888 duplicates from this chunk\n",
      "Progress: 7,100,000/7,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 6,738,425 (94.91%)\n",
      "Processing chunk 72 with 100000 rows...\n",
      "Chunk 72 processed in 0.02 seconds\n",
      "Removed 96018 duplicates from this chunk\n",
      "Progress: 7,200,000/7,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 6,834,443 (94.92%)\n",
      "Processing chunk 73 with 100000 rows...\n",
      "Chunk 73 processed in 0.03 seconds\n",
      "Removed 93544 duplicates from this chunk\n",
      "Progress: 7,300,000/7,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 6,927,987 (94.90%)\n",
      "Processing chunk 74 with 100000 rows...\n",
      "Chunk 74 processed in 0.02 seconds\n",
      "Removed 95298 duplicates from this chunk\n",
      "Progress: 7,400,000/7,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 7,023,285 (94.91%)\n",
      "Processing chunk 75 with 100000 rows...\n",
      "Chunk 75 processed in 0.02 seconds\n",
      "Removed 95018 duplicates from this chunk\n",
      "Progress: 7,500,000/7,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 7,118,303 (94.91%)\n",
      "Processing chunk 76 with 100000 rows...\n",
      "Chunk 76 processed in 0.03 seconds\n",
      "Removed 95133 duplicates from this chunk\n",
      "Progress: 7,600,000/7,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 7,213,436 (94.91%)\n",
      "Processing chunk 77 with 100000 rows...\n",
      "Chunk 77 processed in 0.03 seconds\n",
      "Removed 93306 duplicates from this chunk\n",
      "Progress: 7,700,000/7,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 7,306,742 (94.89%)\n",
      "Processing chunk 78 with 100000 rows...\n",
      "Chunk 78 processed in 0.02 seconds\n",
      "Removed 95288 duplicates from this chunk\n",
      "Progress: 7,800,000/7,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 7,402,030 (94.90%)\n",
      "Processing chunk 79 with 100000 rows...\n",
      "Chunk 79 processed in 0.02 seconds\n",
      "Removed 95290 duplicates from this chunk\n",
      "Progress: 7,900,000/7,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 7,497,320 (94.90%)\n",
      "Processing chunk 80 with 100000 rows...\n",
      "Chunk 80 processed in 0.03 seconds\n",
      "Removed 93963 duplicates from this chunk\n",
      "Progress: 8,000,000/8,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 7,591,283 (94.89%)\n",
      "Processing chunk 81 with 100000 rows...\n",
      "Chunk 81 processed in 0.03 seconds\n",
      "Removed 94576 duplicates from this chunk\n",
      "Progress: 8,100,000/8,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 7,685,859 (94.89%)\n",
      "Processing chunk 82 with 100000 rows...\n",
      "Chunk 82 processed in 0.02 seconds\n",
      "Removed 96471 duplicates from this chunk\n",
      "Progress: 8,200,000/8,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 7,782,330 (94.91%)\n",
      "Processing chunk 83 with 100000 rows...\n",
      "Chunk 83 processed in 0.02 seconds\n",
      "Removed 95911 duplicates from this chunk\n",
      "Progress: 8,300,000/8,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 7,878,241 (94.92%)\n",
      "Processing chunk 84 with 100000 rows...\n",
      "Chunk 84 processed in 0.02 seconds\n",
      "Removed 95130 duplicates from this chunk\n",
      "Progress: 8,400,000/8,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 7,973,371 (94.92%)\n",
      "Processing chunk 85 with 100000 rows...\n",
      "Chunk 85 processed in 0.02 seconds\n",
      "Removed 96508 duplicates from this chunk\n",
      "Progress: 8,500,000/8,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 8,069,879 (94.94%)\n",
      "Processing chunk 86 with 100000 rows...\n",
      "Chunk 86 processed in 0.03 seconds\n",
      "Removed 93777 duplicates from this chunk\n",
      "Progress: 8,600,000/8,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 8,163,656 (94.93%)\n",
      "Processing chunk 87 with 100000 rows...\n",
      "Chunk 87 processed in 0.02 seconds\n",
      "Removed 96237 duplicates from this chunk\n",
      "Progress: 8,700,000/8,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 8,259,893 (94.94%)\n",
      "Processing chunk 88 with 100000 rows...\n",
      "Chunk 88 processed in 0.02 seconds\n",
      "Removed 95808 duplicates from this chunk\n",
      "Progress: 8,800,000/8,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 8,355,701 (94.95%)\n",
      "Processing chunk 89 with 100000 rows...\n",
      "Chunk 89 processed in 0.03 seconds\n",
      "Removed 93307 duplicates from this chunk\n",
      "Progress: 8,900,000/8,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 8,449,008 (94.93%)\n",
      "Processing chunk 90 with 100000 rows...\n",
      "Chunk 90 processed in 0.02 seconds\n",
      "Removed 94866 duplicates from this chunk\n",
      "Progress: 9,000,000/9,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 8,543,874 (94.93%)\n",
      "Processing chunk 91 with 100000 rows...\n",
      "Chunk 91 processed in 0.02 seconds\n",
      "Removed 96409 duplicates from this chunk\n",
      "Progress: 9,100,000/9,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 8,640,283 (94.95%)\n",
      "Processing chunk 92 with 100000 rows...\n",
      "Chunk 92 processed in 0.03 seconds\n",
      "Removed 94259 duplicates from this chunk\n",
      "Progress: 9,200,000/9,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 8,734,542 (94.94%)\n",
      "Processing chunk 93 with 100000 rows...\n",
      "Chunk 93 processed in 0.02 seconds\n",
      "Removed 95405 duplicates from this chunk\n",
      "Progress: 9,300,000/9,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 8,829,947 (94.95%)\n",
      "Processing chunk 94 with 100000 rows...\n",
      "Chunk 94 processed in 0.02 seconds\n",
      "Removed 95199 duplicates from this chunk\n",
      "Progress: 9,400,000/9,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 8,925,146 (94.95%)\n",
      "Processing chunk 95 with 100000 rows...\n",
      "Chunk 95 processed in 0.03 seconds\n",
      "Removed 94830 duplicates from this chunk\n",
      "Progress: 9,500,000/9,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 9,019,976 (94.95%)\n",
      "Processing chunk 96 with 100000 rows...\n",
      "Chunk 96 processed in 0.03 seconds\n",
      "Removed 93868 duplicates from this chunk\n",
      "Progress: 9,600,000/9,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 9,113,844 (94.94%)\n",
      "Processing chunk 97 with 100000 rows...\n",
      "Chunk 97 processed in 0.02 seconds\n",
      "Removed 95735 duplicates from this chunk\n",
      "Progress: 9,700,000/9,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 9,209,579 (94.94%)\n",
      "Processing chunk 98 with 100000 rows...\n",
      "Chunk 98 processed in 0.02 seconds\n",
      "Removed 95397 duplicates from this chunk\n",
      "Progress: 9,800,000/9,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 9,304,976 (94.95%)\n",
      "Processing chunk 99 with 100000 rows...\n",
      "Chunk 99 processed in 0.03 seconds\n",
      "Removed 93778 duplicates from this chunk\n",
      "Progress: 9,900,000/9,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 9,398,754 (94.94%)\n",
      "Processing chunk 100 with 100000 rows...\n",
      "Chunk 100 processed in 0.03 seconds\n",
      "Removed 94579 duplicates from this chunk\n",
      "Progress: 10,000,000/10,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 9,493,333 (94.93%)\n",
      "Processing chunk 101 with 100000 rows...\n",
      "Chunk 101 processed in 0.03 seconds\n",
      "Removed 95010 duplicates from this chunk\n",
      "Progress: 10,100,000/10,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 9,588,343 (94.93%)\n",
      "Processing chunk 102 with 100000 rows...\n",
      "Chunk 102 processed in 0.02 seconds\n",
      "Removed 95875 duplicates from this chunk\n",
      "Progress: 10,200,000/10,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 9,684,218 (94.94%)\n",
      "Processing chunk 103 with 100000 rows...\n",
      "Chunk 103 processed in 0.02 seconds\n",
      "Removed 96442 duplicates from this chunk\n",
      "Progress: 10,300,000/10,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 9,780,660 (94.96%)\n",
      "Processing chunk 104 with 100000 rows...\n",
      "Chunk 104 processed in 0.03 seconds\n",
      "Removed 93569 duplicates from this chunk\n",
      "Progress: 10,400,000/10,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 9,874,229 (94.94%)\n",
      "Processing chunk 105 with 100000 rows...\n",
      "Chunk 105 processed in 0.03 seconds\n",
      "Removed 94665 duplicates from this chunk\n",
      "Progress: 10,500,000/10,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 9,968,894 (94.94%)\n",
      "Processing chunk 106 with 100000 rows...\n",
      "Chunk 106 processed in 0.02 seconds\n",
      "Removed 95324 duplicates from this chunk\n",
      "Progress: 10,600,000/10,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 10,064,218 (94.95%)\n",
      "Processing chunk 107 with 100000 rows...\n",
      "Chunk 107 processed in 0.02 seconds\n",
      "Removed 96317 duplicates from this chunk\n",
      "Progress: 10,700,000/10,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 10,160,535 (94.96%)\n",
      "Processing chunk 108 with 100000 rows...\n",
      "Chunk 108 processed in 0.03 seconds\n",
      "Removed 94950 duplicates from this chunk\n",
      "Progress: 10,800,000/10,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 10,255,485 (94.96%)\n",
      "Processing chunk 109 with 100000 rows...\n",
      "Chunk 109 processed in 0.03 seconds\n",
      "Removed 94504 duplicates from this chunk\n",
      "Progress: 10,900,000/10,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 10,349,989 (94.95%)\n",
      "Processing chunk 110 with 100000 rows...\n",
      "Chunk 110 processed in 0.03 seconds\n",
      "Removed 94752 duplicates from this chunk\n",
      "Progress: 11,000,000/11,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 10,444,741 (94.95%)\n",
      "Processing chunk 111 with 100000 rows...\n",
      "Chunk 111 processed in 0.03 seconds\n",
      "Removed 93678 duplicates from this chunk\n",
      "Progress: 11,100,000/11,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 10,538,419 (94.94%)\n",
      "Processing chunk 112 with 100000 rows...\n",
      "Chunk 112 processed in 0.02 seconds\n",
      "Removed 96219 duplicates from this chunk\n",
      "Progress: 11,200,000/11,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 10,634,638 (94.95%)\n",
      "Processing chunk 113 with 100000 rows...\n",
      "Chunk 113 processed in 0.03 seconds\n",
      "Removed 94853 duplicates from this chunk\n",
      "Progress: 11,300,000/11,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 10,729,491 (94.95%)\n",
      "Processing chunk 114 with 100000 rows...\n",
      "Chunk 114 processed in 0.03 seconds\n",
      "Removed 94227 duplicates from this chunk\n",
      "Progress: 11,400,000/11,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 10,823,718 (94.94%)\n",
      "Processing chunk 115 with 100000 rows...\n",
      "Chunk 115 processed in 0.03 seconds\n",
      "Removed 94540 duplicates from this chunk\n",
      "Progress: 11,500,000/11,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 10,918,258 (94.94%)\n",
      "Processing chunk 116 with 100000 rows...\n",
      "Chunk 116 processed in 0.02 seconds\n",
      "Removed 96564 duplicates from this chunk\n",
      "Progress: 11,600,000/11,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 11,014,822 (94.96%)\n",
      "Processing chunk 117 with 100000 rows...\n",
      "Chunk 117 processed in 0.02 seconds\n",
      "Removed 95804 duplicates from this chunk\n",
      "Progress: 11,700,000/11,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 11,110,626 (94.96%)\n",
      "Processing chunk 118 with 100000 rows...\n",
      "Chunk 118 processed in 0.03 seconds\n",
      "Removed 93406 duplicates from this chunk\n",
      "Progress: 11,800,000/11,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 11,204,032 (94.95%)\n",
      "Processing chunk 119 with 100000 rows...\n",
      "Chunk 119 processed in 0.02 seconds\n",
      "Removed 95547 duplicates from this chunk\n",
      "Progress: 11,900,000/11,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 11,299,579 (94.95%)\n",
      "Processing chunk 120 with 100000 rows...\n",
      "Chunk 120 processed in 0.02 seconds\n",
      "Removed 95455 duplicates from this chunk\n",
      "Progress: 12,000,000/12,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 11,395,034 (94.96%)\n",
      "Processing chunk 121 with 100000 rows...\n",
      "Chunk 121 processed in 0.03 seconds\n",
      "Removed 94793 duplicates from this chunk\n",
      "Progress: 12,100,000/12,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 11,489,827 (94.96%)\n",
      "Processing chunk 122 with 100000 rows...\n",
      "Chunk 122 processed in 0.02 seconds\n",
      "Removed 95189 duplicates from this chunk\n",
      "Progress: 12,200,000/12,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 11,585,016 (94.96%)\n",
      "Processing chunk 123 with 100000 rows...\n",
      "Chunk 123 processed in 0.02 seconds\n",
      "Removed 96160 duplicates from this chunk\n",
      "Progress: 12,300,000/12,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 11,681,176 (94.97%)\n",
      "Processing chunk 124 with 100000 rows...\n",
      "Chunk 124 processed in 0.03 seconds\n",
      "Removed 93876 duplicates from this chunk\n",
      "Progress: 12,400,000/12,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 11,775,052 (94.96%)\n",
      "Processing chunk 125 with 100000 rows...\n",
      "Chunk 125 processed in 0.04 seconds\n",
      "Removed 93468 duplicates from this chunk\n",
      "Progress: 12,500,000/12,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 11,868,520 (94.95%)\n",
      "Processing chunk 126 with 100000 rows...\n",
      "Chunk 126 processed in 0.02 seconds\n",
      "Removed 95644 duplicates from this chunk\n",
      "Progress: 12,600,000/12,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 11,964,164 (94.95%)\n",
      "Processing chunk 127 with 100000 rows...\n",
      "Chunk 127 processed in 0.03 seconds\n",
      "Removed 94632 duplicates from this chunk\n",
      "Progress: 12,700,000/12,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 12,058,796 (94.95%)\n",
      "Processing chunk 128 with 100000 rows...\n",
      "Chunk 128 processed in 0.03 seconds\n",
      "Removed 95010 duplicates from this chunk\n",
      "Progress: 12,800,000/12,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 12,153,806 (94.95%)\n",
      "Processing chunk 129 with 100000 rows...\n",
      "Chunk 129 processed in 0.03 seconds\n",
      "Removed 93555 duplicates from this chunk\n",
      "Progress: 12,900,000/12,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 12,247,361 (94.94%)\n",
      "Processing chunk 130 with 100000 rows...\n",
      "Chunk 130 processed in 0.02 seconds\n",
      "Removed 95084 duplicates from this chunk\n",
      "Progress: 13,000,000/13,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 12,342,445 (94.94%)\n",
      "Processing chunk 131 with 100000 rows...\n",
      "Chunk 131 processed in 0.02 seconds\n",
      "Removed 95724 duplicates from this chunk\n",
      "Progress: 13,100,000/13,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 12,438,169 (94.95%)\n",
      "Processing chunk 132 with 100000 rows...\n",
      "Chunk 132 processed in 0.03 seconds\n",
      "Removed 94803 duplicates from this chunk\n",
      "Progress: 13,200,000/13,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 12,532,972 (94.95%)\n",
      "Processing chunk 133 with 100000 rows...\n",
      "Chunk 133 processed in 0.03 seconds\n",
      "Removed 95067 duplicates from this chunk\n",
      "Progress: 13,300,000/13,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 12,628,039 (94.95%)\n",
      "Processing chunk 134 with 100000 rows...\n",
      "Chunk 134 processed in 0.02 seconds\n",
      "Removed 95594 duplicates from this chunk\n",
      "Progress: 13,400,000/13,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 12,723,633 (94.95%)\n",
      "Processing chunk 135 with 100000 rows...\n",
      "Chunk 135 processed in 0.02 seconds\n",
      "Removed 96423 duplicates from this chunk\n",
      "Progress: 13,500,000/13,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 12,820,056 (94.96%)\n",
      "Processing chunk 136 with 100000 rows...\n",
      "Chunk 136 processed in 0.03 seconds\n",
      "Removed 93831 duplicates from this chunk\n",
      "Progress: 13,600,000/13,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 12,913,887 (94.96%)\n",
      "Processing chunk 137 with 100000 rows...\n",
      "Chunk 137 processed in 0.02 seconds\n",
      "Removed 96798 duplicates from this chunk\n",
      "Progress: 13,700,000/13,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 13,010,685 (94.97%)\n",
      "Processing chunk 138 with 100000 rows...\n",
      "Chunk 138 processed in 0.02 seconds\n",
      "Removed 95199 duplicates from this chunk\n",
      "Progress: 13,800,000/13,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 13,105,884 (94.97%)\n",
      "Processing chunk 139 with 100000 rows...\n",
      "Chunk 139 processed in 0.03 seconds\n",
      "Removed 93953 duplicates from this chunk\n",
      "Progress: 13,900,000/13,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 13,199,837 (94.96%)\n",
      "Processing chunk 140 with 100000 rows...\n",
      "Chunk 140 processed in 0.03 seconds\n",
      "Removed 95022 duplicates from this chunk\n",
      "Progress: 14,000,000/14,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 13,294,859 (94.96%)\n",
      "Processing chunk 141 with 100000 rows...\n",
      "Chunk 141 processed in 0.02 seconds\n",
      "Removed 96165 duplicates from this chunk\n",
      "Progress: 14,100,000/14,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 13,391,024 (94.97%)\n",
      "Processing chunk 142 with 100000 rows...\n",
      "Chunk 142 processed in 0.03 seconds\n",
      "Removed 94936 duplicates from this chunk\n",
      "Progress: 14,200,000/14,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 13,485,960 (94.97%)\n",
      "Processing chunk 143 with 100000 rows...\n",
      "Chunk 143 processed in 0.03 seconds\n",
      "Removed 93417 duplicates from this chunk\n",
      "Progress: 14,300,000/14,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 13,579,377 (94.96%)\n",
      "Processing chunk 144 with 100000 rows...\n",
      "Chunk 144 processed in 0.02 seconds\n",
      "Removed 96440 duplicates from this chunk\n",
      "Progress: 14,400,000/14,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 13,675,817 (94.97%)\n",
      "Processing chunk 145 with 100000 rows...\n",
      "Chunk 145 processed in 0.02 seconds\n",
      "Removed 96351 duplicates from this chunk\n",
      "Progress: 14,500,000/14,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 13,772,168 (94.98%)\n",
      "Processing chunk 146 with 100000 rows...\n",
      "Chunk 146 processed in 0.03 seconds\n",
      "Removed 93852 duplicates from this chunk\n",
      "Progress: 14,600,000/14,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 13,866,020 (94.97%)\n",
      "Processing chunk 147 with 100000 rows...\n",
      "Chunk 147 processed in 0.01 seconds\n",
      "Removed 97660 duplicates from this chunk\n",
      "Progress: 14,700,000/14,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 13,963,680 (94.99%)\n",
      "Processing chunk 148 with 100000 rows...\n",
      "Chunk 148 processed in 0.03 seconds\n",
      "Removed 94296 duplicates from this chunk\n",
      "Progress: 14,800,000/14,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 14,057,976 (94.99%)\n",
      "Processing chunk 149 with 100000 rows...\n",
      "Chunk 149 processed in 0.02 seconds\n",
      "Removed 95188 duplicates from this chunk\n",
      "Progress: 14,900,000/14,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 14,153,164 (94.99%)\n",
      "Processing chunk 150 with 100000 rows...\n",
      "Chunk 150 processed in 0.02 seconds\n",
      "Removed 95856 duplicates from this chunk\n",
      "Progress: 15,000,000/15,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 14,249,020 (94.99%)\n",
      "Processing chunk 151 with 100000 rows...\n",
      "Chunk 151 processed in 0.03 seconds\n",
      "Removed 94642 duplicates from this chunk\n",
      "Progress: 15,100,000/15,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 14,343,662 (94.99%)\n",
      "Processing chunk 152 with 100000 rows...\n",
      "Chunk 152 processed in 0.03 seconds\n",
      "Removed 93648 duplicates from this chunk\n",
      "Progress: 15,200,000/15,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 14,437,310 (94.98%)\n",
      "Processing chunk 153 with 100000 rows...\n",
      "Chunk 153 processed in 0.03 seconds\n",
      "Removed 94943 duplicates from this chunk\n",
      "Progress: 15,300,000/15,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 14,532,253 (94.98%)\n",
      "Processing chunk 154 with 100000 rows...\n",
      "Chunk 154 processed in 0.02 seconds\n",
      "Removed 97134 duplicates from this chunk\n",
      "Progress: 15,400,000/15,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 14,629,387 (95.00%)\n",
      "Processing chunk 155 with 100000 rows...\n",
      "Chunk 155 processed in 0.03 seconds\n",
      "Removed 93853 duplicates from this chunk\n",
      "Progress: 15,500,000/15,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 14,723,240 (94.99%)\n",
      "Processing chunk 156 with 100000 rows...\n",
      "Chunk 156 processed in 0.03 seconds\n",
      "Removed 94651 duplicates from this chunk\n",
      "Progress: 15,600,000/15,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 14,817,891 (94.99%)\n",
      "Processing chunk 157 with 100000 rows...\n",
      "Chunk 157 processed in 0.03 seconds\n",
      "Removed 94663 duplicates from this chunk\n",
      "Progress: 15,700,000/15,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 14,912,554 (94.98%)\n",
      "Processing chunk 158 with 100000 rows...\n",
      "Chunk 158 processed in 0.03 seconds\n",
      "Removed 95012 duplicates from this chunk\n",
      "Progress: 15,800,000/15,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 15,007,566 (94.98%)\n",
      "Processing chunk 159 with 100000 rows...\n",
      "Chunk 159 processed in 0.02 seconds\n",
      "Removed 95495 duplicates from this chunk\n",
      "Progress: 15,900,000/15,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 15,103,061 (94.99%)\n",
      "Processing chunk 160 with 100000 rows...\n",
      "Chunk 160 processed in 0.03 seconds\n",
      "Removed 93713 duplicates from this chunk\n",
      "Progress: 16,000,000/16,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 15,196,774 (94.98%)\n",
      "Processing chunk 161 with 100000 rows...\n",
      "Chunk 161 processed in 0.02 seconds\n",
      "Removed 95134 duplicates from this chunk\n",
      "Progress: 16,100,000/16,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 15,291,908 (94.98%)\n",
      "Processing chunk 162 with 100000 rows...\n",
      "Chunk 162 processed in 0.03 seconds\n",
      "Removed 93969 duplicates from this chunk\n",
      "Progress: 16,200,000/16,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 15,385,877 (94.97%)\n",
      "Processing chunk 163 with 100000 rows...\n",
      "Chunk 163 processed in 0.02 seconds\n",
      "Removed 95564 duplicates from this chunk\n",
      "Progress: 16,300,000/16,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 15,481,441 (94.98%)\n",
      "Processing chunk 164 with 100000 rows...\n",
      "Chunk 164 processed in 0.03 seconds\n",
      "Removed 93482 duplicates from this chunk\n",
      "Progress: 16,400,000/16,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 15,574,923 (94.97%)\n",
      "Processing chunk 165 with 100000 rows...\n",
      "Chunk 165 processed in 0.02 seconds\n",
      "Removed 96734 duplicates from this chunk\n",
      "Progress: 16,500,000/16,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 15,671,657 (94.98%)\n",
      "Processing chunk 166 with 100000 rows...\n",
      "Chunk 166 processed in 0.02 seconds\n",
      "Removed 95930 duplicates from this chunk\n",
      "Progress: 16,600,000/16,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 15,767,587 (94.99%)\n",
      "Processing chunk 167 with 100000 rows...\n",
      "Chunk 167 processed in 0.03 seconds\n",
      "Removed 94708 duplicates from this chunk\n",
      "Progress: 16,700,000/16,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 15,862,295 (94.98%)\n",
      "Processing chunk 168 with 100000 rows...\n",
      "Chunk 168 processed in 0.03 seconds\n",
      "Removed 94635 duplicates from this chunk\n",
      "Progress: 16,800,000/16,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 15,956,930 (94.98%)\n",
      "Processing chunk 169 with 100000 rows...\n",
      "Chunk 169 processed in 0.02 seconds\n",
      "Removed 96291 duplicates from this chunk\n",
      "Progress: 16,900,000/16,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 16,053,221 (94.99%)\n",
      "Processing chunk 170 with 100000 rows...\n",
      "Chunk 170 processed in 0.03 seconds\n",
      "Removed 93358 duplicates from this chunk\n",
      "Progress: 17,000,000/17,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 16,146,579 (94.98%)\n",
      "Processing chunk 171 with 100000 rows...\n",
      "Chunk 171 processed in 0.02 seconds\n",
      "Removed 95219 duplicates from this chunk\n",
      "Progress: 17,100,000/17,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 16,241,798 (94.98%)\n",
      "Processing chunk 172 with 100000 rows...\n",
      "Chunk 172 processed in 0.02 seconds\n",
      "Removed 95158 duplicates from this chunk\n",
      "Progress: 17,200,000/17,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 16,336,956 (94.98%)\n",
      "Processing chunk 173 with 100000 rows...\n",
      "Chunk 173 processed in 0.02 seconds\n",
      "Removed 95598 duplicates from this chunk\n",
      "Progress: 17,300,000/17,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 16,432,554 (94.99%)\n",
      "Processing chunk 174 with 100000 rows...\n",
      "Chunk 174 processed in 0.02 seconds\n",
      "Removed 95444 duplicates from this chunk\n",
      "Progress: 17,400,000/17,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 16,527,998 (94.99%)\n",
      "Processing chunk 175 with 100000 rows...\n",
      "Chunk 175 processed in 0.03 seconds\n",
      "Removed 94417 duplicates from this chunk\n",
      "Progress: 17,500,000/17,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 16,622,415 (94.99%)\n",
      "Processing chunk 176 with 100000 rows...\n",
      "Chunk 176 processed in 0.03 seconds\n",
      "Removed 94716 duplicates from this chunk\n",
      "Progress: 17,600,000/17,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 16,717,131 (94.98%)\n",
      "Processing chunk 177 with 100000 rows...\n",
      "Chunk 177 processed in 0.02 seconds\n",
      "Removed 95564 duplicates from this chunk\n",
      "Progress: 17,700,000/17,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 16,812,695 (94.99%)\n",
      "Processing chunk 178 with 100000 rows...\n",
      "Chunk 178 processed in 0.01 seconds\n",
      "Removed 97517 duplicates from this chunk\n",
      "Progress: 17,800,000/17,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 16,910,212 (95.00%)\n",
      "Processing chunk 179 with 100000 rows...\n",
      "Chunk 179 processed in 0.02 seconds\n",
      "Removed 95274 duplicates from this chunk\n",
      "Progress: 17,900,000/17,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 17,005,486 (95.00%)\n",
      "Processing chunk 180 with 100000 rows...\n",
      "Chunk 180 processed in 0.03 seconds\n",
      "Removed 93820 duplicates from this chunk\n",
      "Progress: 18,000,000/18,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 17,099,306 (95.00%)\n",
      "Processing chunk 181 with 100000 rows...\n",
      "Chunk 181 processed in 0.02 seconds\n",
      "Removed 95466 duplicates from this chunk\n",
      "Progress: 18,100,000/18,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 17,194,772 (95.00%)\n",
      "Processing chunk 182 with 100000 rows...\n",
      "Chunk 182 processed in 0.02 seconds\n",
      "Removed 96002 duplicates from this chunk\n",
      "Progress: 18,200,000/18,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 17,290,774 (95.00%)\n",
      "Processing chunk 183 with 100000 rows...\n",
      "Chunk 183 processed in 0.03 seconds\n",
      "Removed 94261 duplicates from this chunk\n",
      "Progress: 18,300,000/18,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 17,385,035 (95.00%)\n",
      "Processing chunk 184 with 100000 rows...\n",
      "Chunk 184 processed in 0.03 seconds\n",
      "Removed 93520 duplicates from this chunk\n",
      "Progress: 18,400,000/18,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 17,478,555 (94.99%)\n",
      "Processing chunk 185 with 100000 rows...\n",
      "Chunk 185 processed in 0.02 seconds\n",
      "Removed 95524 duplicates from this chunk\n",
      "Progress: 18,500,000/18,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 17,574,079 (95.00%)\n",
      "Processing chunk 186 with 100000 rows...\n",
      "Chunk 186 processed in 0.03 seconds\n",
      "Removed 94416 duplicates from this chunk\n",
      "Progress: 18,600,000/18,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 17,668,495 (94.99%)\n",
      "Processing chunk 187 with 100000 rows...\n",
      "Chunk 187 processed in 0.02 seconds\n",
      "Removed 96207 duplicates from this chunk\n",
      "Progress: 18,700,000/18,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 17,764,702 (95.00%)\n",
      "Processing chunk 188 with 100000 rows...\n",
      "Chunk 188 processed in 0.03 seconds\n",
      "Removed 94190 duplicates from this chunk\n",
      "Progress: 18,800,000/18,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 17,858,892 (94.99%)\n",
      "Processing chunk 189 with 100000 rows...\n",
      "Chunk 189 processed in 0.03 seconds\n",
      "Removed 95015 duplicates from this chunk\n",
      "Progress: 18,900,000/18,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 17,953,907 (94.99%)\n",
      "Processing chunk 190 with 100000 rows...\n",
      "Chunk 190 processed in 0.02 seconds\n",
      "Removed 95801 duplicates from this chunk\n",
      "Progress: 19,000,000/19,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 18,049,708 (95.00%)\n",
      "Processing chunk 191 with 100000 rows...\n",
      "Chunk 191 processed in 0.02 seconds\n",
      "Removed 95299 duplicates from this chunk\n",
      "Progress: 19,100,000/19,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 18,145,007 (95.00%)\n",
      "Processing chunk 192 with 100000 rows...\n",
      "Chunk 192 processed in 0.03 seconds\n",
      "Removed 93460 duplicates from this chunk\n",
      "Progress: 19,200,000/19,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 18,238,467 (94.99%)\n",
      "Processing chunk 193 with 100000 rows...\n",
      "Chunk 193 processed in 0.02 seconds\n",
      "Removed 95558 duplicates from this chunk\n",
      "Progress: 19,300,000/19,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 18,334,025 (94.99%)\n",
      "Processing chunk 194 with 100000 rows...\n",
      "Chunk 194 processed in 0.02 seconds\n",
      "Removed 96245 duplicates from this chunk\n",
      "Progress: 19,400,000/19,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 18,430,270 (95.00%)\n",
      "Processing chunk 195 with 100000 rows...\n",
      "Chunk 195 processed in 0.03 seconds\n",
      "Removed 94676 duplicates from this chunk\n",
      "Progress: 19,500,000/19,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 18,524,946 (95.00%)\n",
      "Processing chunk 196 with 100000 rows...\n",
      "Chunk 196 processed in 0.03 seconds\n",
      "Removed 93535 duplicates from this chunk\n",
      "Progress: 19,600,000/19,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 18,618,481 (94.99%)\n",
      "Processing chunk 197 with 100000 rows...\n",
      "Chunk 197 processed in 0.02 seconds\n",
      "Removed 95373 duplicates from this chunk\n",
      "Progress: 19,700,000/19,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 18,713,854 (94.99%)\n",
      "Processing chunk 198 with 100000 rows...\n",
      "Chunk 198 processed in 0.02 seconds\n",
      "Removed 96157 duplicates from this chunk\n",
      "Progress: 19,800,000/19,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 18,810,011 (95.00%)\n",
      "Processing chunk 199 with 100000 rows...\n",
      "Chunk 199 processed in 0.03 seconds\n",
      "Removed 95064 duplicates from this chunk\n",
      "Progress: 19,900,000/19,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 18,905,075 (95.00%)\n",
      "Processing chunk 200 with 100000 rows...\n",
      "Chunk 200 processed in 0.03 seconds\n",
      "Removed 93836 duplicates from this chunk\n",
      "Progress: 20,000,000/20,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 18,998,911 (94.99%)\n",
      "Processing chunk 201 with 100000 rows...\n",
      "Chunk 201 processed in 0.03 seconds\n",
      "Removed 94162 duplicates from this chunk\n",
      "Progress: 20,100,000/20,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 19,093,073 (94.99%)\n",
      "Processing chunk 202 with 100000 rows...\n",
      "Chunk 202 processed in 0.02 seconds\n",
      "Removed 96628 duplicates from this chunk\n",
      "Progress: 20,200,000/20,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 19,189,701 (95.00%)\n",
      "Processing chunk 203 with 100000 rows...\n",
      "Chunk 203 processed in 0.02 seconds\n",
      "Removed 96111 duplicates from this chunk\n",
      "Progress: 20,300,000/20,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 19,285,812 (95.00%)\n",
      "Processing chunk 204 with 100000 rows...\n",
      "Chunk 204 processed in 0.03 seconds\n",
      "Removed 93789 duplicates from this chunk\n",
      "Progress: 20,400,000/20,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 19,379,601 (95.00%)\n",
      "Processing chunk 205 with 100000 rows...\n",
      "Chunk 205 processed in 0.02 seconds\n",
      "Removed 95417 duplicates from this chunk\n",
      "Progress: 20,500,000/20,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 19,475,018 (95.00%)\n",
      "Processing chunk 206 with 100000 rows...\n",
      "Chunk 206 processed in 0.03 seconds\n",
      "Removed 94549 duplicates from this chunk\n",
      "Progress: 20,600,000/20,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 19,569,567 (95.00%)\n",
      "Processing chunk 207 with 100000 rows...\n",
      "Chunk 207 processed in 0.03 seconds\n",
      "Removed 95111 duplicates from this chunk\n",
      "Progress: 20,700,000/20,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 19,664,678 (95.00%)\n",
      "Processing chunk 208 with 100000 rows...\n",
      "Chunk 208 processed in 0.02 seconds\n",
      "Removed 96093 duplicates from this chunk\n",
      "Progress: 20,800,000/20,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 19,760,771 (95.00%)\n",
      "Processing chunk 209 with 100000 rows...\n",
      "Chunk 209 processed in 0.03 seconds\n",
      "Removed 93919 duplicates from this chunk\n",
      "Progress: 20,900,000/20,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 19,854,690 (95.00%)\n",
      "Processing chunk 210 with 100000 rows...\n",
      "Chunk 210 processed in 0.02 seconds\n",
      "Removed 96285 duplicates from this chunk\n",
      "Progress: 21,000,000/21,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 19,950,975 (95.00%)\n",
      "Processing chunk 211 with 100000 rows...\n",
      "Chunk 211 processed in 0.03 seconds\n",
      "Removed 94955 duplicates from this chunk\n",
      "Progress: 21,100,000/21,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 20,045,930 (95.00%)\n",
      "Processing chunk 212 with 100000 rows...\n",
      "Chunk 212 processed in 0.03 seconds\n",
      "Removed 95061 duplicates from this chunk\n",
      "Progress: 21,200,000/21,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 20,140,991 (95.00%)\n",
      "Processing chunk 213 with 100000 rows...\n",
      "Chunk 213 processed in 0.02 seconds\n",
      "Removed 95161 duplicates from this chunk\n",
      "Progress: 21,300,000/21,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 20,236,152 (95.01%)\n",
      "Processing chunk 214 with 100000 rows...\n",
      "Chunk 214 processed in 0.03 seconds\n",
      "Removed 93890 duplicates from this chunk\n",
      "Progress: 21,400,000/21,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 20,330,042 (95.00%)\n",
      "Processing chunk 215 with 100000 rows...\n",
      "Chunk 215 processed in 0.02 seconds\n",
      "Removed 96358 duplicates from this chunk\n",
      "Progress: 21,500,000/21,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 20,426,400 (95.01%)\n",
      "Processing chunk 216 with 100000 rows...\n",
      "Chunk 216 processed in 0.02 seconds\n",
      "Removed 96476 duplicates from this chunk\n",
      "Progress: 21,600,000/21,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 20,522,876 (95.01%)\n",
      "Processing chunk 217 with 100000 rows...\n",
      "Chunk 217 processed in 0.03 seconds\n",
      "Removed 94009 duplicates from this chunk\n",
      "Progress: 21,700,000/21,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 20,616,885 (95.01%)\n",
      "Processing chunk 218 with 100000 rows...\n",
      "Chunk 218 processed in 0.03 seconds\n",
      "Removed 93665 duplicates from this chunk\n",
      "Progress: 21,800,000/21,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 20,710,550 (95.00%)\n",
      "Processing chunk 219 with 100000 rows...\n",
      "Chunk 219 processed in 0.03 seconds\n",
      "Removed 94552 duplicates from this chunk\n",
      "Progress: 21,900,000/21,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 20,805,102 (95.00%)\n",
      "Processing chunk 220 with 100000 rows...\n",
      "Chunk 220 processed in 0.02 seconds\n",
      "Removed 95366 duplicates from this chunk\n",
      "Progress: 22,000,000/22,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 20,900,468 (95.00%)\n",
      "Processing chunk 221 with 100000 rows...\n",
      "Chunk 221 processed in 0.02 seconds\n",
      "Removed 95011 duplicates from this chunk\n",
      "Progress: 22,100,000/22,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 20,995,479 (95.00%)\n",
      "Processing chunk 222 with 100000 rows...\n",
      "Chunk 222 processed in 0.03 seconds\n",
      "Removed 94880 duplicates from this chunk\n",
      "Progress: 22,200,000/22,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 21,090,359 (95.00%)\n",
      "Processing chunk 223 with 100000 rows...\n",
      "Chunk 223 processed in 0.03 seconds\n",
      "Removed 94119 duplicates from this chunk\n",
      "Progress: 22,300,000/22,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 21,184,478 (95.00%)\n",
      "Processing chunk 224 with 100000 rows...\n",
      "Chunk 224 processed in 0.02 seconds\n",
      "Removed 95541 duplicates from this chunk\n",
      "Progress: 22,400,000/22,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 21,280,019 (95.00%)\n",
      "Processing chunk 225 with 100000 rows...\n",
      "Chunk 225 processed in 0.01 seconds\n",
      "Removed 97384 duplicates from this chunk\n",
      "Progress: 22,500,000/22,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 21,377,403 (95.01%)\n",
      "Processing chunk 226 with 100000 rows...\n",
      "Chunk 226 processed in 0.02 seconds\n",
      "Removed 95326 duplicates from this chunk\n",
      "Progress: 22,600,000/22,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 21,472,729 (95.01%)\n",
      "Processing chunk 227 with 100000 rows...\n",
      "Chunk 227 processed in 0.03 seconds\n",
      "Removed 94058 duplicates from this chunk\n",
      "Progress: 22,700,000/22,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 21,566,787 (95.01%)\n",
      "Processing chunk 228 with 100000 rows...\n",
      "Chunk 228 processed in 0.03 seconds\n",
      "Removed 94388 duplicates from this chunk\n",
      "Progress: 22,800,000/22,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 21,661,175 (95.01%)\n",
      "Processing chunk 229 with 100000 rows...\n",
      "Chunk 229 processed in 0.02 seconds\n",
      "Removed 96049 duplicates from this chunk\n",
      "Progress: 22,900,000/22,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 21,757,224 (95.01%)\n",
      "Processing chunk 230 with 100000 rows...\n",
      "Chunk 230 processed in 0.03 seconds\n",
      "Removed 94244 duplicates from this chunk\n",
      "Progress: 23,000,000/23,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 21,851,468 (95.01%)\n",
      "Processing chunk 231 with 100000 rows...\n",
      "Chunk 231 processed in 0.03 seconds\n",
      "Removed 94324 duplicates from this chunk\n",
      "Progress: 23,100,000/23,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 21,945,792 (95.00%)\n",
      "Processing chunk 232 with 100000 rows...\n",
      "Chunk 232 processed in 0.03 seconds\n",
      "Removed 94300 duplicates from this chunk\n",
      "Progress: 23,200,000/23,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 22,040,092 (95.00%)\n",
      "Processing chunk 233 with 100000 rows...\n",
      "Chunk 233 processed in 0.02 seconds\n",
      "Removed 95456 duplicates from this chunk\n",
      "Progress: 23,300,000/23,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 22,135,548 (95.00%)\n",
      "Processing chunk 234 with 100000 rows...\n",
      "Chunk 234 processed in 0.03 seconds\n",
      "Removed 93968 duplicates from this chunk\n",
      "Progress: 23,400,000/23,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 22,229,516 (95.00%)\n",
      "Processing chunk 235 with 100000 rows...\n",
      "Chunk 235 processed in 0.03 seconds\n",
      "Removed 94723 duplicates from this chunk\n",
      "Progress: 23,500,000/23,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 22,324,239 (95.00%)\n",
      "Processing chunk 236 with 100000 rows...\n",
      "Chunk 236 processed in 0.02 seconds\n",
      "Removed 95956 duplicates from this chunk\n",
      "Progress: 23,600,000/23,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 22,420,195 (95.00%)\n",
      "Processing chunk 237 with 100000 rows...\n",
      "Chunk 237 processed in 0.03 seconds\n",
      "Removed 93986 duplicates from this chunk\n",
      "Progress: 23,700,000/23,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 22,514,181 (95.00%)\n",
      "Processing chunk 238 with 100000 rows...\n",
      "Chunk 238 processed in 0.03 seconds\n",
      "Removed 94702 duplicates from this chunk\n",
      "Progress: 23,800,000/23,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 22,608,883 (95.00%)\n",
      "Processing chunk 239 with 100000 rows...\n",
      "Chunk 239 processed in 0.03 seconds\n",
      "Removed 94925 duplicates from this chunk\n",
      "Progress: 23,900,000/23,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 22,703,808 (95.00%)\n",
      "Processing chunk 240 with 100000 rows...\n",
      "Chunk 240 processed in 0.03 seconds\n",
      "Removed 94764 duplicates from this chunk\n",
      "Progress: 24,000,000/24,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 22,798,572 (94.99%)\n",
      "Processing chunk 241 with 100000 rows...\n",
      "Chunk 241 processed in 0.03 seconds\n",
      "Removed 94507 duplicates from this chunk\n",
      "Progress: 24,100,000/24,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 22,893,079 (94.99%)\n",
      "Processing chunk 242 with 100000 rows...\n",
      "Chunk 242 processed in 0.02 seconds\n",
      "Removed 95979 duplicates from this chunk\n",
      "Progress: 24,200,000/24,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 22,989,058 (95.00%)\n",
      "Processing chunk 243 with 100000 rows...\n",
      "Chunk 243 processed in 0.03 seconds\n",
      "Removed 94918 duplicates from this chunk\n",
      "Progress: 24,300,000/24,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 23,083,976 (95.00%)\n",
      "Processing chunk 244 with 100000 rows...\n",
      "Chunk 244 processed in 0.02 seconds\n",
      "Removed 95152 duplicates from this chunk\n",
      "Progress: 24,400,000/24,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 23,179,128 (95.00%)\n",
      "Processing chunk 245 with 100000 rows...\n",
      "Chunk 245 processed in 0.03 seconds\n",
      "Removed 94941 duplicates from this chunk\n",
      "Progress: 24,500,000/24,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 23,274,069 (95.00%)\n",
      "Processing chunk 246 with 100000 rows...\n",
      "Chunk 246 processed in 0.03 seconds\n",
      "Removed 95052 duplicates from this chunk\n",
      "Progress: 24,600,000/24,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 23,369,121 (95.00%)\n",
      "Processing chunk 247 with 100000 rows...\n",
      "Chunk 247 processed in 0.03 seconds\n",
      "Removed 94629 duplicates from this chunk\n",
      "Progress: 24,700,000/24,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 23,463,750 (94.99%)\n",
      "Processing chunk 248 with 100000 rows...\n",
      "Chunk 248 processed in 0.02 seconds\n",
      "Removed 95675 duplicates from this chunk\n",
      "Progress: 24,800,000/24,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 23,559,425 (95.00%)\n",
      "Processing chunk 249 with 100000 rows...\n",
      "Chunk 249 processed in 0.02 seconds\n",
      "Removed 95968 duplicates from this chunk\n",
      "Progress: 24,900,000/24,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 23,655,393 (95.00%)\n",
      "Processing chunk 250 with 100000 rows...\n",
      "Chunk 250 processed in 0.03 seconds\n",
      "Removed 94196 duplicates from this chunk\n",
      "Progress: 25,000,000/25,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 23,749,589 (95.00%)\n",
      "Processing chunk 251 with 100000 rows...\n",
      "Chunk 251 processed in 0.03 seconds\n",
      "Removed 93679 duplicates from this chunk\n",
      "Progress: 25,100,000/25,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 23,843,268 (94.99%)\n",
      "Processing chunk 252 with 100000 rows...\n",
      "Chunk 252 processed in 0.04 seconds\n",
      "Removed 95687 duplicates from this chunk\n",
      "Progress: 25,200,000/25,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 23,938,955 (95.00%)\n",
      "Processing chunk 253 with 100000 rows...\n",
      "Chunk 253 processed in 0.02 seconds\n",
      "Removed 95370 duplicates from this chunk\n",
      "Progress: 25,300,000/25,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 24,034,325 (95.00%)\n",
      "Processing chunk 254 with 100000 rows...\n",
      "Chunk 254 processed in 0.02 seconds\n",
      "Removed 96037 duplicates from this chunk\n",
      "Progress: 25,400,000/25,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 24,130,362 (95.00%)\n",
      "Processing chunk 255 with 100000 rows...\n",
      "Chunk 255 processed in 0.03 seconds\n",
      "Removed 94543 duplicates from this chunk\n",
      "Progress: 25,500,000/25,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 24,224,905 (95.00%)\n",
      "Processing chunk 256 with 100000 rows...\n",
      "Chunk 256 processed in 0.02 seconds\n",
      "Removed 95698 duplicates from this chunk\n",
      "Progress: 25,600,000/25,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 24,320,603 (95.00%)\n",
      "Processing chunk 257 with 100000 rows...\n",
      "Chunk 257 processed in 0.02 seconds\n",
      "Removed 96786 duplicates from this chunk\n",
      "Progress: 25,700,000/25,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 24,417,389 (95.01%)\n",
      "Processing chunk 258 with 100000 rows...\n",
      "Chunk 258 processed in 0.03 seconds\n",
      "Removed 94323 duplicates from this chunk\n",
      "Progress: 25,800,000/25,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 24,511,712 (95.01%)\n",
      "Processing chunk 259 with 100000 rows...\n",
      "Chunk 259 processed in 0.03 seconds\n",
      "Removed 93641 duplicates from this chunk\n",
      "Progress: 25,900,000/25,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 24,605,353 (95.00%)\n",
      "Processing chunk 260 with 100000 rows...\n",
      "Chunk 260 processed in 0.03 seconds\n",
      "Removed 94905 duplicates from this chunk\n",
      "Progress: 26,000,000/26,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 24,700,258 (95.00%)\n",
      "Processing chunk 261 with 100000 rows...\n",
      "Chunk 261 processed in 0.02 seconds\n",
      "Removed 95863 duplicates from this chunk\n",
      "Progress: 26,100,000/26,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 24,796,121 (95.00%)\n",
      "Processing chunk 262 with 100000 rows...\n",
      "Chunk 262 processed in 0.03 seconds\n",
      "Removed 94822 duplicates from this chunk\n",
      "Progress: 26,200,000/26,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 24,890,943 (95.00%)\n",
      "Processing chunk 263 with 100000 rows...\n",
      "Chunk 263 processed in 0.03 seconds\n",
      "Removed 93251 duplicates from this chunk\n",
      "Progress: 26,300,000/26,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 24,984,194 (95.00%)\n",
      "Processing chunk 264 with 100000 rows...\n",
      "Chunk 264 processed in 0.03 seconds\n",
      "Removed 94756 duplicates from this chunk\n",
      "Progress: 26,400,000/26,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 25,078,950 (95.00%)\n",
      "Processing chunk 265 with 100000 rows...\n",
      "Chunk 265 processed in 0.03 seconds\n",
      "Removed 93829 duplicates from this chunk\n",
      "Progress: 26,500,000/26,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 25,172,779 (94.99%)\n",
      "Processing chunk 266 with 100000 rows...\n",
      "Chunk 266 processed in 0.03 seconds\n",
      "Removed 94120 duplicates from this chunk\n",
      "Progress: 26,600,000/26,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 25,266,899 (94.99%)\n",
      "Processing chunk 267 with 100000 rows...\n",
      "Chunk 267 processed in 0.01 seconds\n",
      "Removed 97635 duplicates from this chunk\n",
      "Progress: 26,700,000/26,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 25,364,534 (95.00%)\n",
      "Processing chunk 268 with 100000 rows...\n",
      "Chunk 268 processed in 0.03 seconds\n",
      "Removed 94439 duplicates from this chunk\n",
      "Progress: 26,800,000/26,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 25,458,973 (95.00%)\n",
      "Processing chunk 269 with 100000 rows...\n",
      "Chunk 269 processed in 0.03 seconds\n",
      "Removed 94201 duplicates from this chunk\n",
      "Progress: 26,900,000/26,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 25,553,174 (94.99%)\n",
      "Processing chunk 270 with 100000 rows...\n",
      "Chunk 270 processed in 0.03 seconds\n",
      "Removed 94782 duplicates from this chunk\n",
      "Progress: 27,000,000/27,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 25,647,956 (94.99%)\n",
      "Processing chunk 271 with 100000 rows...\n",
      "Chunk 271 processed in 0.03 seconds\n",
      "Removed 93515 duplicates from this chunk\n",
      "Progress: 27,100,000/27,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 25,741,471 (94.99%)\n",
      "Processing chunk 272 with 100000 rows...\n",
      "Chunk 272 processed in 0.03 seconds\n",
      "Removed 93997 duplicates from this chunk\n",
      "Progress: 27,200,000/27,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 25,835,468 (94.98%)\n",
      "Processing chunk 273 with 100000 rows...\n",
      "Chunk 273 processed in 0.02 seconds\n",
      "Removed 96975 duplicates from this chunk\n",
      "Progress: 27,300,000/27,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 25,932,443 (94.99%)\n",
      "Processing chunk 274 with 100000 rows...\n",
      "Chunk 274 processed in 0.02 seconds\n",
      "Removed 95513 duplicates from this chunk\n",
      "Progress: 27,400,000/27,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 26,027,956 (94.99%)\n",
      "Processing chunk 275 with 100000 rows...\n",
      "Chunk 275 processed in 0.05 seconds\n",
      "Removed 89996 duplicates from this chunk\n",
      "Progress: 27,500,000/27,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 26,117,952 (94.97%)\n",
      "Processing chunk 276 with 100000 rows...\n",
      "Chunk 276 processed in 0.02 seconds\n",
      "Removed 95918 duplicates from this chunk\n",
      "Progress: 27,600,000/27,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 26,213,870 (94.98%)\n",
      "Processing chunk 277 with 100000 rows...\n",
      "Chunk 277 processed in 0.02 seconds\n",
      "Removed 96501 duplicates from this chunk\n",
      "Progress: 27,700,000/27,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 26,310,371 (94.98%)\n",
      "Processing chunk 278 with 100000 rows...\n",
      "Chunk 278 processed in 0.03 seconds\n",
      "Removed 94416 duplicates from this chunk\n",
      "Progress: 27,800,000/27,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 26,404,787 (94.98%)\n",
      "Processing chunk 279 with 100000 rows...\n",
      "Chunk 279 processed in 0.03 seconds\n",
      "Removed 93673 duplicates from this chunk\n",
      "Progress: 27,900,000/27,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 26,498,460 (94.98%)\n",
      "Processing chunk 280 with 100000 rows...\n",
      "Chunk 280 processed in 0.02 seconds\n",
      "Removed 95056 duplicates from this chunk\n",
      "Progress: 28,000,000/28,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 26,593,516 (94.98%)\n",
      "Processing chunk 281 with 100000 rows...\n",
      "Chunk 281 processed in 0.02 seconds\n",
      "Removed 95990 duplicates from this chunk\n",
      "Progress: 28,100,000/28,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 26,689,506 (94.98%)\n",
      "Processing chunk 282 with 100000 rows...\n",
      "Chunk 282 processed in 0.03 seconds\n",
      "Removed 94133 duplicates from this chunk\n",
      "Progress: 28,200,000/28,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 26,783,639 (94.98%)\n",
      "Processing chunk 283 with 100000 rows...\n",
      "Chunk 283 processed in 0.03 seconds\n",
      "Removed 94675 duplicates from this chunk\n",
      "Progress: 28,300,000/28,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 26,878,314 (94.98%)\n",
      "Processing chunk 284 with 100000 rows...\n",
      "Chunk 284 processed in 0.03 seconds\n",
      "Removed 94062 duplicates from this chunk\n",
      "Progress: 28,400,000/28,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 26,972,376 (94.97%)\n",
      "Processing chunk 285 with 100000 rows...\n",
      "Chunk 285 processed in 0.03 seconds\n",
      "Removed 93899 duplicates from this chunk\n",
      "Progress: 28,500,000/28,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 27,066,275 (94.97%)\n",
      "Processing chunk 286 with 100000 rows...\n",
      "Chunk 286 processed in 0.03 seconds\n",
      "Removed 94504 duplicates from this chunk\n",
      "Progress: 28,600,000/28,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 27,160,779 (94.97%)\n",
      "Processing chunk 287 with 100000 rows...\n",
      "Chunk 287 processed in 0.03 seconds\n",
      "Removed 93790 duplicates from this chunk\n",
      "Progress: 28,700,000/28,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 27,254,569 (94.96%)\n",
      "Processing chunk 288 with 100000 rows...\n",
      "Chunk 288 processed in 0.02 seconds\n",
      "Removed 96394 duplicates from this chunk\n",
      "Progress: 28,800,000/28,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 27,350,963 (94.97%)\n",
      "Processing chunk 289 with 100000 rows...\n",
      "Chunk 289 processed in 0.02 seconds\n",
      "Removed 95923 duplicates from this chunk\n",
      "Progress: 28,900,000/28,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 27,446,886 (94.97%)\n",
      "Processing chunk 290 with 100000 rows...\n",
      "Chunk 290 processed in 0.03 seconds\n",
      "Removed 93792 duplicates from this chunk\n",
      "Progress: 29,000,000/29,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 27,540,678 (94.97%)\n",
      "Processing chunk 291 with 100000 rows...\n",
      "Chunk 291 processed in 0.02 seconds\n",
      "Removed 95336 duplicates from this chunk\n",
      "Progress: 29,100,000/29,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 27,636,014 (94.97%)\n",
      "Processing chunk 292 with 100000 rows...\n",
      "Chunk 292 processed in 0.02 seconds\n",
      "Removed 96112 duplicates from this chunk\n",
      "Progress: 29,200,000/29,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 27,732,126 (94.97%)\n",
      "Processing chunk 293 with 100000 rows...\n",
      "Chunk 293 processed in 0.03 seconds\n",
      "Removed 94230 duplicates from this chunk\n",
      "Progress: 29,300,000/29,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 27,826,356 (94.97%)\n",
      "Processing chunk 294 with 100000 rows...\n",
      "Chunk 294 processed in 0.03 seconds\n",
      "Removed 95157 duplicates from this chunk\n",
      "Progress: 29,400,000/29,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 27,921,513 (94.97%)\n",
      "Processing chunk 295 with 100000 rows...\n",
      "Chunk 295 processed in 0.03 seconds\n",
      "Removed 93729 duplicates from this chunk\n",
      "Progress: 29,500,000/29,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 28,015,242 (94.97%)\n",
      "Processing chunk 296 with 100000 rows...\n",
      "Chunk 296 processed in 0.03 seconds\n",
      "Removed 94465 duplicates from this chunk\n",
      "Progress: 29,600,000/29,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 28,109,707 (94.97%)\n",
      "Processing chunk 297 with 100000 rows...\n",
      "Chunk 297 processed in 0.03 seconds\n",
      "Removed 94771 duplicates from this chunk\n",
      "Progress: 29,700,000/29,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 28,204,478 (94.96%)\n",
      "Processing chunk 298 with 100000 rows...\n",
      "Chunk 298 processed in 0.02 seconds\n",
      "Removed 97213 duplicates from this chunk\n",
      "Progress: 29,800,000/29,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 28,301,691 (94.97%)\n",
      "Processing chunk 299 with 100000 rows...\n",
      "Chunk 299 processed in 0.03 seconds\n",
      "Removed 93730 duplicates from this chunk\n",
      "Progress: 29,900,000/29,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 28,395,421 (94.97%)\n",
      "Processing chunk 300 with 100000 rows...\n",
      "Chunk 300 processed in 0.03 seconds\n",
      "Removed 95056 duplicates from this chunk\n",
      "Progress: 30,000,000/30,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 28,490,477 (94.97%)\n",
      "Processing chunk 301 with 100000 rows...\n",
      "Chunk 301 processed in 0.03 seconds\n",
      "Removed 95046 duplicates from this chunk\n",
      "Progress: 30,100,000/30,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 28,585,523 (94.97%)\n",
      "Processing chunk 302 with 100000 rows...\n",
      "Chunk 302 processed in 0.03 seconds\n",
      "Removed 93699 duplicates from this chunk\n",
      "Progress: 30,200,000/30,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 28,679,222 (94.96%)\n",
      "Processing chunk 303 with 100000 rows...\n",
      "Chunk 303 processed in 0.03 seconds\n",
      "Removed 94766 duplicates from this chunk\n",
      "Progress: 30,300,000/30,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 28,773,988 (94.96%)\n",
      "Processing chunk 304 with 100000 rows...\n",
      "Chunk 304 processed in 0.02 seconds\n",
      "Removed 95180 duplicates from this chunk\n",
      "Progress: 30,400,000/30,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 28,869,168 (94.96%)\n",
      "Processing chunk 305 with 100000 rows...\n",
      "Chunk 305 processed in 0.02 seconds\n",
      "Removed 95687 duplicates from this chunk\n",
      "Progress: 30,500,000/30,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 28,964,855 (94.97%)\n",
      "Processing chunk 306 with 100000 rows...\n",
      "Chunk 306 processed in 0.02 seconds\n",
      "Removed 95631 duplicates from this chunk\n",
      "Progress: 30,600,000/30,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 29,060,486 (94.97%)\n",
      "Processing chunk 307 with 100000 rows...\n",
      "Chunk 307 processed in 0.03 seconds\n",
      "Removed 94887 duplicates from this chunk\n",
      "Progress: 30,700,000/30,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 29,155,373 (94.97%)\n",
      "Processing chunk 308 with 100000 rows...\n",
      "Chunk 308 processed in 0.03 seconds\n",
      "Removed 94263 duplicates from this chunk\n",
      "Progress: 30,800,000/30,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 29,249,636 (94.97%)\n",
      "Processing chunk 309 with 100000 rows...\n",
      "Chunk 309 processed in 0.02 seconds\n",
      "Removed 95972 duplicates from this chunk\n",
      "Progress: 30,900,000/30,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 29,345,608 (94.97%)\n",
      "Processing chunk 310 with 100000 rows...\n",
      "Chunk 310 processed in 0.02 seconds\n",
      "Removed 95698 duplicates from this chunk\n",
      "Progress: 31,000,000/31,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 29,441,306 (94.97%)\n",
      "Processing chunk 311 with 100000 rows...\n",
      "Chunk 311 processed in 0.02 seconds\n",
      "Removed 95517 duplicates from this chunk\n",
      "Progress: 31,100,000/31,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 29,536,823 (94.97%)\n",
      "Processing chunk 312 with 100000 rows...\n",
      "Chunk 312 processed in 0.03 seconds\n",
      "Removed 93737 duplicates from this chunk\n",
      "Progress: 31,200,000/31,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 29,630,560 (94.97%)\n",
      "Processing chunk 313 with 100000 rows...\n",
      "Chunk 313 processed in 0.03 seconds\n",
      "Removed 95104 duplicates from this chunk\n",
      "Progress: 31,300,000/31,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 29,725,664 (94.97%)\n",
      "Processing chunk 314 with 100000 rows...\n",
      "Chunk 314 processed in 0.02 seconds\n",
      "Removed 95309 duplicates from this chunk\n",
      "Progress: 31,400,000/31,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 29,820,973 (94.97%)\n",
      "Processing chunk 315 with 100000 rows...\n",
      "Chunk 315 processed in 0.03 seconds\n",
      "Removed 95079 duplicates from this chunk\n",
      "Progress: 31,500,000/31,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 29,916,052 (94.97%)\n",
      "Processing chunk 316 with 100000 rows...\n",
      "Chunk 316 processed in 0.03 seconds\n",
      "Removed 95223 duplicates from this chunk\n",
      "Progress: 31,600,000/31,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 30,011,275 (94.97%)\n",
      "Processing chunk 317 with 100000 rows...\n",
      "Chunk 317 processed in 0.02 seconds\n",
      "Removed 95644 duplicates from this chunk\n",
      "Progress: 31,700,000/31,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 30,106,919 (94.97%)\n",
      "Processing chunk 318 with 100000 rows...\n",
      "Chunk 318 processed in 0.02 seconds\n",
      "Removed 95747 duplicates from this chunk\n",
      "Progress: 31,800,000/31,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 30,202,666 (94.98%)\n",
      "Processing chunk 319 with 100000 rows...\n",
      "Chunk 319 processed in 0.03 seconds\n",
      "Removed 94951 duplicates from this chunk\n",
      "Progress: 31,900,000/31,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 30,297,617 (94.98%)\n",
      "Processing chunk 320 with 100000 rows...\n",
      "Chunk 320 processed in 0.03 seconds\n",
      "Removed 95205 duplicates from this chunk\n",
      "Progress: 32,000,000/32,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 30,392,822 (94.98%)\n",
      "Processing chunk 321 with 100000 rows...\n",
      "Chunk 321 processed in 0.03 seconds\n",
      "Removed 94685 duplicates from this chunk\n",
      "Progress: 32,100,000/32,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 30,487,507 (94.98%)\n",
      "Processing chunk 322 with 100000 rows...\n",
      "Chunk 322 processed in 0.03 seconds\n",
      "Removed 95025 duplicates from this chunk\n",
      "Progress: 32,200,000/32,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 30,582,532 (94.98%)\n",
      "Processing chunk 323 with 100000 rows...\n",
      "Chunk 323 processed in 0.02 seconds\n",
      "Removed 96490 duplicates from this chunk\n",
      "Progress: 32,300,000/32,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 30,679,022 (94.98%)\n",
      "Processing chunk 324 with 100000 rows...\n",
      "Chunk 324 processed in 0.03 seconds\n",
      "Removed 94296 duplicates from this chunk\n",
      "Progress: 32,400,000/32,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 30,773,318 (94.98%)\n",
      "Processing chunk 325 with 100000 rows...\n",
      "Chunk 325 processed in 0.02 seconds\n",
      "Removed 96848 duplicates from this chunk\n",
      "Progress: 32,500,000/32,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 30,870,166 (94.99%)\n",
      "Processing chunk 326 with 100000 rows...\n",
      "Chunk 326 processed in 0.02 seconds\n",
      "Removed 96077 duplicates from this chunk\n",
      "Progress: 32,600,000/32,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 30,966,243 (94.99%)\n",
      "Processing chunk 327 with 100000 rows...\n",
      "Chunk 327 processed in 0.03 seconds\n",
      "Removed 94992 duplicates from this chunk\n",
      "Progress: 32,700,000/32,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 31,061,235 (94.99%)\n",
      "Processing chunk 328 with 100000 rows...\n",
      "Chunk 328 processed in 0.03 seconds\n",
      "Removed 94618 duplicates from this chunk\n",
      "Progress: 32,800,000/32,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 31,155,853 (94.99%)\n",
      "Processing chunk 329 with 100000 rows...\n",
      "Chunk 329 processed in 0.03 seconds\n",
      "Removed 94781 duplicates from this chunk\n",
      "Progress: 32,900,000/32,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 31,250,634 (94.99%)\n",
      "Processing chunk 330 with 100000 rows...\n",
      "Chunk 330 processed in 0.02 seconds\n",
      "Removed 96288 duplicates from this chunk\n",
      "Progress: 33,000,000/33,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 31,346,922 (94.99%)\n",
      "Processing chunk 331 with 100000 rows...\n",
      "Chunk 331 processed in 0.02 seconds\n",
      "Removed 95637 duplicates from this chunk\n",
      "Progress: 33,100,000/33,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 31,442,559 (94.99%)\n",
      "Processing chunk 332 with 100000 rows...\n",
      "Chunk 332 processed in 0.03 seconds\n",
      "Removed 94752 duplicates from this chunk\n",
      "Progress: 33,200,000/33,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 31,537,311 (94.99%)\n",
      "Processing chunk 333 with 100000 rows...\n",
      "Chunk 333 processed in 0.02 seconds\n",
      "Removed 96484 duplicates from this chunk\n",
      "Progress: 33,300,000/33,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 31,633,795 (95.00%)\n",
      "Processing chunk 334 with 100000 rows...\n",
      "Chunk 334 processed in 0.03 seconds\n",
      "Removed 95337 duplicates from this chunk\n",
      "Progress: 33,400,000/33,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 31,729,132 (95.00%)\n",
      "Processing chunk 335 with 100000 rows...\n",
      "Chunk 335 processed in 0.02 seconds\n",
      "Removed 95633 duplicates from this chunk\n",
      "Progress: 33,500,000/33,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 31,824,765 (95.00%)\n",
      "Processing chunk 336 with 100000 rows...\n",
      "Chunk 336 processed in 0.02 seconds\n",
      "Removed 95937 duplicates from this chunk\n",
      "Progress: 33,600,000/33,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 31,920,702 (95.00%)\n",
      "Processing chunk 337 with 100000 rows...\n",
      "Chunk 337 processed in 0.03 seconds\n",
      "Removed 93998 duplicates from this chunk\n",
      "Progress: 33,700,000/33,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 32,014,700 (95.00%)\n",
      "Processing chunk 338 with 100000 rows...\n",
      "Chunk 338 processed in 0.03 seconds\n",
      "Removed 95171 duplicates from this chunk\n",
      "Progress: 33,800,000/33,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 32,109,871 (95.00%)\n",
      "Processing chunk 339 with 100000 rows...\n",
      "Chunk 339 processed in 0.03 seconds\n",
      "Removed 95018 duplicates from this chunk\n",
      "Progress: 33,900,000/33,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 32,204,889 (95.00%)\n",
      "Processing chunk 340 with 100000 rows...\n",
      "Chunk 340 processed in 0.01 seconds\n",
      "Removed 97353 duplicates from this chunk\n",
      "Progress: 34,000,000/34,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 32,302,242 (95.01%)\n",
      "Processing chunk 341 with 100000 rows...\n",
      "Chunk 341 processed in 0.02 seconds\n",
      "Removed 95914 duplicates from this chunk\n",
      "Progress: 34,100,000/34,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 32,398,156 (95.01%)\n",
      "Processing chunk 342 with 100000 rows...\n",
      "Chunk 342 processed in 0.03 seconds\n",
      "Removed 94005 duplicates from this chunk\n",
      "Progress: 34,200,000/34,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 32,492,161 (95.01%)\n",
      "Processing chunk 343 with 100000 rows...\n",
      "Chunk 343 processed in 0.02 seconds\n",
      "Removed 96421 duplicates from this chunk\n",
      "Progress: 34,300,000/34,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 32,588,582 (95.01%)\n",
      "Processing chunk 344 with 100000 rows...\n",
      "Chunk 344 processed in 0.02 seconds\n",
      "Removed 95991 duplicates from this chunk\n",
      "Progress: 34,400,000/34,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 32,684,573 (95.01%)\n",
      "Processing chunk 345 with 100000 rows...\n",
      "Chunk 345 processed in 0.03 seconds\n",
      "Removed 93690 duplicates from this chunk\n",
      "Progress: 34,500,000/34,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 32,778,263 (95.01%)\n",
      "Processing chunk 346 with 100000 rows...\n",
      "Chunk 346 processed in 0.01 seconds\n",
      "Removed 97332 duplicates from this chunk\n",
      "Progress: 34,600,000/34,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 32,875,595 (95.02%)\n",
      "Processing chunk 347 with 100000 rows...\n",
      "Chunk 347 processed in 0.03 seconds\n",
      "Removed 95058 duplicates from this chunk\n",
      "Progress: 34,700,000/34,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 32,970,653 (95.02%)\n",
      "Processing chunk 348 with 100000 rows...\n",
      "Chunk 348 processed in 0.03 seconds\n",
      "Removed 93956 duplicates from this chunk\n",
      "Progress: 34,800,000/34,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 33,064,609 (95.01%)\n",
      "Processing chunk 349 with 100000 rows...\n",
      "Chunk 349 processed in 0.02 seconds\n",
      "Removed 95899 duplicates from this chunk\n",
      "Progress: 34,900,000/34,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 33,160,508 (95.02%)\n",
      "Processing chunk 350 with 100000 rows...\n",
      "Chunk 350 processed in 0.03 seconds\n",
      "Removed 95140 duplicates from this chunk\n",
      "Progress: 35,000,000/35,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 33,255,648 (95.02%)\n",
      "Processing chunk 351 with 100000 rows...\n",
      "Chunk 351 processed in 0.02 seconds\n",
      "Removed 95739 duplicates from this chunk\n",
      "Progress: 35,100,000/35,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 33,351,387 (95.02%)\n",
      "Processing chunk 352 with 100000 rows...\n",
      "Chunk 352 processed in 0.03 seconds\n",
      "Removed 93427 duplicates from this chunk\n",
      "Progress: 35,200,000/35,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 33,444,814 (95.01%)\n",
      "Processing chunk 353 with 100000 rows...\n",
      "Chunk 353 processed in 0.02 seconds\n",
      "Removed 96938 duplicates from this chunk\n",
      "Progress: 35,300,000/35,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 33,541,752 (95.02%)\n",
      "Processing chunk 354 with 100000 rows...\n",
      "Chunk 354 processed in 0.03 seconds\n",
      "Removed 95043 duplicates from this chunk\n",
      "Progress: 35,400,000/35,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 33,636,795 (95.02%)\n",
      "Processing chunk 355 with 100000 rows...\n",
      "Chunk 355 processed in 0.03 seconds\n",
      "Removed 94549 duplicates from this chunk\n",
      "Progress: 35,500,000/35,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 33,731,344 (95.02%)\n",
      "Processing chunk 356 with 100000 rows...\n",
      "Chunk 356 processed in 0.01 seconds\n",
      "Removed 98034 duplicates from this chunk\n",
      "Progress: 35,600,000/35,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 33,829,378 (95.03%)\n",
      "Processing chunk 357 with 100000 rows...\n",
      "Chunk 357 processed in 0.03 seconds\n",
      "Removed 94043 duplicates from this chunk\n",
      "Progress: 35,700,000/35,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 33,923,421 (95.02%)\n",
      "Processing chunk 358 with 100000 rows...\n",
      "Chunk 358 processed in 0.03 seconds\n",
      "Removed 94693 duplicates from this chunk\n",
      "Progress: 35,800,000/35,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 34,018,114 (95.02%)\n",
      "Processing chunk 359 with 100000 rows...\n",
      "Chunk 359 processed in 0.02 seconds\n",
      "Removed 95469 duplicates from this chunk\n",
      "Progress: 35,900,000/35,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 34,113,583 (95.02%)\n",
      "Processing chunk 360 with 100000 rows...\n",
      "Chunk 360 processed in 0.02 seconds\n",
      "Removed 96706 duplicates from this chunk\n",
      "Progress: 36,000,000/36,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 34,210,289 (95.03%)\n",
      "Processing chunk 361 with 100000 rows...\n",
      "Chunk 361 processed in 0.03 seconds\n",
      "Removed 94341 duplicates from this chunk\n",
      "Progress: 36,100,000/36,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 34,304,630 (95.03%)\n",
      "Processing chunk 362 with 100000 rows...\n",
      "Chunk 362 processed in 0.03 seconds\n",
      "Removed 94207 duplicates from this chunk\n",
      "Progress: 36,200,000/36,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 34,398,837 (95.02%)\n",
      "Processing chunk 363 with 100000 rows...\n",
      "Chunk 363 processed in 0.01 seconds\n",
      "Removed 97164 duplicates from this chunk\n",
      "Progress: 36,300,000/36,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 34,496,001 (95.03%)\n",
      "Processing chunk 364 with 100000 rows...\n",
      "Chunk 364 processed in 0.03 seconds\n",
      "Removed 94837 duplicates from this chunk\n",
      "Progress: 36,400,000/36,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 34,590,838 (95.03%)\n",
      "Processing chunk 365 with 100000 rows...\n",
      "Chunk 365 processed in 0.03 seconds\n",
      "Removed 93726 duplicates from this chunk\n",
      "Progress: 36,500,000/36,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 34,684,564 (95.03%)\n",
      "Processing chunk 366 with 100000 rows...\n",
      "Chunk 366 processed in 0.03 seconds\n",
      "Removed 94909 duplicates from this chunk\n",
      "Progress: 36,600,000/36,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 34,779,473 (95.03%)\n",
      "Processing chunk 367 with 100000 rows...\n",
      "Chunk 367 processed in 0.02 seconds\n",
      "Removed 95586 duplicates from this chunk\n",
      "Progress: 36,700,000/36,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 34,875,059 (95.03%)\n",
      "Processing chunk 368 with 100000 rows...\n",
      "Chunk 368 processed in 0.01 seconds\n",
      "Removed 98634 duplicates from this chunk\n",
      "Progress: 36,800,000/36,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 34,973,693 (95.04%)\n",
      "Processing chunk 369 with 100000 rows...\n",
      "Chunk 369 processed in 0.03 seconds\n",
      "Removed 93277 duplicates from this chunk\n",
      "Progress: 36,900,000/36,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 35,066,970 (95.03%)\n",
      "Processing chunk 370 with 100000 rows...\n",
      "Chunk 370 processed in 0.02 seconds\n",
      "Removed 95848 duplicates from this chunk\n",
      "Progress: 37,000,000/37,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 35,162,818 (95.03%)\n",
      "Processing chunk 371 with 100000 rows...\n",
      "Chunk 371 processed in 0.02 seconds\n",
      "Removed 96039 duplicates from this chunk\n",
      "Progress: 37,100,000/37,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 35,258,857 (95.04%)\n",
      "Processing chunk 372 with 100000 rows...\n",
      "Chunk 372 processed in 0.03 seconds\n",
      "Removed 93556 duplicates from this chunk\n",
      "Progress: 37,200,000/37,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 35,352,413 (95.03%)\n",
      "Processing chunk 373 with 100000 rows...\n",
      "Chunk 373 processed in 0.03 seconds\n",
      "Removed 95369 duplicates from this chunk\n",
      "Progress: 37,300,000/37,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 35,447,782 (95.03%)\n",
      "Processing chunk 374 with 100000 rows...\n",
      "Chunk 374 processed in 0.03 seconds\n",
      "Removed 94494 duplicates from this chunk\n",
      "Progress: 37,400,000/37,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 35,542,276 (95.03%)\n",
      "Processing chunk 375 with 100000 rows...\n",
      "Chunk 375 processed in 0.01 seconds\n",
      "Removed 97350 duplicates from this chunk\n",
      "Progress: 37,500,000/37,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 35,639,626 (95.04%)\n",
      "Processing chunk 376 with 100000 rows...\n",
      "Chunk 376 processed in 0.03 seconds\n",
      "Removed 94988 duplicates from this chunk\n",
      "Progress: 37,600,000/37,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 35,734,614 (95.04%)\n",
      "Processing chunk 377 with 100000 rows...\n",
      "Chunk 377 processed in 0.02 seconds\n",
      "Removed 96302 duplicates from this chunk\n",
      "Progress: 37,700,000/37,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 35,830,916 (95.04%)\n",
      "Processing chunk 378 with 100000 rows...\n",
      "Chunk 378 processed in 0.02 seconds\n",
      "Removed 97038 duplicates from this chunk\n",
      "Progress: 37,800,000/37,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 35,927,954 (95.05%)\n",
      "Processing chunk 379 with 100000 rows...\n",
      "Chunk 379 processed in 0.03 seconds\n",
      "Removed 93956 duplicates from this chunk\n",
      "Progress: 37,900,000/37,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 36,021,910 (95.04%)\n",
      "Processing chunk 380 with 100000 rows...\n",
      "Chunk 380 processed in 0.02 seconds\n",
      "Removed 95360 duplicates from this chunk\n",
      "Progress: 38,000,000/38,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 36,117,270 (95.05%)\n",
      "Processing chunk 381 with 100000 rows...\n",
      "Chunk 381 processed in 0.02 seconds\n",
      "Removed 95309 duplicates from this chunk\n",
      "Progress: 38,100,000/38,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 36,212,579 (95.05%)\n",
      "Processing chunk 382 with 100000 rows...\n",
      "Chunk 382 processed in 0.03 seconds\n",
      "Removed 94271 duplicates from this chunk\n",
      "Progress: 38,200,000/38,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 36,306,850 (95.04%)\n",
      "Processing chunk 383 with 100000 rows...\n",
      "Chunk 383 processed in 0.01 seconds\n",
      "Removed 97575 duplicates from this chunk\n",
      "Progress: 38,300,000/38,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 36,404,425 (95.05%)\n",
      "Processing chunk 384 with 100000 rows...\n",
      "Chunk 384 processed in 0.02 seconds\n",
      "Removed 95400 duplicates from this chunk\n",
      "Progress: 38,400,000/38,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 36,499,825 (95.05%)\n",
      "Processing chunk 385 with 100000 rows...\n",
      "Chunk 385 processed in 0.03 seconds\n",
      "Removed 94416 duplicates from this chunk\n",
      "Progress: 38,500,000/38,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 36,594,241 (95.05%)\n",
      "Processing chunk 386 with 100000 rows...\n",
      "Chunk 386 processed in 0.01 seconds\n",
      "Removed 97410 duplicates from this chunk\n",
      "Progress: 38,600,000/38,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 36,691,651 (95.06%)\n",
      "Processing chunk 387 with 100000 rows...\n",
      "Chunk 387 processed in 0.03 seconds\n",
      "Removed 94925 duplicates from this chunk\n",
      "Progress: 38,700,000/38,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 36,786,576 (95.06%)\n",
      "Processing chunk 388 with 100000 rows...\n",
      "Chunk 388 processed in 0.03 seconds\n",
      "Removed 94635 duplicates from this chunk\n",
      "Progress: 38,800,000/38,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 36,881,211 (95.05%)\n",
      "Processing chunk 389 with 100000 rows...\n",
      "Chunk 389 processed in 0.02 seconds\n",
      "Removed 96677 duplicates from this chunk\n",
      "Progress: 38,900,000/38,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 36,977,888 (95.06%)\n",
      "Processing chunk 390 with 100000 rows...\n",
      "Chunk 390 processed in 0.02 seconds\n",
      "Removed 95277 duplicates from this chunk\n",
      "Progress: 39,000,000/39,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 37,073,165 (95.06%)\n",
      "Processing chunk 391 with 100000 rows...\n",
      "Chunk 391 processed in 0.02 seconds\n",
      "Removed 95172 duplicates from this chunk\n",
      "Progress: 39,100,000/39,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 37,168,337 (95.06%)\n",
      "Processing chunk 392 with 100000 rows...\n",
      "Chunk 392 processed in 0.03 seconds\n",
      "Removed 94563 duplicates from this chunk\n",
      "Progress: 39,200,000/39,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 37,262,900 (95.06%)\n",
      "Processing chunk 393 with 100000 rows...\n",
      "Chunk 393 processed in 0.02 seconds\n",
      "Removed 95018 duplicates from this chunk\n",
      "Progress: 39,300,000/39,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 37,357,918 (95.06%)\n",
      "Processing chunk 394 with 100000 rows...\n",
      "Chunk 394 processed in 0.03 seconds\n",
      "Removed 94292 duplicates from this chunk\n",
      "Progress: 39,400,000/39,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 37,452,210 (95.06%)\n",
      "Processing chunk 395 with 100000 rows...\n",
      "Chunk 395 processed in 0.02 seconds\n",
      "Removed 96724 duplicates from this chunk\n",
      "Progress: 39,500,000/39,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 37,548,934 (95.06%)\n",
      "Processing chunk 396 with 100000 rows...\n",
      "Chunk 396 processed in 0.03 seconds\n",
      "Removed 94062 duplicates from this chunk\n",
      "Progress: 39,600,000/39,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 37,642,996 (95.06%)\n",
      "Processing chunk 397 with 100000 rows...\n",
      "Chunk 397 processed in 0.02 seconds\n",
      "Removed 95145 duplicates from this chunk\n",
      "Progress: 39,700,000/39,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 37,738,141 (95.06%)\n",
      "Processing chunk 398 with 100000 rows...\n",
      "Chunk 398 processed in 0.03 seconds\n",
      "Removed 95252 duplicates from this chunk\n",
      "Progress: 39,800,000/39,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 37,833,393 (95.06%)\n",
      "Processing chunk 399 with 100000 rows...\n",
      "Chunk 399 processed in 0.03 seconds\n",
      "Removed 94736 duplicates from this chunk\n",
      "Progress: 39,900,000/39,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 37,928,129 (95.06%)\n",
      "Processing chunk 400 with 100000 rows...\n",
      "Chunk 400 processed in 0.02 seconds\n",
      "Removed 96172 duplicates from this chunk\n",
      "Progress: 40,000,000/40,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 38,024,301 (95.06%)\n",
      "Processing chunk 401 with 100000 rows...\n",
      "Chunk 401 processed in 0.02 seconds\n",
      "Removed 95743 duplicates from this chunk\n",
      "Progress: 40,100,000/40,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 38,120,044 (95.06%)\n",
      "Processing chunk 402 with 100000 rows...\n",
      "Chunk 402 processed in 0.03 seconds\n",
      "Removed 94896 duplicates from this chunk\n",
      "Progress: 40,200,000/40,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 38,214,940 (95.06%)\n",
      "Processing chunk 403 with 100000 rows...\n",
      "Chunk 403 processed in 0.03 seconds\n",
      "Removed 94593 duplicates from this chunk\n",
      "Progress: 40,300,000/40,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 38,309,533 (95.06%)\n",
      "Processing chunk 404 with 100000 rows...\n",
      "Chunk 404 processed in 0.03 seconds\n",
      "Removed 94126 duplicates from this chunk\n",
      "Progress: 40,400,000/40,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 38,403,659 (95.06%)\n",
      "Processing chunk 405 with 100000 rows...\n",
      "Chunk 405 processed in 0.02 seconds\n",
      "Removed 95648 duplicates from this chunk\n",
      "Progress: 40,500,000/40,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 38,499,307 (95.06%)\n",
      "Processing chunk 406 with 100000 rows...\n",
      "Chunk 406 processed in 0.02 seconds\n",
      "Removed 95983 duplicates from this chunk\n",
      "Progress: 40,600,000/40,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 38,595,290 (95.06%)\n",
      "Processing chunk 407 with 100000 rows...\n",
      "Chunk 407 processed in 0.02 seconds\n",
      "Removed 95630 duplicates from this chunk\n",
      "Progress: 40,700,000/40,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 38,690,920 (95.06%)\n",
      "Processing chunk 408 with 100000 rows...\n",
      "Chunk 408 processed in 0.03 seconds\n",
      "Removed 94002 duplicates from this chunk\n",
      "Progress: 40,800,000/40,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 38,784,922 (95.06%)\n",
      "Processing chunk 409 with 100000 rows...\n",
      "Chunk 409 processed in 0.02 seconds\n",
      "Removed 95784 duplicates from this chunk\n",
      "Progress: 40,900,000/40,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 38,880,706 (95.06%)\n",
      "Processing chunk 410 with 100000 rows...\n",
      "Chunk 410 processed in 0.03 seconds\n",
      "Removed 93292 duplicates from this chunk\n",
      "Progress: 41,000,000/41,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 38,973,998 (95.06%)\n",
      "Processing chunk 411 with 100000 rows...\n",
      "Chunk 411 processed in 0.02 seconds\n",
      "Removed 95474 duplicates from this chunk\n",
      "Progress: 41,100,000/41,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 39,069,472 (95.06%)\n",
      "Processing chunk 412 with 100000 rows...\n",
      "Chunk 412 processed in 0.03 seconds\n",
      "Removed 94762 duplicates from this chunk\n",
      "Progress: 41,200,000/41,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 39,164,234 (95.06%)\n",
      "Processing chunk 413 with 100000 rows...\n",
      "Chunk 413 processed in 0.03 seconds\n",
      "Removed 95037 duplicates from this chunk\n",
      "Progress: 41,300,000/41,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 39,259,271 (95.06%)\n",
      "Processing chunk 414 with 100000 rows...\n",
      "Chunk 414 processed in 0.03 seconds\n",
      "Removed 93533 duplicates from this chunk\n",
      "Progress: 41,400,000/41,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 39,352,804 (95.06%)\n",
      "Processing chunk 415 with 100000 rows...\n",
      "Chunk 415 processed in 0.01 seconds\n",
      "Removed 97569 duplicates from this chunk\n",
      "Progress: 41,500,000/41,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 39,450,373 (95.06%)\n",
      "Processing chunk 416 with 100000 rows...\n",
      "Chunk 416 processed in 0.03 seconds\n",
      "Removed 93907 duplicates from this chunk\n",
      "Progress: 41,600,000/41,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 39,544,280 (95.06%)\n",
      "Processing chunk 417 with 100000 rows...\n",
      "Chunk 417 processed in 0.03 seconds\n",
      "Removed 93463 duplicates from this chunk\n",
      "Progress: 41,700,000/41,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 39,637,743 (95.05%)\n",
      "Processing chunk 418 with 100000 rows...\n",
      "Chunk 418 processed in 0.03 seconds\n",
      "Removed 94052 duplicates from this chunk\n",
      "Progress: 41,800,000/41,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 39,731,795 (95.05%)\n",
      "Processing chunk 419 with 100000 rows...\n",
      "Chunk 419 processed in 0.03 seconds\n",
      "Removed 94158 duplicates from this chunk\n",
      "Progress: 41,900,000/41,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 39,825,953 (95.05%)\n",
      "Processing chunk 420 with 100000 rows...\n",
      "Chunk 420 processed in 0.10 seconds\n",
      "Removed 79798 duplicates from this chunk\n",
      "Progress: 42,000,000/42,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 39,905,751 (95.01%)\n",
      "Processing chunk 421 with 100000 rows...\n",
      "Chunk 421 processed in 0.09 seconds\n",
      "Removed 79813 duplicates from this chunk\n",
      "Progress: 42,100,000/42,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 39,985,564 (94.98%)\n",
      "Processing chunk 422 with 100000 rows...\n",
      "Chunk 422 processed in 0.09 seconds\n",
      "Removed 79962 duplicates from this chunk\n",
      "Progress: 42,200,000/42,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 40,065,526 (94.94%)\n",
      "Processing chunk 423 with 100000 rows...\n",
      "Chunk 423 processed in 0.09 seconds\n",
      "Removed 79911 duplicates from this chunk\n",
      "Progress: 42,300,000/42,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 40,145,437 (94.91%)\n",
      "Processing chunk 424 with 100000 rows...\n",
      "Chunk 424 processed in 0.09 seconds\n",
      "Removed 80395 duplicates from this chunk\n",
      "Progress: 42,400,000/42,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 40,225,832 (94.87%)\n",
      "Processing chunk 425 with 100000 rows...\n",
      "Chunk 425 processed in 0.09 seconds\n",
      "Removed 80479 duplicates from this chunk\n",
      "Progress: 42,500,000/42,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 40,306,311 (94.84%)\n",
      "Processing chunk 426 with 100000 rows...\n",
      "Chunk 426 processed in 0.10 seconds\n",
      "Removed 80341 duplicates from this chunk\n",
      "Progress: 42,600,000/42,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 40,386,652 (94.80%)\n",
      "Processing chunk 427 with 100000 rows...\n",
      "Chunk 427 processed in 0.09 seconds\n",
      "Removed 80539 duplicates from this chunk\n",
      "Progress: 42,700,000/42,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 40,467,191 (94.77%)\n",
      "Processing chunk 428 with 100000 rows...\n",
      "Chunk 428 processed in 0.09 seconds\n",
      "Removed 80671 duplicates from this chunk\n",
      "Progress: 42,800,000/42,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 40,547,862 (94.74%)\n",
      "Processing chunk 429 with 100000 rows...\n",
      "Chunk 429 processed in 0.09 seconds\n",
      "Removed 80495 duplicates from this chunk\n",
      "Progress: 42,900,000/42,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 40,628,357 (94.70%)\n",
      "Processing chunk 430 with 100000 rows...\n",
      "Chunk 430 processed in 0.09 seconds\n",
      "Removed 80834 duplicates from this chunk\n",
      "Progress: 43,000,000/43,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 40,709,191 (94.67%)\n",
      "Processing chunk 431 with 100000 rows...\n",
      "Chunk 431 processed in 0.09 seconds\n",
      "Removed 80791 duplicates from this chunk\n",
      "Progress: 43,100,000/43,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 40,789,982 (94.64%)\n",
      "Processing chunk 432 with 100000 rows...\n",
      "Chunk 432 processed in 0.09 seconds\n",
      "Removed 81590 duplicates from this chunk\n",
      "Progress: 43,200,000/43,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 40,871,572 (94.61%)\n",
      "Processing chunk 433 with 100000 rows...\n",
      "Chunk 433 processed in 0.08 seconds\n",
      "Removed 82219 duplicates from this chunk\n",
      "Progress: 43,300,000/43,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 40,953,791 (94.58%)\n",
      "Processing chunk 434 with 100000 rows...\n",
      "Chunk 434 processed in 0.09 seconds\n",
      "Removed 82383 duplicates from this chunk\n",
      "Progress: 43,400,000/43,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 41,036,174 (94.55%)\n",
      "Processing chunk 435 with 100000 rows...\n",
      "Chunk 435 processed in 0.08 seconds\n",
      "Removed 82481 duplicates from this chunk\n",
      "Progress: 43,500,000/43,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 41,118,655 (94.53%)\n",
      "Processing chunk 436 with 100000 rows...\n",
      "Chunk 436 processed in 0.08 seconds\n",
      "Removed 82820 duplicates from this chunk\n",
      "Progress: 43,600,000/43,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 41,201,475 (94.50%)\n",
      "Processing chunk 437 with 100000 rows...\n",
      "Chunk 437 processed in 0.08 seconds\n",
      "Removed 82984 duplicates from this chunk\n",
      "Progress: 43,700,000/43,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 41,284,459 (94.47%)\n",
      "Processing chunk 438 with 100000 rows...\n",
      "Chunk 438 processed in 0.08 seconds\n",
      "Removed 83320 duplicates from this chunk\n",
      "Progress: 43,800,000/43,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 41,367,779 (94.45%)\n",
      "Processing chunk 439 with 100000 rows...\n",
      "Chunk 439 processed in 0.08 seconds\n",
      "Removed 83359 duplicates from this chunk\n",
      "Progress: 43,900,000/43,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 41,451,138 (94.42%)\n",
      "Processing chunk 440 with 100000 rows...\n",
      "Chunk 440 processed in 0.08 seconds\n",
      "Removed 83349 duplicates from this chunk\n",
      "Progress: 44,000,000/44,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 41,534,487 (94.40%)\n",
      "Processing chunk 441 with 100000 rows...\n",
      "Chunk 441 processed in 0.08 seconds\n",
      "Removed 83380 duplicates from this chunk\n",
      "Progress: 44,100,000/44,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 41,617,867 (94.37%)\n",
      "Processing chunk 442 with 100000 rows...\n",
      "Chunk 442 processed in 0.08 seconds\n",
      "Removed 83710 duplicates from this chunk\n",
      "Progress: 44,200,000/44,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 41,701,577 (94.35%)\n",
      "Processing chunk 443 with 100000 rows...\n",
      "Chunk 443 processed in 0.08 seconds\n",
      "Removed 83624 duplicates from this chunk\n",
      "Progress: 44,300,000/44,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 41,785,201 (94.32%)\n",
      "Processing chunk 444 with 100000 rows...\n",
      "Chunk 444 processed in 0.12 seconds\n",
      "Removed 83823 duplicates from this chunk\n",
      "Progress: 44,400,000/44,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 41,869,024 (94.30%)\n",
      "Processing chunk 445 with 100000 rows...\n",
      "Chunk 445 processed in 0.08 seconds\n",
      "Removed 83722 duplicates from this chunk\n",
      "Progress: 44,500,000/44,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 41,952,746 (94.28%)\n",
      "Processing chunk 446 with 100000 rows...\n",
      "Chunk 446 processed in 0.08 seconds\n",
      "Removed 83851 duplicates from this chunk\n",
      "Progress: 44,600,000/44,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 42,036,597 (94.25%)\n",
      "Processing chunk 447 with 100000 rows...\n",
      "Chunk 447 processed in 0.08 seconds\n",
      "Removed 84066 duplicates from this chunk\n",
      "Progress: 44,700,000/44,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 42,120,663 (94.23%)\n",
      "Processing chunk 448 with 100000 rows...\n",
      "Chunk 448 processed in 0.08 seconds\n",
      "Removed 84013 duplicates from this chunk\n",
      "Progress: 44,800,000/44,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 42,204,676 (94.21%)\n",
      "Processing chunk 449 with 100000 rows...\n",
      "Chunk 449 processed in 0.08 seconds\n",
      "Removed 84214 duplicates from this chunk\n",
      "Progress: 44,900,000/44,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 42,288,890 (94.18%)\n",
      "Processing chunk 450 with 100000 rows...\n",
      "Chunk 450 processed in 0.08 seconds\n",
      "Removed 84349 duplicates from this chunk\n",
      "Progress: 45,000,000/45,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 42,373,239 (94.16%)\n",
      "Processing chunk 451 with 100000 rows...\n",
      "Chunk 451 processed in 0.08 seconds\n",
      "Removed 84521 duplicates from this chunk\n",
      "Progress: 45,100,000/45,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 42,457,760 (94.14%)\n",
      "Processing chunk 452 with 100000 rows...\n",
      "Chunk 452 processed in 0.08 seconds\n",
      "Removed 84349 duplicates from this chunk\n",
      "Progress: 45,200,000/45,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 42,542,109 (94.12%)\n",
      "Processing chunk 453 with 100000 rows...\n",
      "Chunk 453 processed in 0.07 seconds\n",
      "Removed 84872 duplicates from this chunk\n",
      "Progress: 45,300,000/45,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 42,626,981 (94.10%)\n",
      "Processing chunk 454 with 100000 rows...\n",
      "Chunk 454 processed in 0.07 seconds\n",
      "Removed 84769 duplicates from this chunk\n",
      "Progress: 45,400,000/45,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 42,711,750 (94.08%)\n",
      "Processing chunk 455 with 100000 rows...\n",
      "Chunk 455 processed in 0.07 seconds\n",
      "Removed 85319 duplicates from this chunk\n",
      "Progress: 45,500,000/45,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 42,797,069 (94.06%)\n",
      "Processing chunk 456 with 100000 rows...\n",
      "Chunk 456 processed in 0.07 seconds\n",
      "Removed 84870 duplicates from this chunk\n",
      "Progress: 45,600,000/45,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 42,881,939 (94.04%)\n",
      "Processing chunk 457 with 100000 rows...\n",
      "Chunk 457 processed in 0.07 seconds\n",
      "Removed 84993 duplicates from this chunk\n",
      "Progress: 45,700,000/45,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 42,966,932 (94.02%)\n",
      "Processing chunk 458 with 100000 rows...\n",
      "Chunk 458 processed in 0.07 seconds\n",
      "Removed 85167 duplicates from this chunk\n",
      "Progress: 45,800,000/45,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 43,052,099 (94.00%)\n",
      "Processing chunk 459 with 100000 rows...\n",
      "Chunk 459 processed in 0.07 seconds\n",
      "Removed 84902 duplicates from this chunk\n",
      "Progress: 45,900,000/45,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 43,137,001 (93.98%)\n",
      "Processing chunk 460 with 100000 rows...\n",
      "Chunk 460 processed in 0.07 seconds\n",
      "Removed 85356 duplicates from this chunk\n",
      "Progress: 46,000,000/46,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 43,222,357 (93.96%)\n",
      "Processing chunk 461 with 100000 rows...\n",
      "Chunk 461 processed in 0.07 seconds\n",
      "Removed 85392 duplicates from this chunk\n",
      "Progress: 46,100,000/46,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 43,307,749 (93.94%)\n",
      "Processing chunk 462 with 100000 rows...\n",
      "Chunk 462 processed in 0.07 seconds\n",
      "Removed 85407 duplicates from this chunk\n",
      "Progress: 46,200,000/46,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 43,393,156 (93.92%)\n",
      "Processing chunk 463 with 100000 rows...\n",
      "Chunk 463 processed in 0.07 seconds\n",
      "Removed 85798 duplicates from this chunk\n",
      "Progress: 46,300,000/46,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 43,478,954 (93.91%)\n",
      "Processing chunk 464 with 100000 rows...\n",
      "Chunk 464 processed in 0.07 seconds\n",
      "Removed 85740 duplicates from this chunk\n",
      "Progress: 46,400,000/46,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 43,564,694 (93.89%)\n",
      "Processing chunk 465 with 100000 rows...\n",
      "Chunk 465 processed in 0.07 seconds\n",
      "Removed 85631 duplicates from this chunk\n",
      "Progress: 46,500,000/46,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 43,650,325 (93.87%)\n",
      "Processing chunk 466 with 100000 rows...\n",
      "Chunk 466 processed in 0.07 seconds\n",
      "Removed 85928 duplicates from this chunk\n",
      "Progress: 46,600,000/46,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 43,736,253 (93.85%)\n",
      "Processing chunk 467 with 100000 rows...\n",
      "Chunk 467 processed in 0.07 seconds\n",
      "Removed 85804 duplicates from this chunk\n",
      "Progress: 46,700,000/46,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 43,822,057 (93.84%)\n",
      "Processing chunk 468 with 100000 rows...\n",
      "Chunk 468 processed in 0.07 seconds\n",
      "Removed 85995 duplicates from this chunk\n",
      "Progress: 46,800,000/46,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 43,908,052 (93.82%)\n",
      "Processing chunk 469 with 100000 rows...\n",
      "Chunk 469 processed in 0.07 seconds\n",
      "Removed 86124 duplicates from this chunk\n",
      "Progress: 46,900,000/46,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 43,994,176 (93.80%)\n",
      "Processing chunk 470 with 100000 rows...\n",
      "Chunk 470 processed in 0.07 seconds\n",
      "Removed 86206 duplicates from this chunk\n",
      "Progress: 47,000,000/47,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 44,080,382 (93.79%)\n",
      "Processing chunk 471 with 100000 rows...\n",
      "Chunk 471 processed in 0.07 seconds\n",
      "Removed 86439 duplicates from this chunk\n",
      "Progress: 47,100,000/47,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 44,166,821 (93.77%)\n",
      "Processing chunk 472 with 100000 rows...\n",
      "Chunk 472 processed in 0.07 seconds\n",
      "Removed 86559 duplicates from this chunk\n",
      "Progress: 47,200,000/47,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 44,253,380 (93.76%)\n",
      "Processing chunk 473 with 100000 rows...\n",
      "Chunk 473 processed in 0.07 seconds\n",
      "Removed 86324 duplicates from this chunk\n",
      "Progress: 47,300,000/47,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 44,339,704 (93.74%)\n",
      "Processing chunk 474 with 100000 rows...\n",
      "Chunk 474 processed in 0.06 seconds\n",
      "Removed 87066 duplicates from this chunk\n",
      "Progress: 47,400,000/47,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 44,426,770 (93.73%)\n",
      "Processing chunk 475 with 100000 rows...\n",
      "Chunk 475 processed in 0.06 seconds\n",
      "Removed 86751 duplicates from this chunk\n",
      "Progress: 47,500,000/47,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 44,513,521 (93.71%)\n",
      "Processing chunk 476 with 100000 rows...\n",
      "Chunk 476 processed in 0.07 seconds\n",
      "Removed 86846 duplicates from this chunk\n",
      "Progress: 47,600,000/47,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 44,600,367 (93.70%)\n",
      "Processing chunk 477 with 100000 rows...\n",
      "Chunk 477 processed in 0.06 seconds\n",
      "Removed 86871 duplicates from this chunk\n",
      "Progress: 47,700,000/47,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 44,687,238 (93.68%)\n",
      "Processing chunk 478 with 100000 rows...\n",
      "Chunk 478 processed in 0.06 seconds\n",
      "Removed 86946 duplicates from this chunk\n",
      "Progress: 47,800,000/47,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 44,774,184 (93.67%)\n",
      "Processing chunk 479 with 100000 rows...\n",
      "Chunk 479 processed in 0.06 seconds\n",
      "Removed 86877 duplicates from this chunk\n",
      "Progress: 47,900,000/47,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 44,861,061 (93.66%)\n",
      "Processing chunk 480 with 100000 rows...\n",
      "Chunk 480 processed in 0.06 seconds\n",
      "Removed 86812 duplicates from this chunk\n",
      "Progress: 48,000,000/48,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 44,947,873 (93.64%)\n",
      "Processing chunk 481 with 100000 rows...\n",
      "Chunk 481 processed in 0.07 seconds\n",
      "Removed 86669 duplicates from this chunk\n",
      "Progress: 48,100,000/48,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 45,034,542 (93.63%)\n",
      "Processing chunk 482 with 100000 rows...\n",
      "Chunk 482 processed in 0.06 seconds\n",
      "Removed 86861 duplicates from this chunk\n",
      "Progress: 48,200,000/48,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 45,121,403 (93.61%)\n",
      "Processing chunk 483 with 100000 rows...\n",
      "Chunk 483 processed in 0.06 seconds\n",
      "Removed 86974 duplicates from this chunk\n",
      "Progress: 48,300,000/48,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 45,208,377 (93.60%)\n",
      "Processing chunk 484 with 100000 rows...\n",
      "Chunk 484 processed in 0.06 seconds\n",
      "Removed 86768 duplicates from this chunk\n",
      "Progress: 48,400,000/48,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 45,295,145 (93.59%)\n",
      "Processing chunk 485 with 100000 rows...\n",
      "Chunk 485 processed in 0.06 seconds\n",
      "Removed 86851 duplicates from this chunk\n",
      "Progress: 48,500,000/48,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 45,381,996 (93.57%)\n",
      "Processing chunk 486 with 100000 rows...\n",
      "Chunk 486 processed in 0.07 seconds\n",
      "Removed 86641 duplicates from this chunk\n",
      "Progress: 48,600,000/48,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 45,468,637 (93.56%)\n",
      "Processing chunk 487 with 100000 rows...\n",
      "Chunk 487 processed in 0.07 seconds\n",
      "Removed 86736 duplicates from this chunk\n",
      "Progress: 48,700,000/48,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 45,555,373 (93.54%)\n",
      "Processing chunk 488 with 100000 rows...\n",
      "Chunk 488 processed in 0.07 seconds\n",
      "Removed 87035 duplicates from this chunk\n",
      "Progress: 48,800,000/48,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 45,642,408 (93.53%)\n",
      "Processing chunk 489 with 100000 rows...\n",
      "Chunk 489 processed in 0.07 seconds\n",
      "Removed 87081 duplicates from this chunk\n",
      "Progress: 48,900,000/48,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 45,729,489 (93.52%)\n",
      "Processing chunk 490 with 100000 rows...\n",
      "Chunk 490 processed in 0.07 seconds\n",
      "Removed 87011 duplicates from this chunk\n",
      "Progress: 49,000,000/49,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 45,816,500 (93.50%)\n",
      "Processing chunk 491 with 100000 rows...\n",
      "Chunk 491 processed in 0.07 seconds\n",
      "Removed 87022 duplicates from this chunk\n",
      "Progress: 49,100,000/49,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 45,903,522 (93.49%)\n",
      "Processing chunk 492 with 100000 rows...\n",
      "Chunk 492 processed in 0.06 seconds\n",
      "Removed 87289 duplicates from this chunk\n",
      "Progress: 49,200,000/49,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 45,990,811 (93.48%)\n",
      "Processing chunk 493 with 100000 rows...\n",
      "Chunk 493 processed in 0.06 seconds\n",
      "Removed 87159 duplicates from this chunk\n",
      "Progress: 49,300,000/49,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 46,077,970 (93.46%)\n",
      "Processing chunk 494 with 100000 rows...\n",
      "Chunk 494 processed in 0.06 seconds\n",
      "Removed 87354 duplicates from this chunk\n",
      "Progress: 49,400,000/49,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 46,165,324 (93.45%)\n",
      "Processing chunk 495 with 100000 rows...\n",
      "Chunk 495 processed in 0.06 seconds\n",
      "Removed 87394 duplicates from this chunk\n",
      "Progress: 49,500,000/49,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 46,252,718 (93.44%)\n",
      "Processing chunk 496 with 100000 rows...\n",
      "Chunk 496 processed in 0.06 seconds\n",
      "Removed 87485 duplicates from this chunk\n",
      "Progress: 49,600,000/49,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 46,340,203 (93.43%)\n",
      "Processing chunk 497 with 100000 rows...\n",
      "Chunk 497 processed in 0.06 seconds\n",
      "Removed 87424 duplicates from this chunk\n",
      "Progress: 49,700,000/49,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 46,427,627 (93.42%)\n",
      "Processing chunk 498 with 100000 rows...\n",
      "Chunk 498 processed in 0.06 seconds\n",
      "Removed 87687 duplicates from this chunk\n",
      "Progress: 49,800,000/49,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 46,515,314 (93.40%)\n",
      "Processing chunk 499 with 100000 rows...\n",
      "Chunk 499 processed in 0.06 seconds\n",
      "Removed 87535 duplicates from this chunk\n",
      "Progress: 49,900,000/49,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 46,602,849 (93.39%)\n",
      "Processing chunk 500 with 100000 rows...\n",
      "Chunk 500 processed in 0.06 seconds\n",
      "Removed 88022 duplicates from this chunk\n",
      "Progress: 50,000,000/50,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 46,690,871 (93.38%)\n",
      "Processing chunk 501 with 100000 rows...\n",
      "Chunk 501 processed in 0.06 seconds\n",
      "Removed 87738 duplicates from this chunk\n",
      "Progress: 50,100,000/50,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 46,778,609 (93.37%)\n",
      "Processing chunk 502 with 100000 rows...\n",
      "Chunk 502 processed in 0.06 seconds\n",
      "Removed 87714 duplicates from this chunk\n",
      "Progress: 50,200,000/50,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 46,866,323 (93.36%)\n",
      "Processing chunk 503 with 100000 rows...\n",
      "Chunk 503 processed in 0.06 seconds\n",
      "Removed 87824 duplicates from this chunk\n",
      "Progress: 50,300,000/50,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 46,954,147 (93.35%)\n",
      "Processing chunk 504 with 100000 rows...\n",
      "Chunk 504 processed in 0.06 seconds\n",
      "Removed 87935 duplicates from this chunk\n",
      "Progress: 50,400,000/50,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 47,042,082 (93.34%)\n",
      "Processing chunk 505 with 100000 rows...\n",
      "Chunk 505 processed in 0.06 seconds\n",
      "Removed 88240 duplicates from this chunk\n",
      "Progress: 50,500,000/50,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 47,130,322 (93.33%)\n",
      "Processing chunk 506 with 100000 rows...\n",
      "Chunk 506 processed in 0.06 seconds\n",
      "Removed 88134 duplicates from this chunk\n",
      "Progress: 50,600,000/50,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 47,218,456 (93.32%)\n",
      "Processing chunk 507 with 100000 rows...\n",
      "Chunk 507 processed in 0.06 seconds\n",
      "Removed 87914 duplicates from this chunk\n",
      "Progress: 50,700,000/50,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 47,306,370 (93.31%)\n",
      "Processing chunk 508 with 100000 rows...\n",
      "Chunk 508 processed in 0.06 seconds\n",
      "Removed 88375 duplicates from this chunk\n",
      "Progress: 50,800,000/50,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 47,394,745 (93.30%)\n",
      "Processing chunk 509 with 100000 rows...\n",
      "Chunk 509 processed in 0.06 seconds\n",
      "Removed 88126 duplicates from this chunk\n",
      "Progress: 50,900,000/50,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 47,482,871 (93.29%)\n",
      "Processing chunk 510 with 100000 rows...\n",
      "Chunk 510 processed in 0.06 seconds\n",
      "Removed 88473 duplicates from this chunk\n",
      "Progress: 51,000,000/51,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 47,571,344 (93.28%)\n",
      "Processing chunk 511 with 100000 rows...\n",
      "Chunk 511 processed in 0.06 seconds\n",
      "Removed 88137 duplicates from this chunk\n",
      "Progress: 51,100,000/51,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 47,659,481 (93.27%)\n",
      "Processing chunk 512 with 100000 rows...\n",
      "Chunk 512 processed in 0.06 seconds\n",
      "Removed 88444 duplicates from this chunk\n",
      "Progress: 51,200,000/51,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 47,747,925 (93.26%)\n",
      "Processing chunk 513 with 100000 rows...\n",
      "Chunk 513 processed in 0.06 seconds\n",
      "Removed 88923 duplicates from this chunk\n",
      "Progress: 51,300,000/51,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 47,836,848 (93.25%)\n",
      "Processing chunk 514 with 100000 rows...\n",
      "Chunk 514 processed in 0.06 seconds\n",
      "Removed 88519 duplicates from this chunk\n",
      "Progress: 51,400,000/51,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 47,925,367 (93.24%)\n",
      "Processing chunk 515 with 100000 rows...\n",
      "Chunk 515 processed in 0.05 seconds\n",
      "Removed 89358 duplicates from this chunk\n",
      "Progress: 51,500,000/51,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 48,014,725 (93.23%)\n",
      "Processing chunk 516 with 100000 rows...\n",
      "Chunk 516 processed in 0.06 seconds\n",
      "Removed 89054 duplicates from this chunk\n",
      "Progress: 51,600,000/51,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 48,103,779 (93.22%)\n",
      "Processing chunk 517 with 100000 rows...\n",
      "Chunk 517 processed in 0.05 seconds\n",
      "Removed 89042 duplicates from this chunk\n",
      "Progress: 51,700,000/51,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 48,192,821 (93.22%)\n",
      "Processing chunk 518 with 100000 rows...\n",
      "Chunk 518 processed in 0.05 seconds\n",
      "Removed 89340 duplicates from this chunk\n",
      "Progress: 51,800,000/51,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 48,282,161 (93.21%)\n",
      "Processing chunk 519 with 100000 rows...\n",
      "Chunk 519 processed in 0.05 seconds\n",
      "Removed 89254 duplicates from this chunk\n",
      "Progress: 51,900,000/51,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 48,371,415 (93.20%)\n",
      "Processing chunk 520 with 100000 rows...\n",
      "Chunk 520 processed in 0.05 seconds\n",
      "Removed 89194 duplicates from this chunk\n",
      "Progress: 52,000,000/52,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 48,460,609 (93.19%)\n",
      "Processing chunk 521 with 100000 rows...\n",
      "Chunk 521 processed in 0.05 seconds\n",
      "Removed 89397 duplicates from this chunk\n",
      "Progress: 52,100,000/52,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 48,550,006 (93.19%)\n",
      "Processing chunk 522 with 100000 rows...\n",
      "Chunk 522 processed in 0.06 seconds\n",
      "Removed 89245 duplicates from this chunk\n",
      "Progress: 52,200,000/52,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 48,639,251 (93.18%)\n",
      "Processing chunk 523 with 100000 rows...\n",
      "Chunk 523 processed in 0.05 seconds\n",
      "Removed 89657 duplicates from this chunk\n",
      "Progress: 52,300,000/52,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 48,728,908 (93.17%)\n",
      "Processing chunk 524 with 100000 rows...\n",
      "Chunk 524 processed in 0.06 seconds\n",
      "Removed 89443 duplicates from this chunk\n",
      "Progress: 52,400,000/52,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 48,818,351 (93.16%)\n",
      "Processing chunk 525 with 100000 rows...\n",
      "Chunk 525 processed in 0.05 seconds\n",
      "Removed 89391 duplicates from this chunk\n",
      "Progress: 52,500,000/52,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 48,907,742 (93.16%)\n",
      "Processing chunk 526 with 100000 rows...\n",
      "Chunk 526 processed in 0.05 seconds\n",
      "Removed 89525 duplicates from this chunk\n",
      "Progress: 52,600,000/52,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 48,997,267 (93.15%)\n",
      "Processing chunk 527 with 100000 rows...\n",
      "Chunk 527 processed in 0.06 seconds\n",
      "Removed 89524 duplicates from this chunk\n",
      "Progress: 52,700,000/52,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 49,086,791 (93.14%)\n",
      "Processing chunk 528 with 100000 rows...\n",
      "Chunk 528 processed in 0.06 seconds\n",
      "Removed 89636 duplicates from this chunk\n",
      "Progress: 52,800,000/52,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 49,176,427 (93.14%)\n",
      "Processing chunk 529 with 100000 rows...\n",
      "Chunk 529 processed in 0.05 seconds\n",
      "Removed 89743 duplicates from this chunk\n",
      "Progress: 52,900,000/52,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 49,266,170 (93.13%)\n",
      "Processing chunk 530 with 100000 rows...\n",
      "Chunk 530 processed in 0.06 seconds\n",
      "Removed 89500 duplicates from this chunk\n",
      "Progress: 53,000,000/53,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 49,355,670 (93.12%)\n",
      "Processing chunk 531 with 100000 rows...\n",
      "Chunk 531 processed in 0.05 seconds\n",
      "Removed 89722 duplicates from this chunk\n",
      "Progress: 53,100,000/53,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 49,445,392 (93.12%)\n",
      "Processing chunk 532 with 100000 rows...\n",
      "Chunk 532 processed in 0.05 seconds\n",
      "Removed 89554 duplicates from this chunk\n",
      "Progress: 53,200,000/53,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 49,534,946 (93.11%)\n",
      "Processing chunk 533 with 100000 rows...\n",
      "Chunk 533 processed in 0.05 seconds\n",
      "Removed 89816 duplicates from this chunk\n",
      "Progress: 53,300,000/53,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 49,624,762 (93.10%)\n",
      "Processing chunk 534 with 100000 rows...\n",
      "Chunk 534 processed in 0.05 seconds\n",
      "Removed 90233 duplicates from this chunk\n",
      "Progress: 53,400,000/53,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 49,714,995 (93.10%)\n",
      "Processing chunk 535 with 100000 rows...\n",
      "Chunk 535 processed in 0.05 seconds\n",
      "Removed 90416 duplicates from this chunk\n",
      "Progress: 53,500,000/53,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 49,805,411 (93.09%)\n",
      "Processing chunk 536 with 100000 rows...\n",
      "Chunk 536 processed in 0.06 seconds\n",
      "Removed 89780 duplicates from this chunk\n",
      "Progress: 53,600,000/53,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 49,895,191 (93.09%)\n",
      "Processing chunk 537 with 100000 rows...\n",
      "Chunk 537 processed in 0.05 seconds\n",
      "Removed 89873 duplicates from this chunk\n",
      "Progress: 53,700,000/53,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 49,985,064 (93.08%)\n",
      "Processing chunk 538 with 100000 rows...\n",
      "Chunk 538 processed in 0.05 seconds\n",
      "Removed 90463 duplicates from this chunk\n",
      "Progress: 53,800,000/53,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 50,075,527 (93.08%)\n",
      "Processing chunk 539 with 100000 rows...\n",
      "Chunk 539 processed in 0.05 seconds\n",
      "Removed 90504 duplicates from this chunk\n",
      "Progress: 53,900,000/53,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 50,166,031 (93.07%)\n",
      "Processing chunk 540 with 100000 rows...\n",
      "Chunk 540 processed in 0.05 seconds\n",
      "Removed 90093 duplicates from this chunk\n",
      "Progress: 54,000,000/54,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 50,256,124 (93.07%)\n",
      "Processing chunk 541 with 100000 rows...\n",
      "Chunk 541 processed in 0.05 seconds\n",
      "Removed 90223 duplicates from this chunk\n",
      "Progress: 54,100,000/54,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 50,346,347 (93.06%)\n",
      "Processing chunk 542 with 100000 rows...\n",
      "Chunk 542 processed in 0.05 seconds\n",
      "Removed 90283 duplicates from this chunk\n",
      "Progress: 54,200,000/54,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 50,436,630 (93.06%)\n",
      "Processing chunk 543 with 100000 rows...\n",
      "Chunk 543 processed in 0.05 seconds\n",
      "Removed 90595 duplicates from this chunk\n",
      "Progress: 54,300,000/54,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 50,527,225 (93.05%)\n",
      "Processing chunk 544 with 100000 rows...\n",
      "Chunk 544 processed in 0.05 seconds\n",
      "Removed 90276 duplicates from this chunk\n",
      "Progress: 54,400,000/54,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 50,617,501 (93.05%)\n",
      "Processing chunk 545 with 100000 rows...\n",
      "Chunk 545 processed in 0.05 seconds\n",
      "Removed 90337 duplicates from this chunk\n",
      "Progress: 54,500,000/54,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 50,707,838 (93.04%)\n",
      "Processing chunk 546 with 100000 rows...\n",
      "Chunk 546 processed in 0.05 seconds\n",
      "Removed 90443 duplicates from this chunk\n",
      "Progress: 54,600,000/54,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 50,798,281 (93.04%)\n",
      "Processing chunk 547 with 100000 rows...\n",
      "Chunk 547 processed in 0.05 seconds\n",
      "Removed 90403 duplicates from this chunk\n",
      "Progress: 54,700,000/54,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 50,888,684 (93.03%)\n",
      "Processing chunk 548 with 100000 rows...\n",
      "Chunk 548 processed in 0.05 seconds\n",
      "Removed 90471 duplicates from this chunk\n",
      "Progress: 54,800,000/54,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 50,979,155 (93.03%)\n",
      "Processing chunk 549 with 100000 rows...\n",
      "Chunk 549 processed in 0.05 seconds\n",
      "Removed 90174 duplicates from this chunk\n",
      "Progress: 54,900,000/54,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 51,069,329 (93.02%)\n",
      "Processing chunk 550 with 100000 rows...\n",
      "Chunk 550 processed in 0.05 seconds\n",
      "Removed 90420 duplicates from this chunk\n",
      "Progress: 55,000,000/55,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 51,159,749 (93.02%)\n",
      "Processing chunk 551 with 100000 rows...\n",
      "Chunk 551 processed in 0.05 seconds\n",
      "Removed 90494 duplicates from this chunk\n",
      "Progress: 55,100,000/55,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 51,250,243 (93.01%)\n",
      "Processing chunk 552 with 100000 rows...\n",
      "Chunk 552 processed in 0.05 seconds\n",
      "Removed 90301 duplicates from this chunk\n",
      "Progress: 55,200,000/55,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 51,340,544 (93.01%)\n",
      "Processing chunk 553 with 100000 rows...\n",
      "Chunk 553 processed in 0.05 seconds\n",
      "Removed 90541 duplicates from this chunk\n",
      "Progress: 55,300,000/55,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 51,431,085 (93.00%)\n",
      "Processing chunk 554 with 100000 rows...\n",
      "Chunk 554 processed in 0.05 seconds\n",
      "Removed 90299 duplicates from this chunk\n",
      "Progress: 55,400,000/55,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 51,521,384 (93.00%)\n",
      "Processing chunk 555 with 100000 rows...\n",
      "Chunk 555 processed in 0.05 seconds\n",
      "Removed 90489 duplicates from this chunk\n",
      "Progress: 55,500,000/55,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 51,611,873 (92.99%)\n",
      "Processing chunk 556 with 100000 rows...\n",
      "Chunk 556 processed in 0.05 seconds\n",
      "Removed 90346 duplicates from this chunk\n",
      "Progress: 55,600,000/55,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 51,702,219 (92.99%)\n",
      "Processing chunk 557 with 100000 rows...\n",
      "Chunk 557 processed in 0.05 seconds\n",
      "Removed 90469 duplicates from this chunk\n",
      "Progress: 55,700,000/55,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 51,792,688 (92.99%)\n",
      "Processing chunk 558 with 100000 rows...\n",
      "Chunk 558 processed in 0.05 seconds\n",
      "Removed 90022 duplicates from this chunk\n",
      "Progress: 55,800,000/55,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 51,882,710 (92.98%)\n",
      "Processing chunk 559 with 100000 rows...\n",
      "Chunk 559 processed in 0.05 seconds\n",
      "Removed 90346 duplicates from this chunk\n",
      "Progress: 55,900,000/55,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 51,973,056 (92.98%)\n",
      "Processing chunk 560 with 100000 rows...\n",
      "Chunk 560 processed in 0.05 seconds\n",
      "Removed 90816 duplicates from this chunk\n",
      "Progress: 56,000,000/56,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 52,063,872 (92.97%)\n",
      "Processing chunk 561 with 100000 rows...\n",
      "Chunk 561 processed in 0.05 seconds\n",
      "Removed 91148 duplicates from this chunk\n",
      "Progress: 56,100,000/56,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 52,155,020 (92.97%)\n",
      "Processing chunk 562 with 100000 rows...\n",
      "Chunk 562 processed in 0.05 seconds\n",
      "Removed 90835 duplicates from this chunk\n",
      "Progress: 56,200,000/56,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 52,245,855 (92.96%)\n",
      "Processing chunk 563 with 100000 rows...\n",
      "Chunk 563 processed in 0.05 seconds\n",
      "Removed 90712 duplicates from this chunk\n",
      "Progress: 56,300,000/56,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 52,336,567 (92.96%)\n",
      "Processing chunk 564 with 100000 rows...\n",
      "Chunk 564 processed in 0.05 seconds\n",
      "Removed 90744 duplicates from this chunk\n",
      "Progress: 56,400,000/56,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 52,427,311 (92.96%)\n",
      "Processing chunk 565 with 100000 rows...\n",
      "Chunk 565 processed in 0.05 seconds\n",
      "Removed 90672 duplicates from this chunk\n",
      "Progress: 56,500,000/56,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 52,517,983 (92.95%)\n",
      "Processing chunk 566 with 100000 rows...\n",
      "Chunk 566 processed in 0.05 seconds\n",
      "Removed 90738 duplicates from this chunk\n",
      "Progress: 56,600,000/56,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 52,608,721 (92.95%)\n",
      "Processing chunk 567 with 100000 rows...\n",
      "Chunk 567 processed in 0.05 seconds\n",
      "Removed 90626 duplicates from this chunk\n",
      "Progress: 56,700,000/56,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 52,699,347 (92.94%)\n",
      "Processing chunk 568 with 100000 rows...\n",
      "Chunk 568 processed in 0.05 seconds\n",
      "Removed 90967 duplicates from this chunk\n",
      "Progress: 56,800,000/56,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 52,790,314 (92.94%)\n",
      "Processing chunk 569 with 100000 rows...\n",
      "Chunk 569 processed in 0.05 seconds\n",
      "Removed 90677 duplicates from this chunk\n",
      "Progress: 56,900,000/56,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 52,880,991 (92.94%)\n",
      "Processing chunk 570 with 100000 rows...\n",
      "Chunk 570 processed in 0.05 seconds\n",
      "Removed 90423 duplicates from this chunk\n",
      "Progress: 57,000,000/57,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 52,971,414 (92.93%)\n",
      "Processing chunk 571 with 100000 rows...\n",
      "Chunk 571 processed in 0.05 seconds\n",
      "Removed 90566 duplicates from this chunk\n",
      "Progress: 57,100,000/57,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 53,061,980 (92.93%)\n",
      "Processing chunk 572 with 100000 rows...\n",
      "Chunk 572 processed in 0.05 seconds\n",
      "Removed 90678 duplicates from this chunk\n",
      "Progress: 57,200,000/57,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 53,152,658 (92.92%)\n",
      "Processing chunk 573 with 100000 rows...\n",
      "Chunk 573 processed in 0.05 seconds\n",
      "Removed 90852 duplicates from this chunk\n",
      "Progress: 57,300,000/57,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 53,243,510 (92.92%)\n",
      "Processing chunk 574 with 100000 rows...\n",
      "Chunk 574 processed in 0.05 seconds\n",
      "Removed 90397 duplicates from this chunk\n",
      "Progress: 57,400,000/57,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 53,333,907 (92.92%)\n",
      "Processing chunk 575 with 100000 rows...\n",
      "Chunk 575 processed in 0.05 seconds\n",
      "Removed 90634 duplicates from this chunk\n",
      "Progress: 57,500,000/57,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 53,424,541 (92.91%)\n",
      "Processing chunk 576 with 100000 rows...\n",
      "Chunk 576 processed in 0.05 seconds\n",
      "Removed 90767 duplicates from this chunk\n",
      "Progress: 57,600,000/57,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 53,515,308 (92.91%)\n",
      "Processing chunk 577 with 100000 rows...\n",
      "Chunk 577 processed in 0.05 seconds\n",
      "Removed 91174 duplicates from this chunk\n",
      "Progress: 57,700,000/57,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 53,606,482 (92.91%)\n",
      "Processing chunk 578 with 100000 rows...\n",
      "Chunk 578 processed in 0.05 seconds\n",
      "Removed 90853 duplicates from this chunk\n",
      "Progress: 57,800,000/57,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 53,697,335 (92.90%)\n",
      "Processing chunk 579 with 100000 rows...\n",
      "Chunk 579 processed in 0.05 seconds\n",
      "Removed 90640 duplicates from this chunk\n",
      "Progress: 57,900,000/57,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 53,787,975 (92.90%)\n",
      "Processing chunk 580 with 100000 rows...\n",
      "Chunk 580 processed in 0.05 seconds\n",
      "Removed 91105 duplicates from this chunk\n",
      "Progress: 58,000,000/58,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 53,879,080 (92.89%)\n",
      "Processing chunk 581 with 100000 rows...\n",
      "Chunk 581 processed in 0.05 seconds\n",
      "Removed 90848 duplicates from this chunk\n",
      "Progress: 58,100,000/58,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 53,969,928 (92.89%)\n",
      "Processing chunk 582 with 100000 rows...\n",
      "Chunk 582 processed in 0.05 seconds\n",
      "Removed 91075 duplicates from this chunk\n",
      "Progress: 58,200,000/58,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 54,061,003 (92.89%)\n",
      "Processing chunk 583 with 100000 rows...\n",
      "Chunk 583 processed in 0.05 seconds\n",
      "Removed 91017 duplicates from this chunk\n",
      "Progress: 58,300,000/58,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 54,152,020 (92.89%)\n",
      "Processing chunk 584 with 100000 rows...\n",
      "Chunk 584 processed in 0.04 seconds\n",
      "Removed 91397 duplicates from this chunk\n",
      "Progress: 58,400,000/58,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 54,243,417 (92.88%)\n",
      "Processing chunk 585 with 100000 rows...\n",
      "Chunk 585 processed in 0.05 seconds\n",
      "Removed 91150 duplicates from this chunk\n",
      "Progress: 58,500,000/58,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 54,334,567 (92.88%)\n",
      "Processing chunk 586 with 100000 rows...\n",
      "Chunk 586 processed in 0.05 seconds\n",
      "Removed 91049 duplicates from this chunk\n",
      "Progress: 58,600,000/58,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 54,425,616 (92.88%)\n",
      "Processing chunk 587 with 100000 rows...\n",
      "Chunk 587 processed in 0.05 seconds\n",
      "Removed 90908 duplicates from this chunk\n",
      "Progress: 58,700,000/58,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 54,516,524 (92.87%)\n",
      "Processing chunk 588 with 100000 rows...\n",
      "Chunk 588 processed in 0.05 seconds\n",
      "Removed 91043 duplicates from this chunk\n",
      "Progress: 58,800,000/58,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 54,607,567 (92.87%)\n",
      "Processing chunk 589 with 100000 rows...\n",
      "Chunk 589 processed in 0.05 seconds\n",
      "Removed 90760 duplicates from this chunk\n",
      "Progress: 58,900,000/58,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 54,698,327 (92.87%)\n",
      "Processing chunk 590 with 100000 rows...\n",
      "Chunk 590 processed in 0.05 seconds\n",
      "Removed 91301 duplicates from this chunk\n",
      "Progress: 59,000,000/59,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 54,789,628 (92.86%)\n",
      "Processing chunk 591 with 100000 rows...\n",
      "Chunk 591 processed in 0.04 seconds\n",
      "Removed 91360 duplicates from this chunk\n",
      "Progress: 59,100,000/59,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 54,880,988 (92.86%)\n",
      "Processing chunk 592 with 100000 rows...\n",
      "Chunk 592 processed in 0.04 seconds\n",
      "Removed 91753 duplicates from this chunk\n",
      "Progress: 59,200,000/59,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 54,972,741 (92.86%)\n",
      "Processing chunk 593 with 100000 rows...\n",
      "Chunk 593 processed in 0.05 seconds\n",
      "Removed 91024 duplicates from this chunk\n",
      "Progress: 59,300,000/59,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 55,063,765 (92.86%)\n",
      "Processing chunk 594 with 100000 rows...\n",
      "Chunk 594 processed in 0.05 seconds\n",
      "Removed 91401 duplicates from this chunk\n",
      "Progress: 59,400,000/59,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 55,155,166 (92.85%)\n",
      "Processing chunk 595 with 100000 rows...\n",
      "Chunk 595 processed in 0.04 seconds\n",
      "Removed 91491 duplicates from this chunk\n",
      "Progress: 59,500,000/59,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 55,246,657 (92.85%)\n",
      "Processing chunk 596 with 100000 rows...\n",
      "Chunk 596 processed in 0.04 seconds\n",
      "Removed 91518 duplicates from this chunk\n",
      "Progress: 59,600,000/59,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 55,338,175 (92.85%)\n",
      "Processing chunk 597 with 100000 rows...\n",
      "Chunk 597 processed in 0.04 seconds\n",
      "Removed 91513 duplicates from this chunk\n",
      "Progress: 59,700,000/59,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 55,429,688 (92.85%)\n",
      "Processing chunk 598 with 100000 rows...\n",
      "Chunk 598 processed in 0.05 seconds\n",
      "Removed 91202 duplicates from this chunk\n",
      "Progress: 59,800,000/59,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 55,520,890 (92.84%)\n",
      "Processing chunk 599 with 100000 rows...\n",
      "Chunk 599 processed in 0.04 seconds\n",
      "Removed 91595 duplicates from this chunk\n",
      "Progress: 59,900,000/59,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 55,612,485 (92.84%)\n",
      "Processing chunk 600 with 100000 rows...\n",
      "Chunk 600 processed in 0.05 seconds\n",
      "Removed 91167 duplicates from this chunk\n",
      "Progress: 60,000,000/60,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 55,703,652 (92.84%)\n",
      "Processing chunk 601 with 100000 rows...\n",
      "Chunk 601 processed in 0.05 seconds\n",
      "Removed 91563 duplicates from this chunk\n",
      "Progress: 60,100,000/60,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 55,795,215 (92.84%)\n",
      "Processing chunk 602 with 100000 rows...\n",
      "Chunk 602 processed in 0.05 seconds\n",
      "Removed 91374 duplicates from this chunk\n",
      "Progress: 60,200,000/60,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 55,886,589 (92.83%)\n",
      "Processing chunk 603 with 100000 rows...\n",
      "Chunk 603 processed in 0.04 seconds\n",
      "Removed 91507 duplicates from this chunk\n",
      "Progress: 60,300,000/60,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 55,978,096 (92.83%)\n",
      "Processing chunk 604 with 100000 rows...\n",
      "Chunk 604 processed in 0.05 seconds\n",
      "Removed 91095 duplicates from this chunk\n",
      "Progress: 60,400,000/60,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 56,069,191 (92.83%)\n",
      "Processing chunk 605 with 100000 rows...\n",
      "Chunk 605 processed in 0.04 seconds\n",
      "Removed 91445 duplicates from this chunk\n",
      "Progress: 60,500,000/60,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 56,160,636 (92.83%)\n",
      "Processing chunk 606 with 100000 rows...\n",
      "Chunk 606 processed in 0.04 seconds\n",
      "Removed 91491 duplicates from this chunk\n",
      "Progress: 60,600,000/60,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 56,252,127 (92.83%)\n",
      "Processing chunk 607 with 100000 rows...\n",
      "Chunk 607 processed in 0.04 seconds\n",
      "Removed 91428 duplicates from this chunk\n",
      "Progress: 60,700,000/60,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 56,343,555 (92.82%)\n",
      "Processing chunk 608 with 100000 rows...\n",
      "Chunk 608 processed in 0.05 seconds\n",
      "Removed 91450 duplicates from this chunk\n",
      "Progress: 60,800,000/60,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 56,435,005 (92.82%)\n",
      "Processing chunk 609 with 100000 rows...\n",
      "Chunk 609 processed in 0.04 seconds\n",
      "Removed 91820 duplicates from this chunk\n",
      "Progress: 60,900,000/60,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 56,526,825 (92.82%)\n",
      "Processing chunk 610 with 100000 rows...\n",
      "Chunk 610 processed in 0.04 seconds\n",
      "Removed 91809 duplicates from this chunk\n",
      "Progress: 61,000,000/61,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 56,618,634 (92.82%)\n",
      "Processing chunk 611 with 100000 rows...\n",
      "Chunk 611 processed in 0.04 seconds\n",
      "Removed 91759 duplicates from this chunk\n",
      "Progress: 61,100,000/61,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 56,710,393 (92.82%)\n",
      "Processing chunk 612 with 100000 rows...\n",
      "Chunk 612 processed in 0.04 seconds\n",
      "Removed 91886 duplicates from this chunk\n",
      "Progress: 61,200,000/61,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 56,802,279 (92.81%)\n",
      "Processing chunk 613 with 100000 rows...\n",
      "Chunk 613 processed in 0.04 seconds\n",
      "Removed 91703 duplicates from this chunk\n",
      "Progress: 61,300,000/61,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 56,893,982 (92.81%)\n",
      "Processing chunk 614 with 100000 rows...\n",
      "Chunk 614 processed in 0.04 seconds\n",
      "Removed 91821 duplicates from this chunk\n",
      "Progress: 61,400,000/61,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 56,985,803 (92.81%)\n",
      "Processing chunk 615 with 100000 rows...\n",
      "Chunk 615 processed in 0.04 seconds\n",
      "Removed 91471 duplicates from this chunk\n",
      "Progress: 61,500,000/61,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 57,077,274 (92.81%)\n",
      "Processing chunk 616 with 100000 rows...\n",
      "Chunk 616 processed in 0.04 seconds\n",
      "Removed 92003 duplicates from this chunk\n",
      "Progress: 61,600,000/61,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 57,169,277 (92.81%)\n",
      "Processing chunk 617 with 100000 rows...\n",
      "Chunk 617 processed in 0.04 seconds\n",
      "Removed 91576 duplicates from this chunk\n",
      "Progress: 61,700,000/61,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 57,260,853 (92.81%)\n",
      "Processing chunk 618 with 100000 rows...\n",
      "Chunk 618 processed in 0.04 seconds\n",
      "Removed 91703 duplicates from this chunk\n",
      "Progress: 61,800,000/61,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 57,352,556 (92.80%)\n",
      "Processing chunk 619 with 100000 rows...\n",
      "Chunk 619 processed in 0.05 seconds\n",
      "Removed 91624 duplicates from this chunk\n",
      "Progress: 61,900,000/61,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 57,444,180 (92.80%)\n",
      "Processing chunk 620 with 100000 rows...\n",
      "Chunk 620 processed in 0.04 seconds\n",
      "Removed 92389 duplicates from this chunk\n",
      "Progress: 62,000,000/62,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 57,536,569 (92.80%)\n",
      "Processing chunk 621 with 100000 rows...\n",
      "Chunk 621 processed in 0.03 seconds\n",
      "Removed 93814 duplicates from this chunk\n",
      "Progress: 62,100,000/62,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 57,630,383 (92.80%)\n",
      "Processing chunk 622 with 100000 rows...\n",
      "Chunk 622 processed in 0.02 seconds\n",
      "Removed 95583 duplicates from this chunk\n",
      "Progress: 62,200,000/62,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 57,725,966 (92.81%)\n",
      "Processing chunk 623 with 100000 rows...\n",
      "Chunk 623 processed in 0.04 seconds\n",
      "Removed 93290 duplicates from this chunk\n",
      "Progress: 62,300,000/62,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 57,819,256 (92.81%)\n",
      "Processing chunk 624 with 100000 rows...\n",
      "Chunk 624 processed in 0.04 seconds\n",
      "Removed 92024 duplicates from this chunk\n",
      "Progress: 62,400,000/62,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 57,911,280 (92.81%)\n",
      "Processing chunk 625 with 100000 rows...\n",
      "Chunk 625 processed in 0.04 seconds\n",
      "Removed 91522 duplicates from this chunk\n",
      "Progress: 62,500,000/62,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 58,002,802 (92.80%)\n",
      "Processing chunk 626 with 100000 rows...\n",
      "Chunk 626 processed in 0.04 seconds\n",
      "Removed 91925 duplicates from this chunk\n",
      "Progress: 62,600,000/62,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 58,094,727 (92.80%)\n",
      "Processing chunk 627 with 100000 rows...\n",
      "Chunk 627 processed in 0.04 seconds\n",
      "Removed 91727 duplicates from this chunk\n",
      "Progress: 62,700,000/62,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 58,186,454 (92.80%)\n",
      "Processing chunk 628 with 100000 rows...\n",
      "Chunk 628 processed in 0.04 seconds\n",
      "Removed 91598 duplicates from this chunk\n",
      "Progress: 62,800,000/62,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 58,278,052 (92.80%)\n",
      "Processing chunk 629 with 100000 rows...\n",
      "Chunk 629 processed in 0.04 seconds\n",
      "Removed 91840 duplicates from this chunk\n",
      "Progress: 62,900,000/62,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 58,369,892 (92.80%)\n",
      "Processing chunk 630 with 100000 rows...\n",
      "Chunk 630 processed in 0.04 seconds\n",
      "Removed 91752 duplicates from this chunk\n",
      "Progress: 63,000,000/63,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 58,461,644 (92.80%)\n",
      "Processing chunk 631 with 100000 rows...\n",
      "Chunk 631 processed in 0.04 seconds\n",
      "Removed 91874 duplicates from this chunk\n",
      "Progress: 63,100,000/63,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 58,553,518 (92.79%)\n",
      "Processing chunk 632 with 100000 rows...\n",
      "Chunk 632 processed in 0.04 seconds\n",
      "Removed 92214 duplicates from this chunk\n",
      "Progress: 63,200,000/63,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 58,645,732 (92.79%)\n",
      "Processing chunk 633 with 100000 rows...\n",
      "Chunk 633 processed in 0.04 seconds\n",
      "Removed 92117 duplicates from this chunk\n",
      "Progress: 63,300,000/63,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 58,737,849 (92.79%)\n",
      "Processing chunk 634 with 100000 rows...\n",
      "Chunk 634 processed in 0.04 seconds\n",
      "Removed 92041 duplicates from this chunk\n",
      "Progress: 63,400,000/63,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 58,829,890 (92.79%)\n",
      "Processing chunk 635 with 100000 rows...\n",
      "Chunk 635 processed in 0.04 seconds\n",
      "Removed 91912 duplicates from this chunk\n",
      "Progress: 63,500,000/63,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 58,921,802 (92.79%)\n",
      "Processing chunk 636 with 100000 rows...\n",
      "Chunk 636 processed in 0.04 seconds\n",
      "Removed 91979 duplicates from this chunk\n",
      "Progress: 63,600,000/63,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 59,013,781 (92.79%)\n",
      "Processing chunk 637 with 100000 rows...\n",
      "Chunk 637 processed in 0.04 seconds\n",
      "Removed 92143 duplicates from this chunk\n",
      "Progress: 63,700,000/63,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 59,105,924 (92.79%)\n",
      "Processing chunk 638 with 100000 rows...\n",
      "Chunk 638 processed in 0.04 seconds\n",
      "Removed 92211 duplicates from this chunk\n",
      "Progress: 63,800,000/63,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 59,198,135 (92.79%)\n",
      "Processing chunk 639 with 100000 rows...\n",
      "Chunk 639 processed in 0.04 seconds\n",
      "Removed 91767 duplicates from this chunk\n",
      "Progress: 63,900,000/63,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 59,289,902 (92.79%)\n",
      "Processing chunk 640 with 100000 rows...\n",
      "Chunk 640 processed in 0.04 seconds\n",
      "Removed 92253 duplicates from this chunk\n",
      "Progress: 64,000,000/64,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 59,382,155 (92.78%)\n",
      "Processing chunk 641 with 100000 rows...\n",
      "Chunk 641 processed in 0.04 seconds\n",
      "Removed 92533 duplicates from this chunk\n",
      "Progress: 64,100,000/64,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 59,474,688 (92.78%)\n",
      "Processing chunk 642 with 100000 rows...\n",
      "Chunk 642 processed in 0.04 seconds\n",
      "Removed 92227 duplicates from this chunk\n",
      "Progress: 64,200,000/64,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 59,566,915 (92.78%)\n",
      "Processing chunk 643 with 100000 rows...\n",
      "Chunk 643 processed in 0.04 seconds\n",
      "Removed 91952 duplicates from this chunk\n",
      "Progress: 64,300,000/64,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 59,658,867 (92.78%)\n",
      "Processing chunk 644 with 100000 rows...\n",
      "Chunk 644 processed in 0.04 seconds\n",
      "Removed 92379 duplicates from this chunk\n",
      "Progress: 64,400,000/64,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 59,751,246 (92.78%)\n",
      "Processing chunk 645 with 100000 rows...\n",
      "Chunk 645 processed in 0.04 seconds\n",
      "Removed 92151 duplicates from this chunk\n",
      "Progress: 64,500,000/64,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 59,843,397 (92.78%)\n",
      "Processing chunk 646 with 100000 rows...\n",
      "Chunk 646 processed in 0.04 seconds\n",
      "Removed 92556 duplicates from this chunk\n",
      "Progress: 64,600,000/64,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 59,935,953 (92.78%)\n",
      "Processing chunk 647 with 100000 rows...\n",
      "Chunk 647 processed in 0.04 seconds\n",
      "Removed 92561 duplicates from this chunk\n",
      "Progress: 64,700,000/64,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 60,028,514 (92.78%)\n",
      "Processing chunk 648 with 100000 rows...\n",
      "Chunk 648 processed in 0.04 seconds\n",
      "Removed 92360 duplicates from this chunk\n",
      "Progress: 64,800,000/64,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 60,120,874 (92.78%)\n",
      "Processing chunk 649 with 100000 rows...\n",
      "Chunk 649 processed in 0.04 seconds\n",
      "Removed 92419 duplicates from this chunk\n",
      "Progress: 64,900,000/64,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 60,213,293 (92.78%)\n",
      "Processing chunk 650 with 100000 rows...\n",
      "Chunk 650 processed in 0.04 seconds\n",
      "Removed 92600 duplicates from this chunk\n",
      "Progress: 65,000,000/65,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 60,305,893 (92.78%)\n",
      "Processing chunk 651 with 100000 rows...\n",
      "Chunk 651 processed in 0.04 seconds\n",
      "Removed 92474 duplicates from this chunk\n",
      "Progress: 65,100,000/65,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 60,398,367 (92.78%)\n",
      "Processing chunk 652 with 100000 rows...\n",
      "Chunk 652 processed in 0.04 seconds\n",
      "Removed 92677 duplicates from this chunk\n",
      "Progress: 65,200,000/65,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 60,491,044 (92.78%)\n",
      "Processing chunk 653 with 100000 rows...\n",
      "Chunk 653 processed in 0.04 seconds\n",
      "Removed 92597 duplicates from this chunk\n",
      "Progress: 65,300,000/65,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 60,583,641 (92.78%)\n",
      "Processing chunk 654 with 100000 rows...\n",
      "Chunk 654 processed in 0.04 seconds\n",
      "Removed 92650 duplicates from this chunk\n",
      "Progress: 65,400,000/65,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 60,676,291 (92.78%)\n",
      "Processing chunk 655 with 100000 rows...\n",
      "Chunk 655 processed in 0.04 seconds\n",
      "Removed 92610 duplicates from this chunk\n",
      "Progress: 65,500,000/65,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 60,768,901 (92.78%)\n",
      "Processing chunk 656 with 100000 rows...\n",
      "Chunk 656 processed in 0.04 seconds\n",
      "Removed 92992 duplicates from this chunk\n",
      "Progress: 65,600,000/65,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 60,861,893 (92.78%)\n",
      "Processing chunk 657 with 100000 rows...\n",
      "Chunk 657 processed in 0.04 seconds\n",
      "Removed 93076 duplicates from this chunk\n",
      "Progress: 65,700,000/65,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 60,954,969 (92.78%)\n",
      "Processing chunk 658 with 100000 rows...\n",
      "Chunk 658 processed in 0.04 seconds\n",
      "Removed 92285 duplicates from this chunk\n",
      "Progress: 65,800,000/65,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 61,047,254 (92.78%)\n",
      "Processing chunk 659 with 100000 rows...\n",
      "Chunk 659 processed in 0.04 seconds\n",
      "Removed 92622 duplicates from this chunk\n",
      "Progress: 65,900,000/65,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 61,139,876 (92.78%)\n",
      "Processing chunk 660 with 100000 rows...\n",
      "Chunk 660 processed in 0.04 seconds\n",
      "Removed 92796 duplicates from this chunk\n",
      "Progress: 66,000,000/66,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 61,232,672 (92.78%)\n",
      "Processing chunk 661 with 100000 rows...\n",
      "Chunk 661 processed in 0.04 seconds\n",
      "Removed 92607 duplicates from this chunk\n",
      "Progress: 66,100,000/66,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 61,325,279 (92.78%)\n",
      "Processing chunk 662 with 100000 rows...\n",
      "Chunk 662 processed in 0.04 seconds\n",
      "Removed 92714 duplicates from this chunk\n",
      "Progress: 66,200,000/66,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 61,417,993 (92.78%)\n",
      "Processing chunk 663 with 100000 rows...\n",
      "Chunk 663 processed in 0.04 seconds\n",
      "Removed 92516 duplicates from this chunk\n",
      "Progress: 66,300,000/66,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 61,510,509 (92.78%)\n",
      "Processing chunk 664 with 100000 rows...\n",
      "Chunk 664 processed in 0.04 seconds\n",
      "Removed 92574 duplicates from this chunk\n",
      "Progress: 66,400,000/66,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 61,603,083 (92.78%)\n",
      "Processing chunk 665 with 100000 rows...\n",
      "Chunk 665 processed in 0.04 seconds\n",
      "Removed 92500 duplicates from this chunk\n",
      "Progress: 66,500,000/66,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 61,695,583 (92.78%)\n",
      "Processing chunk 666 with 100000 rows...\n",
      "Chunk 666 processed in 0.04 seconds\n",
      "Removed 92918 duplicates from this chunk\n",
      "Progress: 66,600,000/66,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 61,788,501 (92.78%)\n",
      "Processing chunk 667 with 100000 rows...\n",
      "Chunk 667 processed in 0.04 seconds\n",
      "Removed 93070 duplicates from this chunk\n",
      "Progress: 66,700,000/66,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 61,881,571 (92.78%)\n",
      "Processing chunk 668 with 100000 rows...\n",
      "Chunk 668 processed in 0.03 seconds\n",
      "Removed 93157 duplicates from this chunk\n",
      "Progress: 66,800,000/66,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 61,974,728 (92.78%)\n",
      "Processing chunk 669 with 100000 rows...\n",
      "Chunk 669 processed in 0.04 seconds\n",
      "Removed 92904 duplicates from this chunk\n",
      "Progress: 66,900,000/66,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 62,067,632 (92.78%)\n",
      "Processing chunk 670 with 100000 rows...\n",
      "Chunk 670 processed in 0.03 seconds\n",
      "Removed 93099 duplicates from this chunk\n",
      "Progress: 67,000,000/67,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 62,160,731 (92.78%)\n",
      "Processing chunk 671 with 100000 rows...\n",
      "Chunk 671 processed in 0.03 seconds\n",
      "Removed 93109 duplicates from this chunk\n",
      "Progress: 67,100,000/67,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 62,253,840 (92.78%)\n",
      "Processing chunk 672 with 100000 rows...\n",
      "Chunk 672 processed in 0.04 seconds\n",
      "Removed 92786 duplicates from this chunk\n",
      "Progress: 67,200,000/67,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 62,346,626 (92.78%)\n",
      "Processing chunk 673 with 100000 rows...\n",
      "Chunk 673 processed in 0.03 seconds\n",
      "Removed 93301 duplicates from this chunk\n",
      "Progress: 67,300,000/67,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 62,439,927 (92.78%)\n",
      "Processing chunk 674 with 100000 rows...\n",
      "Chunk 674 processed in 0.04 seconds\n",
      "Removed 92473 duplicates from this chunk\n",
      "Progress: 67,400,000/67,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 62,532,400 (92.78%)\n",
      "Processing chunk 675 with 100000 rows...\n",
      "Chunk 675 processed in 0.03 seconds\n",
      "Removed 93095 duplicates from this chunk\n",
      "Progress: 67,500,000/67,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 62,625,495 (92.78%)\n",
      "Processing chunk 676 with 100000 rows...\n",
      "Chunk 676 processed in 0.03 seconds\n",
      "Removed 93214 duplicates from this chunk\n",
      "Progress: 67,600,000/67,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 62,718,709 (92.78%)\n",
      "Processing chunk 677 with 100000 rows...\n",
      "Chunk 677 processed in 0.03 seconds\n",
      "Removed 93166 duplicates from this chunk\n",
      "Progress: 67,700,000/67,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 62,811,875 (92.78%)\n",
      "Processing chunk 678 with 100000 rows...\n",
      "Chunk 678 processed in 0.04 seconds\n",
      "Removed 92722 duplicates from this chunk\n",
      "Progress: 67,800,000/67,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 62,904,597 (92.78%)\n",
      "Processing chunk 679 with 100000 rows...\n",
      "Chunk 679 processed in 0.04 seconds\n",
      "Removed 92866 duplicates from this chunk\n",
      "Progress: 67,900,000/67,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 62,997,463 (92.78%)\n",
      "Processing chunk 680 with 100000 rows...\n",
      "Chunk 680 processed in 0.04 seconds\n",
      "Removed 92369 duplicates from this chunk\n",
      "Progress: 68,000,000/68,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 63,089,832 (92.78%)\n",
      "Processing chunk 681 with 100000 rows...\n",
      "Chunk 681 processed in 0.04 seconds\n",
      "Removed 92664 duplicates from this chunk\n",
      "Progress: 68,100,000/68,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 63,182,496 (92.78%)\n",
      "Processing chunk 682 with 100000 rows...\n",
      "Chunk 682 processed in 0.04 seconds\n",
      "Removed 92755 duplicates from this chunk\n",
      "Progress: 68,200,000/68,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 63,275,251 (92.78%)\n",
      "Processing chunk 683 with 100000 rows...\n",
      "Chunk 683 processed in 0.04 seconds\n",
      "Removed 92879 duplicates from this chunk\n",
      "Progress: 68,300,000/68,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 63,368,130 (92.78%)\n",
      "Processing chunk 684 with 100000 rows...\n",
      "Chunk 684 processed in 0.04 seconds\n",
      "Removed 92614 duplicates from this chunk\n",
      "Progress: 68,400,000/68,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 63,460,744 (92.78%)\n",
      "Processing chunk 685 with 100000 rows...\n",
      "Chunk 685 processed in 0.04 seconds\n",
      "Removed 92907 duplicates from this chunk\n",
      "Progress: 68,500,000/68,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 63,553,651 (92.78%)\n",
      "Processing chunk 686 with 100000 rows...\n",
      "Chunk 686 processed in 0.03 seconds\n",
      "Removed 93130 duplicates from this chunk\n",
      "Progress: 68,600,000/68,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 63,646,781 (92.78%)\n",
      "Processing chunk 687 with 100000 rows...\n",
      "Chunk 687 processed in 0.04 seconds\n",
      "Removed 92882 duplicates from this chunk\n",
      "Progress: 68,700,000/68,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 63,739,663 (92.78%)\n",
      "Processing chunk 688 with 100000 rows...\n",
      "Chunk 688 processed in 0.03 seconds\n",
      "Removed 93065 duplicates from this chunk\n",
      "Progress: 68,800,000/68,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 63,832,728 (92.78%)\n",
      "Processing chunk 689 with 100000 rows...\n",
      "Chunk 689 processed in 0.04 seconds\n",
      "Removed 92755 duplicates from this chunk\n",
      "Progress: 68,900,000/68,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 63,925,483 (92.78%)\n",
      "Processing chunk 690 with 100000 rows...\n",
      "Chunk 690 processed in 0.03 seconds\n",
      "Removed 93342 duplicates from this chunk\n",
      "Progress: 69,000,000/69,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 64,018,825 (92.78%)\n",
      "Processing chunk 691 with 100000 rows...\n",
      "Chunk 691 processed in 0.03 seconds\n",
      "Removed 93189 duplicates from this chunk\n",
      "Progress: 69,100,000/69,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 64,112,014 (92.78%)\n",
      "Processing chunk 692 with 100000 rows...\n",
      "Chunk 692 processed in 0.03 seconds\n",
      "Removed 93421 duplicates from this chunk\n",
      "Progress: 69,200,000/69,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 64,205,435 (92.78%)\n",
      "Processing chunk 693 with 100000 rows...\n",
      "Chunk 693 processed in 0.03 seconds\n",
      "Removed 93512 duplicates from this chunk\n",
      "Progress: 69,300,000/69,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 64,298,947 (92.78%)\n",
      "Processing chunk 694 with 100000 rows...\n",
      "Chunk 694 processed in 0.03 seconds\n",
      "Removed 93182 duplicates from this chunk\n",
      "Progress: 69,400,000/69,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 64,392,129 (92.78%)\n",
      "Processing chunk 695 with 100000 rows...\n",
      "Chunk 695 processed in 0.03 seconds\n",
      "Removed 93478 duplicates from this chunk\n",
      "Progress: 69,500,000/69,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 64,485,607 (92.79%)\n",
      "Processing chunk 696 with 100000 rows...\n",
      "Chunk 696 processed in 0.03 seconds\n",
      "Removed 93793 duplicates from this chunk\n",
      "Progress: 69,600,000/69,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 64,579,400 (92.79%)\n",
      "Processing chunk 697 with 100000 rows...\n",
      "Chunk 697 processed in 0.03 seconds\n",
      "Removed 93486 duplicates from this chunk\n",
      "Progress: 69,700,000/69,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 64,672,886 (92.79%)\n",
      "Processing chunk 698 with 100000 rows...\n",
      "Chunk 698 processed in 0.11 seconds\n",
      "Removed 93262 duplicates from this chunk\n",
      "Progress: 69,800,000/69,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 64,766,148 (92.79%)\n",
      "Processing chunk 699 with 100000 rows...\n",
      "Chunk 699 processed in 0.04 seconds\n",
      "Removed 92884 duplicates from this chunk\n",
      "Progress: 69,900,000/69,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 64,859,032 (92.79%)\n",
      "Processing chunk 700 with 100000 rows...\n",
      "Chunk 700 processed in 0.03 seconds\n",
      "Removed 93375 duplicates from this chunk\n",
      "Progress: 70,000,000/70,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 64,952,407 (92.79%)\n",
      "Processing chunk 701 with 100000 rows...\n",
      "Chunk 701 processed in 0.03 seconds\n",
      "Removed 93213 duplicates from this chunk\n",
      "Progress: 70,100,000/70,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 65,045,620 (92.79%)\n",
      "Processing chunk 702 with 100000 rows...\n",
      "Chunk 702 processed in 0.03 seconds\n",
      "Removed 93100 duplicates from this chunk\n",
      "Progress: 70,200,000/70,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 65,138,720 (92.79%)\n",
      "Processing chunk 703 with 100000 rows...\n",
      "Chunk 703 processed in 0.03 seconds\n",
      "Removed 93194 duplicates from this chunk\n",
      "Progress: 70,300,000/70,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 65,231,914 (92.79%)\n",
      "Processing chunk 704 with 100000 rows...\n",
      "Chunk 704 processed in 0.03 seconds\n",
      "Removed 93344 duplicates from this chunk\n",
      "Progress: 70,400,000/70,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 65,325,258 (92.79%)\n",
      "Processing chunk 705 with 100000 rows...\n",
      "Chunk 705 processed in 0.03 seconds\n",
      "Removed 93547 duplicates from this chunk\n",
      "Progress: 70,500,000/70,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 65,418,805 (92.79%)\n",
      "Processing chunk 706 with 100000 rows...\n",
      "Chunk 706 processed in 0.03 seconds\n",
      "Removed 93644 duplicates from this chunk\n",
      "Progress: 70,600,000/70,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 65,512,449 (92.79%)\n",
      "Processing chunk 707 with 100000 rows...\n",
      "Chunk 707 processed in 0.03 seconds\n",
      "Removed 93894 duplicates from this chunk\n",
      "Progress: 70,700,000/70,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 65,606,343 (92.80%)\n",
      "Processing chunk 708 with 100000 rows...\n",
      "Chunk 708 processed in 0.03 seconds\n",
      "Removed 93709 duplicates from this chunk\n",
      "Progress: 70,800,000/70,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 65,700,052 (92.80%)\n",
      "Processing chunk 709 with 100000 rows...\n",
      "Chunk 709 processed in 0.03 seconds\n",
      "Removed 93550 duplicates from this chunk\n",
      "Progress: 70,900,000/70,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 65,793,602 (92.80%)\n",
      "Processing chunk 710 with 100000 rows...\n",
      "Chunk 710 processed in 0.03 seconds\n",
      "Removed 94048 duplicates from this chunk\n",
      "Progress: 71,000,000/71,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 65,887,650 (92.80%)\n",
      "Processing chunk 711 with 100000 rows...\n",
      "Chunk 711 processed in 0.03 seconds\n",
      "Removed 93835 duplicates from this chunk\n",
      "Progress: 71,100,000/71,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 65,981,485 (92.80%)\n",
      "Processing chunk 712 with 100000 rows...\n",
      "Chunk 712 processed in 0.03 seconds\n",
      "Removed 94070 duplicates from this chunk\n",
      "Progress: 71,200,000/71,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 66,075,555 (92.80%)\n",
      "Processing chunk 713 with 100000 rows...\n",
      "Chunk 713 processed in 0.03 seconds\n",
      "Removed 93459 duplicates from this chunk\n",
      "Progress: 71,300,000/71,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 66,169,014 (92.80%)\n",
      "Processing chunk 714 with 100000 rows...\n",
      "Chunk 714 processed in 0.03 seconds\n",
      "Removed 93943 duplicates from this chunk\n",
      "Progress: 71,400,000/71,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 66,262,957 (92.81%)\n",
      "Processing chunk 715 with 100000 rows...\n",
      "Chunk 715 processed in 0.03 seconds\n",
      "Removed 93745 duplicates from this chunk\n",
      "Progress: 71,500,000/71,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 66,356,702 (92.81%)\n",
      "Processing chunk 716 with 100000 rows...\n",
      "Chunk 716 processed in 0.03 seconds\n",
      "Removed 93540 duplicates from this chunk\n",
      "Progress: 71,600,000/71,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 66,450,242 (92.81%)\n",
      "Processing chunk 717 with 100000 rows...\n",
      "Chunk 717 processed in 0.03 seconds\n",
      "Removed 93452 duplicates from this chunk\n",
      "Progress: 71,700,000/71,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 66,543,694 (92.81%)\n",
      "Processing chunk 718 with 100000 rows...\n",
      "Chunk 718 processed in 0.03 seconds\n",
      "Removed 93537 duplicates from this chunk\n",
      "Progress: 71,800,000/71,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 66,637,231 (92.81%)\n",
      "Processing chunk 719 with 100000 rows...\n",
      "Chunk 719 processed in 0.03 seconds\n",
      "Removed 93942 duplicates from this chunk\n",
      "Progress: 71,900,000/71,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 66,731,173 (92.81%)\n",
      "Processing chunk 720 with 100000 rows...\n",
      "Chunk 720 processed in 0.03 seconds\n",
      "Removed 93515 duplicates from this chunk\n",
      "Progress: 72,000,000/72,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 66,824,688 (92.81%)\n",
      "Processing chunk 721 with 100000 rows...\n",
      "Chunk 721 processed in 0.03 seconds\n",
      "Removed 94073 duplicates from this chunk\n",
      "Progress: 72,100,000/72,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 66,918,761 (92.81%)\n",
      "Processing chunk 722 with 100000 rows...\n",
      "Chunk 722 processed in 0.03 seconds\n",
      "Removed 93835 duplicates from this chunk\n",
      "Progress: 72,200,000/72,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 67,012,596 (92.82%)\n",
      "Processing chunk 723 with 100000 rows...\n",
      "Chunk 723 processed in 0.03 seconds\n",
      "Removed 93583 duplicates from this chunk\n",
      "Progress: 72,300,000/72,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 67,106,179 (92.82%)\n",
      "Processing chunk 724 with 100000 rows...\n",
      "Chunk 724 processed in 0.03 seconds\n",
      "Removed 94114 duplicates from this chunk\n",
      "Progress: 72,400,000/72,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 67,200,293 (92.82%)\n",
      "Processing chunk 725 with 100000 rows...\n",
      "Chunk 725 processed in 0.03 seconds\n",
      "Removed 94336 duplicates from this chunk\n",
      "Progress: 72,500,000/72,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 67,294,629 (92.82%)\n",
      "Processing chunk 726 with 100000 rows...\n",
      "Chunk 726 processed in 0.03 seconds\n",
      "Removed 94134 duplicates from this chunk\n",
      "Progress: 72,600,000/72,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 67,388,763 (92.82%)\n",
      "Processing chunk 727 with 100000 rows...\n",
      "Chunk 727 processed in 0.03 seconds\n",
      "Removed 93730 duplicates from this chunk\n",
      "Progress: 72,700,000/72,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 67,482,493 (92.82%)\n",
      "Processing chunk 728 with 100000 rows...\n",
      "Chunk 728 processed in 0.03 seconds\n",
      "Removed 93797 duplicates from this chunk\n",
      "Progress: 72,800,000/72,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 67,576,290 (92.82%)\n",
      "Processing chunk 729 with 100000 rows...\n",
      "Chunk 729 processed in 0.03 seconds\n",
      "Removed 94342 duplicates from this chunk\n",
      "Progress: 72,900,000/72,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 67,670,632 (92.83%)\n",
      "Processing chunk 730 with 100000 rows...\n",
      "Chunk 730 processed in 0.03 seconds\n",
      "Removed 93882 duplicates from this chunk\n",
      "Progress: 73,000,000/73,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 67,764,514 (92.83%)\n",
      "Processing chunk 731 with 100000 rows...\n",
      "Chunk 731 processed in 0.03 seconds\n",
      "Removed 93834 duplicates from this chunk\n",
      "Progress: 73,100,000/73,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 67,858,348 (92.83%)\n",
      "Processing chunk 732 with 100000 rows...\n",
      "Chunk 732 processed in 0.03 seconds\n",
      "Removed 94152 duplicates from this chunk\n",
      "Progress: 73,200,000/73,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 67,952,500 (92.83%)\n",
      "Processing chunk 733 with 100000 rows...\n",
      "Chunk 733 processed in 0.03 seconds\n",
      "Removed 93952 duplicates from this chunk\n",
      "Progress: 73,300,000/73,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 68,046,452 (92.83%)\n",
      "Processing chunk 734 with 100000 rows...\n",
      "Chunk 734 processed in 0.03 seconds\n",
      "Removed 93905 duplicates from this chunk\n",
      "Progress: 73,400,000/73,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 68,140,357 (92.83%)\n",
      "Processing chunk 735 with 100000 rows...\n",
      "Chunk 735 processed in 0.03 seconds\n",
      "Removed 93704 duplicates from this chunk\n",
      "Progress: 73,500,000/73,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 68,234,061 (92.84%)\n",
      "Processing chunk 736 with 100000 rows...\n",
      "Chunk 736 processed in 0.03 seconds\n",
      "Removed 94052 duplicates from this chunk\n",
      "Progress: 73,600,000/73,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 68,328,113 (92.84%)\n",
      "Processing chunk 737 with 100000 rows...\n",
      "Chunk 737 processed in 0.03 seconds\n",
      "Removed 93820 duplicates from this chunk\n",
      "Progress: 73,700,000/73,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 68,421,933 (92.84%)\n",
      "Processing chunk 738 with 100000 rows...\n",
      "Chunk 738 processed in 0.03 seconds\n",
      "Removed 93837 duplicates from this chunk\n",
      "Progress: 73,800,000/73,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 68,515,770 (92.84%)\n",
      "Processing chunk 739 with 100000 rows...\n",
      "Chunk 739 processed in 0.03 seconds\n",
      "Removed 93709 duplicates from this chunk\n",
      "Progress: 73,900,000/73,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 68,609,479 (92.84%)\n",
      "Processing chunk 740 with 100000 rows...\n",
      "Chunk 740 processed in 0.03 seconds\n",
      "Removed 94280 duplicates from this chunk\n",
      "Progress: 74,000,000/74,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 68,703,759 (92.84%)\n",
      "Processing chunk 741 with 100000 rows...\n",
      "Chunk 741 processed in 0.03 seconds\n",
      "Removed 94102 duplicates from this chunk\n",
      "Progress: 74,100,000/74,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 68,797,861 (92.84%)\n",
      "Processing chunk 742 with 100000 rows...\n",
      "Chunk 742 processed in 0.03 seconds\n",
      "Removed 93635 duplicates from this chunk\n",
      "Progress: 74,200,000/74,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 68,891,496 (92.85%)\n",
      "Processing chunk 743 with 100000 rows...\n",
      "Chunk 743 processed in 0.03 seconds\n",
      "Removed 93830 duplicates from this chunk\n",
      "Progress: 74,300,000/74,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 68,985,326 (92.85%)\n",
      "Processing chunk 744 with 100000 rows...\n",
      "Chunk 744 processed in 0.03 seconds\n",
      "Removed 94203 duplicates from this chunk\n",
      "Progress: 74,400,000/74,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 69,079,529 (92.85%)\n",
      "Processing chunk 745 with 100000 rows...\n",
      "Chunk 745 processed in 0.03 seconds\n",
      "Removed 94133 duplicates from this chunk\n",
      "Progress: 74,500,000/74,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 69,173,662 (92.85%)\n",
      "Processing chunk 746 with 100000 rows...\n",
      "Chunk 746 processed in 0.03 seconds\n",
      "Removed 93994 duplicates from this chunk\n",
      "Progress: 74,600,000/74,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 69,267,656 (92.85%)\n",
      "Processing chunk 747 with 100000 rows...\n",
      "Chunk 747 processed in 0.03 seconds\n",
      "Removed 93752 duplicates from this chunk\n",
      "Progress: 74,700,000/74,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 69,361,408 (92.85%)\n",
      "Processing chunk 748 with 100000 rows...\n",
      "Chunk 748 processed in 0.03 seconds\n",
      "Removed 93985 duplicates from this chunk\n",
      "Progress: 74,800,000/74,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 69,455,393 (92.85%)\n",
      "Processing chunk 749 with 100000 rows...\n",
      "Chunk 749 processed in 0.03 seconds\n",
      "Removed 94185 duplicates from this chunk\n",
      "Progress: 74,900,000/74,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 69,549,578 (92.86%)\n",
      "Processing chunk 750 with 100000 rows...\n",
      "Chunk 750 processed in 0.03 seconds\n",
      "Removed 94148 duplicates from this chunk\n",
      "Progress: 75,000,000/75,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 69,643,726 (92.86%)\n",
      "Processing chunk 751 with 100000 rows...\n",
      "Chunk 751 processed in 0.03 seconds\n",
      "Removed 94194 duplicates from this chunk\n",
      "Progress: 75,100,000/75,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 69,737,920 (92.86%)\n",
      "Processing chunk 752 with 100000 rows...\n",
      "Chunk 752 processed in 0.03 seconds\n",
      "Removed 93809 duplicates from this chunk\n",
      "Progress: 75,200,000/75,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 69,831,729 (92.86%)\n",
      "Processing chunk 753 with 100000 rows...\n",
      "Chunk 753 processed in 0.03 seconds\n",
      "Removed 94018 duplicates from this chunk\n",
      "Progress: 75,300,000/75,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 69,925,747 (92.86%)\n",
      "Processing chunk 754 with 100000 rows...\n",
      "Chunk 754 processed in 0.03 seconds\n",
      "Removed 93280 duplicates from this chunk\n",
      "Progress: 75,400,000/75,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 70,019,027 (92.86%)\n",
      "Processing chunk 755 with 100000 rows...\n",
      "Chunk 755 processed in 0.03 seconds\n",
      "Removed 93693 duplicates from this chunk\n",
      "Progress: 75,500,000/75,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 70,112,720 (92.86%)\n",
      "Processing chunk 756 with 100000 rows...\n",
      "Chunk 756 processed in 0.03 seconds\n",
      "Removed 93742 duplicates from this chunk\n",
      "Progress: 75,600,000/75,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 70,206,462 (92.87%)\n",
      "Processing chunk 757 with 100000 rows...\n",
      "Chunk 757 processed in 0.03 seconds\n",
      "Removed 93897 duplicates from this chunk\n",
      "Progress: 75,700,000/75,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 70,300,359 (92.87%)\n",
      "Processing chunk 758 with 100000 rows...\n",
      "Chunk 758 processed in 0.03 seconds\n",
      "Removed 94218 duplicates from this chunk\n",
      "Progress: 75,800,000/75,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 70,394,577 (92.87%)\n",
      "Processing chunk 759 with 100000 rows...\n",
      "Chunk 759 processed in 0.03 seconds\n",
      "Removed 93782 duplicates from this chunk\n",
      "Progress: 75,900,000/75,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 70,488,359 (92.87%)\n",
      "Processing chunk 760 with 100000 rows...\n",
      "Chunk 760 processed in 0.03 seconds\n",
      "Removed 93963 duplicates from this chunk\n",
      "Progress: 76,000,000/76,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 70,582,322 (92.87%)\n",
      "Processing chunk 761 with 100000 rows...\n",
      "Chunk 761 processed in 0.03 seconds\n",
      "Removed 94041 duplicates from this chunk\n",
      "Progress: 76,100,000/76,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 70,676,363 (92.87%)\n",
      "Processing chunk 762 with 100000 rows...\n",
      "Chunk 762 processed in 0.03 seconds\n",
      "Removed 94083 duplicates from this chunk\n",
      "Progress: 76,200,000/76,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 70,770,446 (92.87%)\n",
      "Processing chunk 763 with 100000 rows...\n",
      "Chunk 763 processed in 0.03 seconds\n",
      "Removed 94314 duplicates from this chunk\n",
      "Progress: 76,300,000/76,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 70,864,760 (92.88%)\n",
      "Processing chunk 764 with 100000 rows...\n",
      "Chunk 764 processed in 0.03 seconds\n",
      "Removed 94252 duplicates from this chunk\n",
      "Progress: 76,400,000/76,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 70,959,012 (92.88%)\n",
      "Processing chunk 765 with 100000 rows...\n",
      "Chunk 765 processed in 0.03 seconds\n",
      "Removed 93975 duplicates from this chunk\n",
      "Progress: 76,500,000/76,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 71,052,987 (92.88%)\n",
      "Processing chunk 766 with 100000 rows...\n",
      "Chunk 766 processed in 0.03 seconds\n",
      "Removed 94252 duplicates from this chunk\n",
      "Progress: 76,600,000/76,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 71,147,239 (92.88%)\n",
      "Processing chunk 767 with 100000 rows...\n",
      "Chunk 767 processed in 0.03 seconds\n",
      "Removed 93947 duplicates from this chunk\n",
      "Progress: 76,700,000/76,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 71,241,186 (92.88%)\n",
      "Processing chunk 768 with 100000 rows...\n",
      "Chunk 768 processed in 0.03 seconds\n",
      "Removed 93810 duplicates from this chunk\n",
      "Progress: 76,800,000/76,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 71,334,996 (92.88%)\n",
      "Processing chunk 769 with 100000 rows...\n",
      "Chunk 769 processed in 0.03 seconds\n",
      "Removed 93818 duplicates from this chunk\n",
      "Progress: 76,900,000/76,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 71,428,814 (92.89%)\n",
      "Processing chunk 770 with 100000 rows...\n",
      "Chunk 770 processed in 0.03 seconds\n",
      "Removed 94071 duplicates from this chunk\n",
      "Progress: 77,000,000/77,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 71,522,885 (92.89%)\n",
      "Processing chunk 771 with 100000 rows...\n",
      "Chunk 771 processed in 0.03 seconds\n",
      "Removed 93765 duplicates from this chunk\n",
      "Progress: 77,100,000/77,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 71,616,650 (92.89%)\n",
      "Processing chunk 772 with 100000 rows...\n",
      "Chunk 772 processed in 0.03 seconds\n",
      "Removed 94251 duplicates from this chunk\n",
      "Progress: 77,200,000/77,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 71,710,901 (92.89%)\n",
      "Processing chunk 773 with 100000 rows...\n",
      "Chunk 773 processed in 0.03 seconds\n",
      "Removed 94069 duplicates from this chunk\n",
      "Progress: 77,300,000/77,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 71,804,970 (92.89%)\n",
      "Processing chunk 774 with 100000 rows...\n",
      "Chunk 774 processed in 0.03 seconds\n",
      "Removed 93777 duplicates from this chunk\n",
      "Progress: 77,400,000/77,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 71,898,747 (92.89%)\n",
      "Processing chunk 775 with 100000 rows...\n",
      "Chunk 775 processed in 0.03 seconds\n",
      "Removed 93863 duplicates from this chunk\n",
      "Progress: 77,500,000/77,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 71,992,610 (92.89%)\n",
      "Processing chunk 776 with 100000 rows...\n",
      "Chunk 776 processed in 0.03 seconds\n",
      "Removed 93920 duplicates from this chunk\n",
      "Progress: 77,600,000/77,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 72,086,530 (92.90%)\n",
      "Processing chunk 777 with 100000 rows...\n",
      "Chunk 777 processed in 0.03 seconds\n",
      "Removed 94190 duplicates from this chunk\n",
      "Progress: 77,700,000/77,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 72,180,720 (92.90%)\n",
      "Processing chunk 778 with 100000 rows...\n",
      "Chunk 778 processed in 0.03 seconds\n",
      "Removed 94035 duplicates from this chunk\n",
      "Progress: 77,800,000/77,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 72,274,755 (92.90%)\n",
      "Processing chunk 779 with 100000 rows...\n",
      "Chunk 779 processed in 0.03 seconds\n",
      "Removed 93891 duplicates from this chunk\n",
      "Progress: 77,900,000/77,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 72,368,646 (92.90%)\n",
      "Processing chunk 780 with 100000 rows...\n",
      "Chunk 780 processed in 0.03 seconds\n",
      "Removed 93886 duplicates from this chunk\n",
      "Progress: 78,000,000/78,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 72,462,532 (92.90%)\n",
      "Processing chunk 781 with 100000 rows...\n",
      "Chunk 781 processed in 0.03 seconds\n",
      "Removed 94137 duplicates from this chunk\n",
      "Progress: 78,100,000/78,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 72,556,669 (92.90%)\n",
      "Processing chunk 782 with 100000 rows...\n",
      "Chunk 782 processed in 0.03 seconds\n",
      "Removed 94173 duplicates from this chunk\n",
      "Progress: 78,200,000/78,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 72,650,842 (92.90%)\n",
      "Processing chunk 783 with 100000 rows...\n",
      "Chunk 783 processed in 0.03 seconds\n",
      "Removed 94521 duplicates from this chunk\n",
      "Progress: 78,300,000/78,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 72,745,363 (92.91%)\n",
      "Processing chunk 784 with 100000 rows...\n",
      "Chunk 784 processed in 0.03 seconds\n",
      "Removed 94151 duplicates from this chunk\n",
      "Progress: 78,400,000/78,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 72,839,514 (92.91%)\n",
      "Processing chunk 785 with 100000 rows...\n",
      "Chunk 785 processed in 0.03 seconds\n",
      "Removed 93755 duplicates from this chunk\n",
      "Progress: 78,500,000/78,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 72,933,269 (92.91%)\n",
      "Processing chunk 786 with 100000 rows...\n",
      "Chunk 786 processed in 0.03 seconds\n",
      "Removed 93927 duplicates from this chunk\n",
      "Progress: 78,600,000/78,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 73,027,196 (92.91%)\n",
      "Processing chunk 787 with 100000 rows...\n",
      "Chunk 787 processed in 0.03 seconds\n",
      "Removed 94289 duplicates from this chunk\n",
      "Progress: 78,700,000/78,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 73,121,485 (92.91%)\n",
      "Processing chunk 788 with 100000 rows...\n",
      "Chunk 788 processed in 0.03 seconds\n",
      "Removed 93990 duplicates from this chunk\n",
      "Progress: 78,800,000/78,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 73,215,475 (92.91%)\n",
      "Processing chunk 789 with 100000 rows...\n",
      "Chunk 789 processed in 0.03 seconds\n",
      "Removed 94123 duplicates from this chunk\n",
      "Progress: 78,900,000/78,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 73,309,598 (92.91%)\n",
      "Processing chunk 790 with 100000 rows...\n",
      "Chunk 790 processed in 0.03 seconds\n",
      "Removed 94350 duplicates from this chunk\n",
      "Progress: 79,000,000/79,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 73,403,948 (92.92%)\n",
      "Processing chunk 791 with 100000 rows...\n",
      "Chunk 791 processed in 0.03 seconds\n",
      "Removed 94056 duplicates from this chunk\n",
      "Progress: 79,100,000/79,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 73,498,004 (92.92%)\n",
      "Processing chunk 792 with 100000 rows...\n",
      "Chunk 792 processed in 0.03 seconds\n",
      "Removed 94379 duplicates from this chunk\n",
      "Progress: 79,200,000/79,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 73,592,383 (92.92%)\n",
      "Processing chunk 793 with 100000 rows...\n",
      "Chunk 793 processed in 0.03 seconds\n",
      "Removed 93884 duplicates from this chunk\n",
      "Progress: 79,300,000/79,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 73,686,267 (92.92%)\n",
      "Processing chunk 794 with 100000 rows...\n",
      "Chunk 794 processed in 0.03 seconds\n",
      "Removed 94506 duplicates from this chunk\n",
      "Progress: 79,400,000/79,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 73,780,773 (92.92%)\n",
      "Processing chunk 795 with 100000 rows...\n",
      "Chunk 795 processed in 0.03 seconds\n",
      "Removed 94269 duplicates from this chunk\n",
      "Progress: 79,500,000/79,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 73,875,042 (92.92%)\n",
      "Processing chunk 796 with 100000 rows...\n",
      "Chunk 796 processed in 0.03 seconds\n",
      "Removed 94814 duplicates from this chunk\n",
      "Progress: 79,600,000/79,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 73,969,856 (92.93%)\n",
      "Processing chunk 797 with 100000 rows...\n",
      "Chunk 797 processed in 0.03 seconds\n",
      "Removed 94653 duplicates from this chunk\n",
      "Progress: 79,700,000/79,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 74,064,509 (92.93%)\n",
      "Processing chunk 798 with 100000 rows...\n",
      "Chunk 798 processed in 0.03 seconds\n",
      "Removed 93913 duplicates from this chunk\n",
      "Progress: 79,800,000/79,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 74,158,422 (92.93%)\n",
      "Processing chunk 799 with 100000 rows...\n",
      "Chunk 799 processed in 0.03 seconds\n",
      "Removed 94249 duplicates from this chunk\n",
      "Progress: 79,900,000/79,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 74,252,671 (92.93%)\n",
      "Processing chunk 800 with 100000 rows...\n",
      "Chunk 800 processed in 0.03 seconds\n",
      "Removed 94436 duplicates from this chunk\n",
      "Progress: 80,000,000/80,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 74,347,107 (92.93%)\n",
      "Processing chunk 801 with 100000 rows...\n",
      "Chunk 801 processed in 0.03 seconds\n",
      "Removed 93994 duplicates from this chunk\n",
      "Progress: 80,100,000/80,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 74,441,101 (92.94%)\n",
      "Processing chunk 802 with 100000 rows...\n",
      "Chunk 802 processed in 0.03 seconds\n",
      "Removed 94746 duplicates from this chunk\n",
      "Progress: 80,200,000/80,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 74,535,847 (92.94%)\n",
      "Processing chunk 803 with 100000 rows...\n",
      "Chunk 803 processed in 0.03 seconds\n",
      "Removed 94233 duplicates from this chunk\n",
      "Progress: 80,300,000/80,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 74,630,080 (92.94%)\n",
      "Processing chunk 804 with 100000 rows...\n",
      "Chunk 804 processed in 0.03 seconds\n",
      "Removed 94389 duplicates from this chunk\n",
      "Progress: 80,400,000/80,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 74,724,469 (92.94%)\n",
      "Processing chunk 805 with 100000 rows...\n",
      "Chunk 805 processed in 0.03 seconds\n",
      "Removed 94264 duplicates from this chunk\n",
      "Progress: 80,500,000/80,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 74,818,733 (92.94%)\n",
      "Processing chunk 806 with 100000 rows...\n",
      "Chunk 806 processed in 0.03 seconds\n",
      "Removed 94195 duplicates from this chunk\n",
      "Progress: 80,600,000/80,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 74,912,928 (92.94%)\n",
      "Processing chunk 807 with 100000 rows...\n",
      "Chunk 807 processed in 0.03 seconds\n",
      "Removed 94495 duplicates from this chunk\n",
      "Progress: 80,700,000/80,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 75,007,423 (92.95%)\n",
      "Processing chunk 808 with 100000 rows...\n",
      "Chunk 808 processed in 0.03 seconds\n",
      "Removed 94700 duplicates from this chunk\n",
      "Progress: 80,800,000/80,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 75,102,123 (92.95%)\n",
      "Processing chunk 809 with 100000 rows...\n",
      "Chunk 809 processed in 0.03 seconds\n",
      "Removed 94187 duplicates from this chunk\n",
      "Progress: 80,900,000/80,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 75,196,310 (92.95%)\n",
      "Processing chunk 810 with 100000 rows...\n",
      "Chunk 810 processed in 0.03 seconds\n",
      "Removed 94342 duplicates from this chunk\n",
      "Progress: 81,000,000/81,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 75,290,652 (92.95%)\n",
      "Processing chunk 811 with 100000 rows...\n",
      "Chunk 811 processed in 0.03 seconds\n",
      "Removed 94823 duplicates from this chunk\n",
      "Progress: 81,100,000/81,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 75,385,475 (92.95%)\n",
      "Processing chunk 812 with 100000 rows...\n",
      "Chunk 812 processed in 0.03 seconds\n",
      "Removed 94631 duplicates from this chunk\n",
      "Progress: 81,200,000/81,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 75,480,106 (92.96%)\n",
      "Processing chunk 813 with 100000 rows...\n",
      "Chunk 813 processed in 0.03 seconds\n",
      "Removed 94451 duplicates from this chunk\n",
      "Progress: 81,300,000/81,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 75,574,557 (92.96%)\n",
      "Processing chunk 814 with 100000 rows...\n",
      "Chunk 814 processed in 0.03 seconds\n",
      "Removed 94696 duplicates from this chunk\n",
      "Progress: 81,400,000/81,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 75,669,253 (92.96%)\n",
      "Processing chunk 815 with 100000 rows...\n",
      "Chunk 815 processed in 0.03 seconds\n",
      "Removed 94744 duplicates from this chunk\n",
      "Progress: 81,500,000/81,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 75,763,997 (92.96%)\n",
      "Processing chunk 816 with 100000 rows...\n",
      "Chunk 816 processed in 0.03 seconds\n",
      "Removed 94424 duplicates from this chunk\n",
      "Progress: 81,600,000/81,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 75,858,421 (92.96%)\n",
      "Processing chunk 817 with 100000 rows...\n",
      "Chunk 817 processed in 0.02 seconds\n",
      "Removed 94957 duplicates from this chunk\n",
      "Progress: 81,700,000/81,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 75,953,378 (92.97%)\n",
      "Processing chunk 818 with 100000 rows...\n",
      "Chunk 818 processed in 0.03 seconds\n",
      "Removed 94630 duplicates from this chunk\n",
      "Progress: 81,800,000/81,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 76,048,008 (92.97%)\n",
      "Processing chunk 819 with 100000 rows...\n",
      "Chunk 819 processed in 0.03 seconds\n",
      "Removed 94860 duplicates from this chunk\n",
      "Progress: 81,900,000/81,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 76,142,868 (92.97%)\n",
      "Processing chunk 820 with 100000 rows...\n",
      "Chunk 820 processed in 0.03 seconds\n",
      "Removed 94457 duplicates from this chunk\n",
      "Progress: 82,000,000/82,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 76,237,325 (92.97%)\n",
      "Processing chunk 821 with 100000 rows...\n",
      "Chunk 821 processed in 0.03 seconds\n",
      "Removed 94514 duplicates from this chunk\n",
      "Progress: 82,100,000/82,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 76,331,839 (92.97%)\n",
      "Processing chunk 822 with 100000 rows...\n",
      "Chunk 822 processed in 0.02 seconds\n",
      "Removed 94808 duplicates from this chunk\n",
      "Progress: 82,200,000/82,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 76,426,647 (92.98%)\n",
      "Processing chunk 823 with 100000 rows...\n",
      "Chunk 823 processed in 0.03 seconds\n",
      "Removed 94661 duplicates from this chunk\n",
      "Progress: 82,300,000/82,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 76,521,308 (92.98%)\n",
      "Processing chunk 824 with 100000 rows...\n",
      "Chunk 824 processed in 0.02 seconds\n",
      "Removed 95122 duplicates from this chunk\n",
      "Progress: 82,400,000/82,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 76,616,430 (92.98%)\n",
      "Processing chunk 825 with 100000 rows...\n",
      "Chunk 825 processed in 0.03 seconds\n",
      "Removed 93825 duplicates from this chunk\n",
      "Progress: 82,500,000/82,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 76,710,255 (92.98%)\n",
      "Processing chunk 826 with 100000 rows...\n",
      "Chunk 826 processed in 0.03 seconds\n",
      "Removed 94811 duplicates from this chunk\n",
      "Progress: 82,600,000/82,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 76,805,066 (92.98%)\n",
      "Processing chunk 827 with 100000 rows...\n",
      "Chunk 827 processed in 0.02 seconds\n",
      "Removed 94804 duplicates from this chunk\n",
      "Progress: 82,700,000/82,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 76,899,870 (92.99%)\n",
      "Processing chunk 828 with 100000 rows...\n",
      "Chunk 828 processed in 0.03 seconds\n",
      "Removed 94626 duplicates from this chunk\n",
      "Progress: 82,800,000/82,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 76,994,496 (92.99%)\n",
      "Processing chunk 829 with 100000 rows...\n",
      "Chunk 829 processed in 0.03 seconds\n",
      "Removed 94594 duplicates from this chunk\n",
      "Progress: 82,900,000/82,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 77,089,090 (92.99%)\n",
      "Processing chunk 830 with 100000 rows...\n",
      "Chunk 830 processed in 0.02 seconds\n",
      "Removed 94918 duplicates from this chunk\n",
      "Progress: 83,000,000/83,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 77,184,008 (92.99%)\n",
      "Processing chunk 831 with 100000 rows...\n",
      "Chunk 831 processed in 0.03 seconds\n",
      "Removed 94020 duplicates from this chunk\n",
      "Progress: 83,100,000/83,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 77,278,028 (92.99%)\n",
      "Processing chunk 832 with 100000 rows...\n",
      "Chunk 832 processed in 0.03 seconds\n",
      "Removed 94445 duplicates from this chunk\n",
      "Progress: 83,200,000/83,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 77,372,473 (93.00%)\n",
      "Processing chunk 833 with 100000 rows...\n",
      "Chunk 833 processed in 0.03 seconds\n",
      "Removed 94599 duplicates from this chunk\n",
      "Progress: 83,300,000/83,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 77,467,072 (93.00%)\n",
      "Processing chunk 834 with 100000 rows...\n",
      "Chunk 834 processed in 0.02 seconds\n",
      "Removed 95200 duplicates from this chunk\n",
      "Progress: 83,400,000/83,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 77,562,272 (93.00%)\n",
      "Processing chunk 835 with 100000 rows...\n",
      "Chunk 835 processed in 0.03 seconds\n",
      "Removed 94730 duplicates from this chunk\n",
      "Progress: 83,500,000/83,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 77,657,002 (93.00%)\n",
      "Processing chunk 836 with 100000 rows...\n",
      "Chunk 836 processed in 0.03 seconds\n",
      "Removed 94363 duplicates from this chunk\n",
      "Progress: 83,600,000/83,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 77,751,365 (93.00%)\n",
      "Processing chunk 837 with 100000 rows...\n",
      "Chunk 837 processed in 0.03 seconds\n",
      "Removed 94670 duplicates from this chunk\n",
      "Progress: 83,700,000/83,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 77,846,035 (93.01%)\n",
      "Processing chunk 838 with 100000 rows...\n",
      "Chunk 838 processed in 0.03 seconds\n",
      "Removed 94768 duplicates from this chunk\n",
      "Progress: 83,800,000/83,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 77,940,803 (93.01%)\n",
      "Processing chunk 839 with 100000 rows...\n",
      "Chunk 839 processed in 0.02 seconds\n",
      "Removed 94975 duplicates from this chunk\n",
      "Progress: 83,900,000/83,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 78,035,778 (93.01%)\n",
      "Processing chunk 840 with 100000 rows...\n",
      "Chunk 840 processed in 0.02 seconds\n",
      "Removed 95252 duplicates from this chunk\n",
      "Progress: 84,000,000/84,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 78,131,030 (93.01%)\n",
      "Processing chunk 841 with 100000 rows...\n",
      "Chunk 841 processed in 0.03 seconds\n",
      "Removed 94720 duplicates from this chunk\n",
      "Progress: 84,100,000/84,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 78,225,750 (93.02%)\n",
      "Processing chunk 842 with 100000 rows...\n",
      "Chunk 842 processed in 0.03 seconds\n",
      "Removed 94477 duplicates from this chunk\n",
      "Progress: 84,200,000/84,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 78,320,227 (93.02%)\n",
      "Processing chunk 843 with 100000 rows...\n",
      "Chunk 843 processed in 0.03 seconds\n",
      "Removed 94463 duplicates from this chunk\n",
      "Progress: 84,300,000/84,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 78,414,690 (93.02%)\n",
      "Processing chunk 844 with 100000 rows...\n",
      "Chunk 844 processed in 0.02 seconds\n",
      "Removed 95295 duplicates from this chunk\n",
      "Progress: 84,400,000/84,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 78,509,985 (93.02%)\n",
      "Processing chunk 845 with 100000 rows...\n",
      "Chunk 845 processed in 0.02 seconds\n",
      "Removed 95227 duplicates from this chunk\n",
      "Progress: 84,500,000/84,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 78,605,212 (93.02%)\n",
      "Processing chunk 846 with 100000 rows...\n",
      "Chunk 846 processed in 0.02 seconds\n",
      "Removed 95128 duplicates from this chunk\n",
      "Progress: 84,600,000/84,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 78,700,340 (93.03%)\n",
      "Processing chunk 847 with 100000 rows...\n",
      "Chunk 847 processed in 0.02 seconds\n",
      "Removed 94966 duplicates from this chunk\n",
      "Progress: 84,700,000/84,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 78,795,306 (93.03%)\n",
      "Processing chunk 848 with 100000 rows...\n",
      "Chunk 848 processed in 0.02 seconds\n",
      "Removed 95290 duplicates from this chunk\n",
      "Progress: 84,800,000/84,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 78,890,596 (93.03%)\n",
      "Processing chunk 849 with 100000 rows...\n",
      "Chunk 849 processed in 0.03 seconds\n",
      "Removed 94526 duplicates from this chunk\n",
      "Progress: 84,900,000/84,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 78,985,122 (93.03%)\n",
      "Processing chunk 850 with 100000 rows...\n",
      "Chunk 850 processed in 0.03 seconds\n",
      "Removed 94873 duplicates from this chunk\n",
      "Progress: 85,000,000/85,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 79,079,995 (93.04%)\n",
      "Processing chunk 851 with 100000 rows...\n",
      "Chunk 851 processed in 0.03 seconds\n",
      "Removed 94456 duplicates from this chunk\n",
      "Progress: 85,100,000/85,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 79,174,451 (93.04%)\n",
      "Processing chunk 852 with 100000 rows...\n",
      "Chunk 852 processed in 0.02 seconds\n",
      "Removed 94987 duplicates from this chunk\n",
      "Progress: 85,200,000/85,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 79,269,438 (93.04%)\n",
      "Processing chunk 853 with 100000 rows...\n",
      "Chunk 853 processed in 0.02 seconds\n",
      "Removed 95111 duplicates from this chunk\n",
      "Progress: 85,300,000/85,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 79,364,549 (93.04%)\n",
      "Processing chunk 854 with 100000 rows...\n",
      "Chunk 854 processed in 0.03 seconds\n",
      "Removed 94894 duplicates from this chunk\n",
      "Progress: 85,400,000/85,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 79,459,443 (93.04%)\n",
      "Processing chunk 855 with 100000 rows...\n",
      "Chunk 855 processed in 0.02 seconds\n",
      "Removed 95088 duplicates from this chunk\n",
      "Progress: 85,500,000/85,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 79,554,531 (93.05%)\n",
      "Processing chunk 856 with 100000 rows...\n",
      "Chunk 856 processed in 0.02 seconds\n",
      "Removed 94964 duplicates from this chunk\n",
      "Progress: 85,600,000/85,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 79,649,495 (93.05%)\n",
      "Processing chunk 857 with 100000 rows...\n",
      "Chunk 857 processed in 0.02 seconds\n",
      "Removed 95435 duplicates from this chunk\n",
      "Progress: 85,700,000/85,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 79,744,930 (93.05%)\n",
      "Processing chunk 858 with 100000 rows...\n",
      "Chunk 858 processed in 0.02 seconds\n",
      "Removed 94893 duplicates from this chunk\n",
      "Progress: 85,800,000/85,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 79,839,823 (93.05%)\n",
      "Processing chunk 859 with 100000 rows...\n",
      "Chunk 859 processed in 0.02 seconds\n",
      "Removed 95021 duplicates from this chunk\n",
      "Progress: 85,900,000/85,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 79,934,844 (93.06%)\n",
      "Processing chunk 860 with 100000 rows...\n",
      "Chunk 860 processed in 0.02 seconds\n",
      "Removed 95471 duplicates from this chunk\n",
      "Progress: 86,000,000/86,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 80,030,315 (93.06%)\n",
      "Processing chunk 861 with 100000 rows...\n",
      "Chunk 861 processed in 0.02 seconds\n",
      "Removed 95252 duplicates from this chunk\n",
      "Progress: 86,100,000/86,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 80,125,567 (93.06%)\n",
      "Processing chunk 862 with 100000 rows...\n",
      "Chunk 862 processed in 0.03 seconds\n",
      "Removed 94394 duplicates from this chunk\n",
      "Progress: 86,200,000/86,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 80,219,961 (93.06%)\n",
      "Processing chunk 863 with 100000 rows...\n",
      "Chunk 863 processed in 0.02 seconds\n",
      "Removed 95206 duplicates from this chunk\n",
      "Progress: 86,300,000/86,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 80,315,167 (93.07%)\n",
      "Processing chunk 864 with 100000 rows...\n",
      "Chunk 864 processed in 0.02 seconds\n",
      "Removed 95056 duplicates from this chunk\n",
      "Progress: 86,400,000/86,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 80,410,223 (93.07%)\n",
      "Processing chunk 865 with 100000 rows...\n",
      "Chunk 865 processed in 0.02 seconds\n",
      "Removed 95027 duplicates from this chunk\n",
      "Progress: 86,500,000/86,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 80,505,250 (93.07%)\n",
      "Processing chunk 866 with 100000 rows...\n",
      "Chunk 866 processed in 0.03 seconds\n",
      "Removed 94607 duplicates from this chunk\n",
      "Progress: 86,600,000/86,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 80,599,857 (93.07%)\n",
      "Processing chunk 867 with 100000 rows...\n",
      "Chunk 867 processed in 0.02 seconds\n",
      "Removed 95032 duplicates from this chunk\n",
      "Progress: 86,700,000/86,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 80,694,889 (93.07%)\n",
      "Processing chunk 868 with 100000 rows...\n",
      "Chunk 868 processed in 0.03 seconds\n",
      "Removed 94845 duplicates from this chunk\n",
      "Progress: 86,800,000/86,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 80,789,734 (93.08%)\n",
      "Processing chunk 869 with 100000 rows...\n",
      "Chunk 869 processed in 0.02 seconds\n",
      "Removed 95087 duplicates from this chunk\n",
      "Progress: 86,900,000/86,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 80,884,821 (93.08%)\n",
      "Processing chunk 870 with 100000 rows...\n",
      "Chunk 870 processed in 0.03 seconds\n",
      "Removed 94842 duplicates from this chunk\n",
      "Progress: 87,000,000/87,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 80,979,663 (93.08%)\n",
      "Processing chunk 871 with 100000 rows...\n",
      "Chunk 871 processed in 0.02 seconds\n",
      "Removed 95455 duplicates from this chunk\n",
      "Progress: 87,100,000/87,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 81,075,118 (93.08%)\n",
      "Processing chunk 872 with 100000 rows...\n",
      "Chunk 872 processed in 0.02 seconds\n",
      "Removed 95303 duplicates from this chunk\n",
      "Progress: 87,200,000/87,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 81,170,421 (93.09%)\n",
      "Processing chunk 873 with 100000 rows...\n",
      "Chunk 873 processed in 0.02 seconds\n",
      "Removed 95381 duplicates from this chunk\n",
      "Progress: 87,300,000/87,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 81,265,802 (93.09%)\n",
      "Processing chunk 874 with 100000 rows...\n",
      "Chunk 874 processed in 0.02 seconds\n",
      "Removed 95291 duplicates from this chunk\n",
      "Progress: 87,400,000/87,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 81,361,093 (93.09%)\n",
      "Processing chunk 875 with 100000 rows...\n",
      "Chunk 875 processed in 0.03 seconds\n",
      "Removed 94850 duplicates from this chunk\n",
      "Progress: 87,500,000/87,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 81,455,943 (93.09%)\n",
      "Processing chunk 876 with 100000 rows...\n",
      "Chunk 876 processed in 0.02 seconds\n",
      "Removed 95096 duplicates from this chunk\n",
      "Progress: 87,600,000/87,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 81,551,039 (93.09%)\n",
      "Processing chunk 877 with 100000 rows...\n",
      "Chunk 877 processed in 0.02 seconds\n",
      "Removed 94919 duplicates from this chunk\n",
      "Progress: 87,700,000/87,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 81,645,958 (93.10%)\n",
      "Processing chunk 878 with 100000 rows...\n",
      "Chunk 878 processed in 0.02 seconds\n",
      "Removed 95637 duplicates from this chunk\n",
      "Progress: 87,800,000/87,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 81,741,595 (93.10%)\n",
      "Processing chunk 879 with 100000 rows...\n",
      "Chunk 879 processed in 0.02 seconds\n",
      "Removed 94985 duplicates from this chunk\n",
      "Progress: 87,900,000/87,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 81,836,580 (93.10%)\n",
      "Processing chunk 880 with 100000 rows...\n",
      "Chunk 880 processed in 0.03 seconds\n",
      "Removed 94839 duplicates from this chunk\n",
      "Progress: 88,000,000/88,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 81,931,419 (93.10%)\n",
      "Processing chunk 881 with 100000 rows...\n",
      "Chunk 881 processed in 0.02 seconds\n",
      "Removed 95032 duplicates from this chunk\n",
      "Progress: 88,100,000/88,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 82,026,451 (93.11%)\n",
      "Processing chunk 882 with 100000 rows...\n",
      "Chunk 882 processed in 0.02 seconds\n",
      "Removed 95237 duplicates from this chunk\n",
      "Progress: 88,200,000/88,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 82,121,688 (93.11%)\n",
      "Processing chunk 883 with 100000 rows...\n",
      "Chunk 883 processed in 0.02 seconds\n",
      "Removed 95346 duplicates from this chunk\n",
      "Progress: 88,300,000/88,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 82,217,034 (93.11%)\n",
      "Processing chunk 884 with 100000 rows...\n",
      "Chunk 884 processed in 0.03 seconds\n",
      "Removed 94845 duplicates from this chunk\n",
      "Progress: 88,400,000/88,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 82,311,879 (93.11%)\n",
      "Processing chunk 885 with 100000 rows...\n",
      "Chunk 885 processed in 0.03 seconds\n",
      "Removed 94640 duplicates from this chunk\n",
      "Progress: 88,500,000/88,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 82,406,519 (93.11%)\n",
      "Processing chunk 886 with 100000 rows...\n",
      "Chunk 886 processed in 0.02 seconds\n",
      "Removed 94848 duplicates from this chunk\n",
      "Progress: 88,600,000/88,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 82,501,367 (93.12%)\n",
      "Processing chunk 887 with 100000 rows...\n",
      "Chunk 887 processed in 0.02 seconds\n",
      "Removed 95178 duplicates from this chunk\n",
      "Progress: 88,700,000/88,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 82,596,545 (93.12%)\n",
      "Processing chunk 888 with 100000 rows...\n",
      "Chunk 888 processed in 0.02 seconds\n",
      "Removed 95083 duplicates from this chunk\n",
      "Progress: 88,800,000/88,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 82,691,628 (93.12%)\n",
      "Processing chunk 889 with 100000 rows...\n",
      "Chunk 889 processed in 0.03 seconds\n",
      "Removed 94584 duplicates from this chunk\n",
      "Progress: 88,900,000/88,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 82,786,212 (93.12%)\n",
      "Processing chunk 890 with 100000 rows...\n",
      "Chunk 890 processed in 0.02 seconds\n",
      "Removed 94992 duplicates from this chunk\n",
      "Progress: 89,000,000/89,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 82,881,204 (93.12%)\n",
      "Processing chunk 891 with 100000 rows...\n",
      "Chunk 891 processed in 0.02 seconds\n",
      "Removed 95025 duplicates from this chunk\n",
      "Progress: 89,100,000/89,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 82,976,229 (93.13%)\n",
      "Processing chunk 892 with 100000 rows...\n",
      "Chunk 892 processed in 0.02 seconds\n",
      "Removed 95148 duplicates from this chunk\n",
      "Progress: 89,200,000/89,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 83,071,377 (93.13%)\n",
      "Processing chunk 893 with 100000 rows...\n",
      "Chunk 893 processed in 0.03 seconds\n",
      "Removed 94844 duplicates from this chunk\n",
      "Progress: 89,300,000/89,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 83,166,221 (93.13%)\n",
      "Processing chunk 894 with 100000 rows...\n",
      "Chunk 894 processed in 0.02 seconds\n",
      "Removed 95031 duplicates from this chunk\n",
      "Progress: 89,400,000/89,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 83,261,252 (93.13%)\n",
      "Processing chunk 895 with 100000 rows...\n",
      "Chunk 895 processed in 0.02 seconds\n",
      "Removed 95561 duplicates from this chunk\n",
      "Progress: 89,500,000/89,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 83,356,813 (93.14%)\n",
      "Processing chunk 896 with 100000 rows...\n",
      "Chunk 896 processed in 0.02 seconds\n",
      "Removed 95100 duplicates from this chunk\n",
      "Progress: 89,600,000/89,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 83,451,913 (93.14%)\n",
      "Processing chunk 897 with 100000 rows...\n",
      "Chunk 897 processed in 0.02 seconds\n",
      "Removed 95819 duplicates from this chunk\n",
      "Progress: 89,700,000/89,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 83,547,732 (93.14%)\n",
      "Processing chunk 898 with 100000 rows...\n",
      "Chunk 898 processed in 0.02 seconds\n",
      "Removed 95023 duplicates from this chunk\n",
      "Progress: 89,800,000/89,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 83,642,755 (93.14%)\n",
      "Processing chunk 899 with 100000 rows...\n",
      "Chunk 899 processed in 0.03 seconds\n",
      "Removed 93996 duplicates from this chunk\n",
      "Progress: 89,900,000/89,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 83,736,751 (93.14%)\n",
      "Processing chunk 900 with 100000 rows...\n",
      "Chunk 900 processed in 0.03 seconds\n",
      "Removed 94381 duplicates from this chunk\n",
      "Progress: 90,000,000/90,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 83,831,132 (93.15%)\n",
      "Processing chunk 901 with 100000 rows...\n",
      "Chunk 901 processed in 0.02 seconds\n",
      "Removed 95494 duplicates from this chunk\n",
      "Progress: 90,100,000/90,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 83,926,626 (93.15%)\n",
      "Processing chunk 902 with 100000 rows...\n",
      "Chunk 902 processed in 0.03 seconds\n",
      "Removed 94513 duplicates from this chunk\n",
      "Progress: 90,200,000/90,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 84,021,139 (93.15%)\n",
      "Processing chunk 903 with 100000 rows...\n",
      "Chunk 903 processed in 0.02 seconds\n",
      "Removed 95426 duplicates from this chunk\n",
      "Progress: 90,300,000/90,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 84,116,565 (93.15%)\n",
      "Processing chunk 904 with 100000 rows...\n",
      "Chunk 904 processed in 0.02 seconds\n",
      "Removed 95298 duplicates from this chunk\n",
      "Progress: 90,400,000/90,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 84,211,863 (93.15%)\n",
      "Processing chunk 905 with 100000 rows...\n",
      "Chunk 905 processed in 0.02 seconds\n",
      "Removed 96138 duplicates from this chunk\n",
      "Progress: 90,500,000/90,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 84,308,001 (93.16%)\n",
      "Processing chunk 906 with 100000 rows...\n",
      "Chunk 906 processed in 0.02 seconds\n",
      "Removed 95573 duplicates from this chunk\n",
      "Progress: 90,600,000/90,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 84,403,574 (93.16%)\n",
      "Processing chunk 907 with 100000 rows...\n",
      "Chunk 907 processed in 0.02 seconds\n",
      "Removed 95091 duplicates from this chunk\n",
      "Progress: 90,700,000/90,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 84,498,665 (93.16%)\n",
      "Processing chunk 908 with 100000 rows...\n",
      "Chunk 908 processed in 0.02 seconds\n",
      "Removed 95159 duplicates from this chunk\n",
      "Progress: 90,800,000/90,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 84,593,824 (93.17%)\n",
      "Processing chunk 909 with 100000 rows...\n",
      "Chunk 909 processed in 0.02 seconds\n",
      "Removed 95090 duplicates from this chunk\n",
      "Progress: 90,900,000/90,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 84,688,914 (93.17%)\n",
      "Processing chunk 910 with 100000 rows...\n",
      "Chunk 910 processed in 0.02 seconds\n",
      "Removed 95361 duplicates from this chunk\n",
      "Progress: 91,000,000/91,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 84,784,275 (93.17%)\n",
      "Processing chunk 911 with 100000 rows...\n",
      "Chunk 911 processed in 0.03 seconds\n",
      "Removed 94166 duplicates from this chunk\n",
      "Progress: 91,100,000/91,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 84,878,441 (93.17%)\n",
      "Processing chunk 912 with 100000 rows...\n",
      "Chunk 912 processed in 0.02 seconds\n",
      "Removed 95144 duplicates from this chunk\n",
      "Progress: 91,200,000/91,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 84,973,585 (93.17%)\n",
      "Processing chunk 913 with 100000 rows...\n",
      "Chunk 913 processed in 0.02 seconds\n",
      "Removed 94824 duplicates from this chunk\n",
      "Progress: 91,300,000/91,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 85,068,409 (93.17%)\n",
      "Processing chunk 914 with 100000 rows...\n",
      "Chunk 914 processed in 0.02 seconds\n",
      "Removed 95341 duplicates from this chunk\n",
      "Progress: 91,400,000/91,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 85,163,750 (93.18%)\n",
      "Processing chunk 915 with 100000 rows...\n",
      "Chunk 915 processed in 0.02 seconds\n",
      "Removed 95269 duplicates from this chunk\n",
      "Progress: 91,500,000/91,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 85,259,019 (93.18%)\n",
      "Processing chunk 916 with 100000 rows...\n",
      "Chunk 916 processed in 0.02 seconds\n",
      "Removed 95055 duplicates from this chunk\n",
      "Progress: 91,600,000/91,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 85,354,074 (93.18%)\n",
      "Processing chunk 917 with 100000 rows...\n",
      "Chunk 917 processed in 0.02 seconds\n",
      "Removed 95079 duplicates from this chunk\n",
      "Progress: 91,700,000/91,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 85,449,153 (93.18%)\n",
      "Processing chunk 918 with 100000 rows...\n",
      "Chunk 918 processed in 0.02 seconds\n",
      "Removed 94961 duplicates from this chunk\n",
      "Progress: 91,800,000/91,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 85,544,114 (93.19%)\n",
      "Processing chunk 919 with 100000 rows...\n",
      "Chunk 919 processed in 0.03 seconds\n",
      "Removed 94387 duplicates from this chunk\n",
      "Progress: 91,900,000/91,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 85,638,501 (93.19%)\n",
      "Processing chunk 920 with 100000 rows...\n",
      "Chunk 920 processed in 0.03 seconds\n",
      "Removed 94191 duplicates from this chunk\n",
      "Progress: 92,000,000/92,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 85,732,692 (93.19%)\n",
      "Processing chunk 921 with 100000 rows...\n",
      "Chunk 921 processed in 0.02 seconds\n",
      "Removed 95016 duplicates from this chunk\n",
      "Progress: 92,100,000/92,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 85,827,708 (93.19%)\n",
      "Processing chunk 922 with 100000 rows...\n",
      "Chunk 922 processed in 0.03 seconds\n",
      "Removed 94723 duplicates from this chunk\n",
      "Progress: 92,200,000/92,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 85,922,431 (93.19%)\n",
      "Processing chunk 923 with 100000 rows...\n",
      "Chunk 923 processed in 0.02 seconds\n",
      "Removed 94861 duplicates from this chunk\n",
      "Progress: 92,300,000/92,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 86,017,292 (93.19%)\n",
      "Processing chunk 924 with 100000 rows...\n",
      "Chunk 924 processed in 0.02 seconds\n",
      "Removed 95882 duplicates from this chunk\n",
      "Progress: 92,400,000/92,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 86,113,174 (93.20%)\n",
      "Processing chunk 925 with 100000 rows...\n",
      "Chunk 925 processed in 0.03 seconds\n",
      "Removed 94178 duplicates from this chunk\n",
      "Progress: 92,500,000/92,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 86,207,352 (93.20%)\n",
      "Processing chunk 926 with 100000 rows...\n",
      "Chunk 926 processed in 0.02 seconds\n",
      "Removed 95016 duplicates from this chunk\n",
      "Progress: 92,600,000/92,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 86,302,368 (93.20%)\n",
      "Processing chunk 927 with 100000 rows...\n",
      "Chunk 927 processed in 0.02 seconds\n",
      "Removed 94943 duplicates from this chunk\n",
      "Progress: 92,700,000/92,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 86,397,311 (93.20%)\n",
      "Processing chunk 928 with 100000 rows...\n",
      "Chunk 928 processed in 0.03 seconds\n",
      "Removed 94687 duplicates from this chunk\n",
      "Progress: 92,800,000/92,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 86,491,998 (93.20%)\n",
      "Processing chunk 929 with 100000 rows...\n",
      "Chunk 929 processed in 0.02 seconds\n",
      "Removed 95331 duplicates from this chunk\n",
      "Progress: 92,900,000/92,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 86,587,329 (93.20%)\n",
      "Processing chunk 930 with 100000 rows...\n",
      "Chunk 930 processed in 0.02 seconds\n",
      "Removed 95092 duplicates from this chunk\n",
      "Progress: 93,000,000/93,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 86,682,421 (93.21%)\n",
      "Processing chunk 931 with 100000 rows...\n",
      "Chunk 931 processed in 0.03 seconds\n",
      "Removed 94998 duplicates from this chunk\n",
      "Progress: 93,100,000/93,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 86,777,419 (93.21%)\n",
      "Processing chunk 932 with 100000 rows...\n",
      "Chunk 932 processed in 0.03 seconds\n",
      "Removed 94783 duplicates from this chunk\n",
      "Progress: 93,200,000/93,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 86,872,202 (93.21%)\n",
      "Processing chunk 933 with 100000 rows...\n",
      "Chunk 933 processed in 0.02 seconds\n",
      "Removed 95032 duplicates from this chunk\n",
      "Progress: 93,300,000/93,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 86,967,234 (93.21%)\n",
      "Processing chunk 934 with 100000 rows...\n",
      "Chunk 934 processed in 0.03 seconds\n",
      "Removed 94898 duplicates from this chunk\n",
      "Progress: 93,400,000/93,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 87,062,132 (93.21%)\n",
      "Processing chunk 935 with 100000 rows...\n",
      "Chunk 935 processed in 0.03 seconds\n",
      "Removed 94802 duplicates from this chunk\n",
      "Progress: 93,500,000/93,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 87,156,934 (93.22%)\n",
      "Processing chunk 936 with 100000 rows...\n",
      "Chunk 936 processed in 0.02 seconds\n",
      "Removed 95428 duplicates from this chunk\n",
      "Progress: 93,600,000/93,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 87,252,362 (93.22%)\n",
      "Processing chunk 937 with 100000 rows...\n",
      "Chunk 937 processed in 0.02 seconds\n",
      "Removed 95194 duplicates from this chunk\n",
      "Progress: 93,700,000/93,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 87,347,556 (93.22%)\n",
      "Processing chunk 938 with 100000 rows...\n",
      "Chunk 938 processed in 0.02 seconds\n",
      "Removed 95310 duplicates from this chunk\n",
      "Progress: 93,800,000/93,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 87,442,866 (93.22%)\n",
      "Processing chunk 939 with 100000 rows...\n",
      "Chunk 939 processed in 0.02 seconds\n",
      "Removed 95148 duplicates from this chunk\n",
      "Progress: 93,900,000/93,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 87,538,014 (93.22%)\n",
      "Processing chunk 940 with 100000 rows...\n",
      "Chunk 940 processed in 0.03 seconds\n",
      "Removed 94236 duplicates from this chunk\n",
      "Progress: 94,000,000/94,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 87,632,250 (93.23%)\n",
      "Processing chunk 941 with 100000 rows...\n",
      "Chunk 941 processed in 0.03 seconds\n",
      "Removed 94502 duplicates from this chunk\n",
      "Progress: 94,100,000/94,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 87,726,752 (93.23%)\n",
      "Processing chunk 942 with 100000 rows...\n",
      "Chunk 942 processed in 0.03 seconds\n",
      "Removed 94848 duplicates from this chunk\n",
      "Progress: 94,200,000/94,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 87,821,600 (93.23%)\n",
      "Processing chunk 943 with 100000 rows...\n",
      "Chunk 943 processed in 0.03 seconds\n",
      "Removed 94961 duplicates from this chunk\n",
      "Progress: 94,300,000/94,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 87,916,561 (93.23%)\n",
      "Processing chunk 944 with 100000 rows...\n",
      "Chunk 944 processed in 0.02 seconds\n",
      "Removed 94981 duplicates from this chunk\n",
      "Progress: 94,400,000/94,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 88,011,542 (93.23%)\n",
      "Processing chunk 945 with 100000 rows...\n",
      "Chunk 945 processed in 0.02 seconds\n",
      "Removed 95306 duplicates from this chunk\n",
      "Progress: 94,500,000/94,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 88,106,848 (93.23%)\n",
      "Processing chunk 946 with 100000 rows...\n",
      "Chunk 946 processed in 0.02 seconds\n",
      "Removed 95334 duplicates from this chunk\n",
      "Progress: 94,600,000/94,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 88,202,182 (93.24%)\n",
      "Processing chunk 947 with 100000 rows...\n",
      "Chunk 947 processed in 0.03 seconds\n",
      "Removed 94586 duplicates from this chunk\n",
      "Progress: 94,700,000/94,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 88,296,768 (93.24%)\n",
      "Processing chunk 948 with 100000 rows...\n",
      "Chunk 948 processed in 0.02 seconds\n",
      "Removed 95049 duplicates from this chunk\n",
      "Progress: 94,800,000/94,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 88,391,817 (93.24%)\n",
      "Processing chunk 949 with 100000 rows...\n",
      "Chunk 949 processed in 0.02 seconds\n",
      "Removed 95099 duplicates from this chunk\n",
      "Progress: 94,900,000/94,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 88,486,916 (93.24%)\n",
      "Processing chunk 950 with 100000 rows...\n",
      "Chunk 950 processed in 0.02 seconds\n",
      "Removed 95117 duplicates from this chunk\n",
      "Progress: 95,000,000/95,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 88,582,033 (93.24%)\n",
      "Processing chunk 951 with 100000 rows...\n",
      "Chunk 951 processed in 0.03 seconds\n",
      "Removed 94652 duplicates from this chunk\n",
      "Progress: 95,100,000/95,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 88,676,685 (93.25%)\n",
      "Processing chunk 952 with 100000 rows...\n",
      "Chunk 952 processed in 0.03 seconds\n",
      "Removed 94287 duplicates from this chunk\n",
      "Progress: 95,200,000/95,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 88,770,972 (93.25%)\n",
      "Processing chunk 953 with 100000 rows...\n",
      "Chunk 953 processed in 0.02 seconds\n",
      "Removed 94924 duplicates from this chunk\n",
      "Progress: 95,300,000/95,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 88,865,896 (93.25%)\n",
      "Processing chunk 954 with 100000 rows...\n",
      "Chunk 954 processed in 0.02 seconds\n",
      "Removed 95320 duplicates from this chunk\n",
      "Progress: 95,400,000/95,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 88,961,216 (93.25%)\n",
      "Processing chunk 955 with 100000 rows...\n",
      "Chunk 955 processed in 0.03 seconds\n",
      "Removed 94758 duplicates from this chunk\n",
      "Progress: 95,500,000/95,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 89,055,974 (93.25%)\n",
      "Processing chunk 956 with 100000 rows...\n",
      "Chunk 956 processed in 0.02 seconds\n",
      "Removed 95116 duplicates from this chunk\n",
      "Progress: 95,600,000/95,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 89,151,090 (93.25%)\n",
      "Processing chunk 957 with 100000 rows...\n",
      "Chunk 957 processed in 0.03 seconds\n",
      "Removed 94485 duplicates from this chunk\n",
      "Progress: 95,700,000/95,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 89,245,575 (93.26%)\n",
      "Processing chunk 958 with 100000 rows...\n",
      "Chunk 958 processed in 0.02 seconds\n",
      "Removed 95761 duplicates from this chunk\n",
      "Progress: 95,800,000/95,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 89,341,336 (93.26%)\n",
      "Processing chunk 959 with 100000 rows...\n",
      "Chunk 959 processed in 0.03 seconds\n",
      "Removed 94562 duplicates from this chunk\n",
      "Progress: 95,900,000/95,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 89,435,898 (93.26%)\n",
      "Processing chunk 960 with 100000 rows...\n",
      "Chunk 960 processed in 0.03 seconds\n",
      "Removed 94403 duplicates from this chunk\n",
      "Progress: 96,000,000/96,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 89,530,301 (93.26%)\n",
      "Processing chunk 961 with 100000 rows...\n",
      "Chunk 961 processed in 0.02 seconds\n",
      "Removed 95061 duplicates from this chunk\n",
      "Progress: 96,100,000/96,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 89,625,362 (93.26%)\n",
      "Processing chunk 962 with 100000 rows...\n",
      "Chunk 962 processed in 0.03 seconds\n",
      "Removed 95105 duplicates from this chunk\n",
      "Progress: 96,200,000/96,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 89,720,467 (93.26%)\n",
      "Processing chunk 963 with 100000 rows...\n",
      "Chunk 963 processed in 0.02 seconds\n",
      "Removed 95843 duplicates from this chunk\n",
      "Progress: 96,300,000/96,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 89,816,310 (93.27%)\n",
      "Processing chunk 964 with 100000 rows...\n",
      "Chunk 964 processed in 0.03 seconds\n",
      "Removed 95086 duplicates from this chunk\n",
      "Progress: 96,400,000/96,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 89,911,396 (93.27%)\n",
      "Processing chunk 965 with 100000 rows...\n",
      "Chunk 965 processed in 0.02 seconds\n",
      "Removed 95256 duplicates from this chunk\n",
      "Progress: 96,500,000/96,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 90,006,652 (93.27%)\n",
      "Processing chunk 966 with 100000 rows...\n",
      "Chunk 966 processed in 0.03 seconds\n",
      "Removed 94769 duplicates from this chunk\n",
      "Progress: 96,600,000/96,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 90,101,421 (93.27%)\n",
      "Processing chunk 967 with 100000 rows...\n",
      "Chunk 967 processed in 0.03 seconds\n",
      "Removed 95153 duplicates from this chunk\n",
      "Progress: 96,700,000/96,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 90,196,574 (93.27%)\n",
      "Processing chunk 968 with 100000 rows...\n",
      "Chunk 968 processed in 0.03 seconds\n",
      "Removed 94333 duplicates from this chunk\n",
      "Progress: 96,800,000/96,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 90,290,907 (93.28%)\n",
      "Processing chunk 969 with 100000 rows...\n",
      "Chunk 969 processed in 0.03 seconds\n",
      "Removed 94588 duplicates from this chunk\n",
      "Progress: 96,900,000/96,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 90,385,495 (93.28%)\n",
      "Processing chunk 970 with 100000 rows...\n",
      "Chunk 970 processed in 0.02 seconds\n",
      "Removed 95383 duplicates from this chunk\n",
      "Progress: 97,000,000/97,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 90,480,878 (93.28%)\n",
      "Processing chunk 971 with 100000 rows...\n",
      "Chunk 971 processed in 0.03 seconds\n",
      "Removed 94837 duplicates from this chunk\n",
      "Progress: 97,100,000/97,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 90,575,715 (93.28%)\n",
      "Processing chunk 972 with 100000 rows...\n",
      "Chunk 972 processed in 0.03 seconds\n",
      "Removed 94890 duplicates from this chunk\n",
      "Progress: 97,200,000/97,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 90,670,605 (93.28%)\n",
      "Processing chunk 973 with 100000 rows...\n",
      "Chunk 973 processed in 0.03 seconds\n",
      "Removed 94664 duplicates from this chunk\n",
      "Progress: 97,300,000/97,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 90,765,269 (93.28%)\n",
      "Processing chunk 974 with 100000 rows...\n",
      "Chunk 974 processed in 0.02 seconds\n",
      "Removed 95435 duplicates from this chunk\n",
      "Progress: 97,400,000/97,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 90,860,704 (93.29%)\n",
      "Processing chunk 975 with 100000 rows...\n",
      "Chunk 975 processed in 0.03 seconds\n",
      "Removed 94231 duplicates from this chunk\n",
      "Progress: 97,500,000/97,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 90,954,935 (93.29%)\n",
      "Processing chunk 976 with 100000 rows...\n",
      "Chunk 976 processed in 0.03 seconds\n",
      "Removed 94799 duplicates from this chunk\n",
      "Progress: 97,600,000/97,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 91,049,734 (93.29%)\n",
      "Processing chunk 977 with 100000 rows...\n",
      "Chunk 977 processed in 0.03 seconds\n",
      "Removed 94951 duplicates from this chunk\n",
      "Progress: 97,700,000/97,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 91,144,685 (93.29%)\n",
      "Processing chunk 978 with 100000 rows...\n",
      "Chunk 978 processed in 0.02 seconds\n",
      "Removed 95276 duplicates from this chunk\n",
      "Progress: 97,800,000/97,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 91,239,961 (93.29%)\n",
      "Processing chunk 979 with 100000 rows...\n",
      "Chunk 979 processed in 0.02 seconds\n",
      "Removed 95090 duplicates from this chunk\n",
      "Progress: 97,900,000/97,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 91,335,051 (93.29%)\n",
      "Processing chunk 980 with 100000 rows...\n",
      "Chunk 980 processed in 0.02 seconds\n",
      "Removed 94945 duplicates from this chunk\n",
      "Progress: 98,000,000/98,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 91,429,996 (93.30%)\n",
      "Processing chunk 981 with 100000 rows...\n",
      "Chunk 981 processed in 0.03 seconds\n",
      "Removed 94985 duplicates from this chunk\n",
      "Progress: 98,100,000/98,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 91,524,981 (93.30%)\n",
      "Processing chunk 982 with 100000 rows...\n",
      "Chunk 982 processed in 0.03 seconds\n",
      "Removed 94749 duplicates from this chunk\n",
      "Progress: 98,200,000/98,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 91,619,730 (93.30%)\n",
      "Processing chunk 983 with 100000 rows...\n",
      "Chunk 983 processed in 0.02 seconds\n",
      "Removed 95133 duplicates from this chunk\n",
      "Progress: 98,300,000/98,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 91,714,863 (93.30%)\n",
      "Processing chunk 984 with 100000 rows...\n",
      "Chunk 984 processed in 0.02 seconds\n",
      "Removed 95322 duplicates from this chunk\n",
      "Progress: 98,400,000/98,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 91,810,185 (93.30%)\n",
      "Processing chunk 985 with 100000 rows...\n",
      "Chunk 985 processed in 0.03 seconds\n",
      "Removed 95002 duplicates from this chunk\n",
      "Progress: 98,500,000/98,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 91,905,187 (93.30%)\n",
      "Processing chunk 986 with 100000 rows...\n",
      "Chunk 986 processed in 0.03 seconds\n",
      "Removed 94817 duplicates from this chunk\n",
      "Progress: 98,600,000/98,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 92,000,004 (93.31%)\n",
      "Processing chunk 987 with 100000 rows...\n",
      "Chunk 987 processed in 0.03 seconds\n",
      "Removed 94937 duplicates from this chunk\n",
      "Progress: 98,700,000/98,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 92,094,941 (93.31%)\n",
      "Processing chunk 988 with 100000 rows...\n",
      "Chunk 988 processed in 0.02 seconds\n",
      "Removed 95146 duplicates from this chunk\n",
      "Progress: 98,800,000/98,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 92,190,087 (93.31%)\n",
      "Processing chunk 989 with 100000 rows...\n",
      "Chunk 989 processed in 0.03 seconds\n",
      "Removed 94586 duplicates from this chunk\n",
      "Progress: 98,900,000/98,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 92,284,673 (93.31%)\n",
      "Processing chunk 990 with 100000 rows...\n",
      "Chunk 990 processed in 0.03 seconds\n",
      "Removed 95025 duplicates from this chunk\n",
      "Progress: 99,000,000/99,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 92,379,698 (93.31%)\n",
      "Processing chunk 991 with 100000 rows...\n",
      "Chunk 991 processed in 0.03 seconds\n",
      "Removed 94865 duplicates from this chunk\n",
      "Progress: 99,100,000/99,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 92,474,563 (93.31%)\n",
      "Processing chunk 992 with 100000 rows...\n",
      "Chunk 992 processed in 0.03 seconds\n",
      "Removed 94942 duplicates from this chunk\n",
      "Progress: 99,200,000/99,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 92,569,505 (93.32%)\n",
      "Processing chunk 993 with 100000 rows...\n",
      "Chunk 993 processed in 0.03 seconds\n",
      "Removed 95145 duplicates from this chunk\n",
      "Progress: 99,300,000/99,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 92,664,650 (93.32%)\n",
      "Processing chunk 994 with 100000 rows...\n",
      "Chunk 994 processed in 0.03 seconds\n",
      "Removed 94583 duplicates from this chunk\n",
      "Progress: 99,400,000/99,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 92,759,233 (93.32%)\n",
      "Processing chunk 995 with 100000 rows...\n",
      "Chunk 995 processed in 0.02 seconds\n",
      "Removed 95101 duplicates from this chunk\n",
      "Progress: 99,500,000/99,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 92,854,334 (93.32%)\n",
      "Processing chunk 996 with 100000 rows...\n",
      "Chunk 996 processed in 0.03 seconds\n",
      "Removed 94831 duplicates from this chunk\n",
      "Progress: 99,600,000/99,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 92,949,165 (93.32%)\n",
      "Processing chunk 997 with 100000 rows...\n",
      "Chunk 997 processed in 0.03 seconds\n",
      "Removed 94934 duplicates from this chunk\n",
      "Progress: 99,700,000/99,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 93,044,099 (93.32%)\n",
      "Processing chunk 998 with 100000 rows...\n",
      "Chunk 998 processed in 0.02 seconds\n",
      "Removed 95469 duplicates from this chunk\n",
      "Progress: 99,800,000/99,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 93,139,568 (93.33%)\n",
      "Processing chunk 999 with 100000 rows...\n",
      "Chunk 999 processed in 0.02 seconds\n",
      "Removed 95405 duplicates from this chunk\n",
      "Progress: 99,900,000/99,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 93,234,973 (93.33%)\n",
      "Processing chunk 1000 with 100000 rows...\n",
      "Chunk 1000 processed in 0.03 seconds\n",
      "Removed 95019 duplicates from this chunk\n",
      "Progress: 100,000,000/100,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 93,329,992 (93.33%)\n",
      "Processing chunk 1001 with 100000 rows...\n",
      "Chunk 1001 processed in 0.03 seconds\n",
      "Removed 94835 duplicates from this chunk\n",
      "Progress: 100,100,000/100,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 93,424,827 (93.33%)\n",
      "Processing chunk 1002 with 100000 rows...\n",
      "Chunk 1002 processed in 0.02 seconds\n",
      "Removed 95494 duplicates from this chunk\n",
      "Progress: 100,200,000/100,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 93,520,321 (93.33%)\n",
      "Processing chunk 1003 with 100000 rows...\n",
      "Chunk 1003 processed in 0.02 seconds\n",
      "Removed 95126 duplicates from this chunk\n",
      "Progress: 100,300,000/100,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 93,615,447 (93.34%)\n",
      "Processing chunk 1004 with 100000 rows...\n",
      "Chunk 1004 processed in 0.02 seconds\n",
      "Removed 95681 duplicates from this chunk\n",
      "Progress: 100,400,000/100,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 93,711,128 (93.34%)\n",
      "Processing chunk 1005 with 100000 rows...\n",
      "Chunk 1005 processed in 0.02 seconds\n",
      "Removed 95466 duplicates from this chunk\n",
      "Progress: 100,500,000/100,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 93,806,594 (93.34%)\n",
      "Processing chunk 1006 with 100000 rows...\n",
      "Chunk 1006 processed in 0.03 seconds\n",
      "Removed 94539 duplicates from this chunk\n",
      "Progress: 100,600,000/100,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 93,901,133 (93.34%)\n",
      "Processing chunk 1007 with 100000 rows...\n",
      "Chunk 1007 processed in 0.03 seconds\n",
      "Removed 95044 duplicates from this chunk\n",
      "Progress: 100,700,000/100,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 93,996,177 (93.34%)\n",
      "Processing chunk 1008 with 100000 rows...\n",
      "Chunk 1008 processed in 0.03 seconds\n",
      "Removed 94801 duplicates from this chunk\n",
      "Progress: 100,800,000/100,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 94,090,978 (93.34%)\n",
      "Processing chunk 1009 with 100000 rows...\n",
      "Chunk 1009 processed in 0.02 seconds\n",
      "Removed 95444 duplicates from this chunk\n",
      "Progress: 100,900,000/100,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 94,186,422 (93.35%)\n",
      "Processing chunk 1010 with 100000 rows...\n",
      "Chunk 1010 processed in 0.02 seconds\n",
      "Removed 95288 duplicates from this chunk\n",
      "Progress: 101,000,000/101,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 94,281,710 (93.35%)\n",
      "Processing chunk 1011 with 100000 rows...\n",
      "Chunk 1011 processed in 0.02 seconds\n",
      "Removed 95809 duplicates from this chunk\n",
      "Progress: 101,100,000/101,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 94,377,519 (93.35%)\n",
      "Processing chunk 1012 with 100000 rows...\n",
      "Chunk 1012 processed in 0.03 seconds\n",
      "Removed 94824 duplicates from this chunk\n",
      "Progress: 101,200,000/101,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 94,472,343 (93.35%)\n",
      "Processing chunk 1013 with 100000 rows...\n",
      "Chunk 1013 processed in 0.03 seconds\n",
      "Removed 94936 duplicates from this chunk\n",
      "Progress: 101,300,000/101,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 94,567,279 (93.35%)\n",
      "Processing chunk 1014 with 100000 rows...\n",
      "Chunk 1014 processed in 0.02 seconds\n",
      "Removed 95784 duplicates from this chunk\n",
      "Progress: 101,400,000/101,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 94,663,063 (93.36%)\n",
      "Processing chunk 1015 with 100000 rows...\n",
      "Chunk 1015 processed in 0.02 seconds\n",
      "Removed 95227 duplicates from this chunk\n",
      "Progress: 101,500,000/101,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 94,758,290 (93.36%)\n",
      "Processing chunk 1016 with 100000 rows...\n",
      "Chunk 1016 processed in 0.03 seconds\n",
      "Removed 95097 duplicates from this chunk\n",
      "Progress: 101,600,000/101,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 94,853,387 (93.36%)\n",
      "Processing chunk 1017 with 100000 rows...\n",
      "Chunk 1017 processed in 0.02 seconds\n",
      "Removed 95493 duplicates from this chunk\n",
      "Progress: 101,700,000/101,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 94,948,880 (93.36%)\n",
      "Processing chunk 1018 with 100000 rows...\n",
      "Chunk 1018 processed in 0.02 seconds\n",
      "Removed 95759 duplicates from this chunk\n",
      "Progress: 101,800,000/101,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 95,044,639 (93.36%)\n",
      "Processing chunk 1019 with 100000 rows...\n",
      "Chunk 1019 processed in 0.02 seconds\n",
      "Removed 95180 duplicates from this chunk\n",
      "Progress: 101,900,000/101,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 95,139,819 (93.37%)\n",
      "Processing chunk 1020 with 100000 rows...\n",
      "Chunk 1020 processed in 0.03 seconds\n",
      "Removed 94809 duplicates from this chunk\n",
      "Progress: 102,000,000/102,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 95,234,628 (93.37%)\n",
      "Processing chunk 1021 with 100000 rows...\n",
      "Chunk 1021 processed in 0.03 seconds\n",
      "Removed 94773 duplicates from this chunk\n",
      "Progress: 102,100,000/102,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 95,329,401 (93.37%)\n",
      "Processing chunk 1022 with 100000 rows...\n",
      "Chunk 1022 processed in 0.03 seconds\n",
      "Removed 94867 duplicates from this chunk\n",
      "Progress: 102,200,000/102,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 95,424,268 (93.37%)\n",
      "Processing chunk 1023 with 100000 rows...\n",
      "Chunk 1023 processed in 0.03 seconds\n",
      "Removed 94906 duplicates from this chunk\n",
      "Progress: 102,300,000/102,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 95,519,174 (93.37%)\n",
      "Processing chunk 1024 with 100000 rows...\n",
      "Chunk 1024 processed in 0.03 seconds\n",
      "Removed 95134 duplicates from this chunk\n",
      "Progress: 102,400,000/102,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 95,614,308 (93.37%)\n",
      "Processing chunk 1025 with 100000 rows...\n",
      "Chunk 1025 processed in 0.02 seconds\n",
      "Removed 95534 duplicates from this chunk\n",
      "Progress: 102,500,000/102,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 95,709,842 (93.38%)\n",
      "Processing chunk 1026 with 100000 rows...\n",
      "Chunk 1026 processed in 0.02 seconds\n",
      "Removed 95267 duplicates from this chunk\n",
      "Progress: 102,600,000/102,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 95,805,109 (93.38%)\n",
      "Processing chunk 1027 with 100000 rows...\n",
      "Chunk 1027 processed in 0.03 seconds\n",
      "Removed 95130 duplicates from this chunk\n",
      "Progress: 102,700,000/102,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 95,900,239 (93.38%)\n",
      "Processing chunk 1028 with 100000 rows...\n",
      "Chunk 1028 processed in 0.02 seconds\n",
      "Removed 95479 duplicates from this chunk\n",
      "Progress: 102,800,000/102,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 95,995,718 (93.38%)\n",
      "Processing chunk 1029 with 100000 rows...\n",
      "Chunk 1029 processed in 0.03 seconds\n",
      "Removed 95239 duplicates from this chunk\n",
      "Progress: 102,900,000/102,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 96,090,957 (93.38%)\n",
      "Processing chunk 1030 with 100000 rows...\n",
      "Chunk 1030 processed in 0.03 seconds\n",
      "Removed 94365 duplicates from this chunk\n",
      "Progress: 103,000,000/103,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 96,185,322 (93.38%)\n",
      "Processing chunk 1031 with 100000 rows...\n",
      "Chunk 1031 processed in 0.03 seconds\n",
      "Removed 94760 duplicates from this chunk\n",
      "Progress: 103,100,000/103,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 96,280,082 (93.39%)\n",
      "Processing chunk 1032 with 100000 rows...\n",
      "Chunk 1032 processed in 0.02 seconds\n",
      "Removed 95287 duplicates from this chunk\n",
      "Progress: 103,200,000/103,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 96,375,369 (93.39%)\n",
      "Processing chunk 1033 with 100000 rows...\n",
      "Chunk 1033 processed in 0.03 seconds\n",
      "Removed 95049 duplicates from this chunk\n",
      "Progress: 103,300,000/103,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 96,470,418 (93.39%)\n",
      "Processing chunk 1034 with 100000 rows...\n",
      "Chunk 1034 processed in 0.02 seconds\n",
      "Removed 95599 duplicates from this chunk\n",
      "Progress: 103,400,000/103,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 96,566,017 (93.39%)\n",
      "Processing chunk 1035 with 100000 rows...\n",
      "Chunk 1035 processed in 0.03 seconds\n",
      "Removed 94621 duplicates from this chunk\n",
      "Progress: 103,500,000/103,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 96,660,638 (93.39%)\n",
      "Processing chunk 1036 with 100000 rows...\n",
      "Chunk 1036 processed in 0.02 seconds\n",
      "Removed 95208 duplicates from this chunk\n",
      "Progress: 103,600,000/103,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 96,755,846 (93.39%)\n",
      "Processing chunk 1037 with 100000 rows...\n",
      "Chunk 1037 processed in 0.03 seconds\n",
      "Removed 94708 duplicates from this chunk\n",
      "Progress: 103,700,000/103,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 96,850,554 (93.39%)\n",
      "Processing chunk 1038 with 100000 rows...\n",
      "Chunk 1038 processed in 0.02 seconds\n",
      "Removed 95873 duplicates from this chunk\n",
      "Progress: 103,800,000/103,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 96,946,427 (93.40%)\n",
      "Processing chunk 1039 with 100000 rows...\n",
      "Chunk 1039 processed in 0.03 seconds\n",
      "Removed 94964 duplicates from this chunk\n",
      "Progress: 103,900,000/103,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 97,041,391 (93.40%)\n",
      "Processing chunk 1040 with 100000 rows...\n",
      "Chunk 1040 processed in 0.03 seconds\n",
      "Removed 94688 duplicates from this chunk\n",
      "Progress: 104,000,000/104,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 97,136,079 (93.40%)\n",
      "Processing chunk 1041 with 100000 rows...\n",
      "Chunk 1041 processed in 0.02 seconds\n",
      "Removed 95269 duplicates from this chunk\n",
      "Progress: 104,100,000/104,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 97,231,348 (93.40%)\n",
      "Processing chunk 1042 with 100000 rows...\n",
      "Chunk 1042 processed in 0.02 seconds\n",
      "Removed 95357 duplicates from this chunk\n",
      "Progress: 104,200,000/104,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 97,326,705 (93.40%)\n",
      "Processing chunk 1043 with 100000 rows...\n",
      "Chunk 1043 processed in 0.02 seconds\n",
      "Removed 95483 duplicates from this chunk\n",
      "Progress: 104,300,000/104,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 97,422,188 (93.41%)\n",
      "Processing chunk 1044 with 100000 rows...\n",
      "Chunk 1044 processed in 0.02 seconds\n",
      "Removed 95543 duplicates from this chunk\n",
      "Progress: 104,400,000/104,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 97,517,731 (93.41%)\n",
      "Processing chunk 1045 with 100000 rows...\n",
      "Chunk 1045 processed in 0.03 seconds\n",
      "Removed 94502 duplicates from this chunk\n",
      "Progress: 104,500,000/104,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 97,612,233 (93.41%)\n",
      "Processing chunk 1046 with 100000 rows...\n",
      "Chunk 1046 processed in 0.03 seconds\n",
      "Removed 94938 duplicates from this chunk\n",
      "Progress: 104,600,000/104,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 97,707,171 (93.41%)\n",
      "Processing chunk 1047 with 100000 rows...\n",
      "Chunk 1047 processed in 0.02 seconds\n",
      "Removed 95063 duplicates from this chunk\n",
      "Progress: 104,700,000/104,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 97,802,234 (93.41%)\n",
      "Processing chunk 1048 with 100000 rows...\n",
      "Chunk 1048 processed in 0.02 seconds\n",
      "Removed 95774 duplicates from this chunk\n",
      "Progress: 104,800,000/104,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 97,898,008 (93.41%)\n",
      "Processing chunk 1049 with 100000 rows...\n",
      "Chunk 1049 processed in 0.03 seconds\n",
      "Removed 94851 duplicates from this chunk\n",
      "Progress: 104,900,000/104,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 97,992,859 (93.42%)\n",
      "Processing chunk 1050 with 100000 rows...\n",
      "Chunk 1050 processed in 0.02 seconds\n",
      "Removed 95189 duplicates from this chunk\n",
      "Progress: 105,000,000/105,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 98,088,048 (93.42%)\n",
      "Processing chunk 1051 with 100000 rows...\n",
      "Chunk 1051 processed in 0.02 seconds\n",
      "Removed 95338 duplicates from this chunk\n",
      "Progress: 105,100,000/105,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 98,183,386 (93.42%)\n",
      "Processing chunk 1052 with 100000 rows...\n",
      "Chunk 1052 processed in 0.02 seconds\n",
      "Removed 95467 duplicates from this chunk\n",
      "Progress: 105,200,000/105,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 98,278,853 (93.42%)\n",
      "Processing chunk 1053 with 100000 rows...\n",
      "Chunk 1053 processed in 0.02 seconds\n",
      "Removed 95385 duplicates from this chunk\n",
      "Progress: 105,300,000/105,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 98,374,238 (93.42%)\n",
      "Processing chunk 1054 with 100000 rows...\n",
      "Chunk 1054 processed in 0.03 seconds\n",
      "Removed 93847 duplicates from this chunk\n",
      "Progress: 105,400,000/105,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 98,468,085 (93.42%)\n",
      "Processing chunk 1055 with 100000 rows...\n",
      "Chunk 1055 processed in 0.03 seconds\n",
      "Removed 94242 duplicates from this chunk\n",
      "Progress: 105,500,000/105,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 98,562,327 (93.42%)\n",
      "Processing chunk 1056 with 100000 rows...\n",
      "Chunk 1056 processed in 0.03 seconds\n",
      "Removed 94850 duplicates from this chunk\n",
      "Progress: 105,600,000/105,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 98,657,177 (93.43%)\n",
      "Processing chunk 1057 with 100000 rows...\n",
      "Chunk 1057 processed in 0.03 seconds\n",
      "Removed 94491 duplicates from this chunk\n",
      "Progress: 105,700,000/105,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 98,751,668 (93.43%)\n",
      "Processing chunk 1058 with 100000 rows...\n",
      "Chunk 1058 processed in 0.02 seconds\n",
      "Removed 95180 duplicates from this chunk\n",
      "Progress: 105,800,000/105,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 98,846,848 (93.43%)\n",
      "Processing chunk 1059 with 100000 rows...\n",
      "Chunk 1059 processed in 0.02 seconds\n",
      "Removed 95195 duplicates from this chunk\n",
      "Progress: 105,900,000/105,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 98,942,043 (93.43%)\n",
      "Processing chunk 1060 with 100000 rows...\n",
      "Chunk 1060 processed in 0.02 seconds\n",
      "Removed 95242 duplicates from this chunk\n",
      "Progress: 106,000,000/106,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 99,037,285 (93.43%)\n",
      "Processing chunk 1061 with 100000 rows...\n",
      "Chunk 1061 processed in 0.03 seconds\n",
      "Removed 94352 duplicates from this chunk\n",
      "Progress: 106,100,000/106,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 99,131,637 (93.43%)\n",
      "Processing chunk 1062 with 100000 rows...\n",
      "Chunk 1062 processed in 0.03 seconds\n",
      "Removed 94298 duplicates from this chunk\n",
      "Progress: 106,200,000/106,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 99,225,935 (93.43%)\n",
      "Processing chunk 1063 with 100000 rows...\n",
      "Chunk 1063 processed in 0.03 seconds\n",
      "Removed 94316 duplicates from this chunk\n",
      "Progress: 106,300,000/106,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 99,320,251 (93.43%)\n",
      "Processing chunk 1064 with 100000 rows...\n",
      "Chunk 1064 processed in 0.02 seconds\n",
      "Removed 95135 duplicates from this chunk\n",
      "Progress: 106,400,000/106,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 99,415,386 (93.44%)\n",
      "Processing chunk 1065 with 100000 rows...\n",
      "Chunk 1065 processed in 0.02 seconds\n",
      "Removed 95886 duplicates from this chunk\n",
      "Progress: 106,500,000/106,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 99,511,272 (93.44%)\n",
      "Processing chunk 1066 with 100000 rows...\n",
      "Chunk 1066 processed in 0.02 seconds\n",
      "Removed 95024 duplicates from this chunk\n",
      "Progress: 106,600,000/106,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 99,606,296 (93.44%)\n",
      "Processing chunk 1067 with 100000 rows...\n",
      "Chunk 1067 processed in 0.03 seconds\n",
      "Removed 94382 duplicates from this chunk\n",
      "Progress: 106,700,000/106,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 99,700,678 (93.44%)\n",
      "Processing chunk 1068 with 100000 rows...\n",
      "Chunk 1068 processed in 0.03 seconds\n",
      "Removed 94332 duplicates from this chunk\n",
      "Progress: 106,800,000/106,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 99,795,010 (93.44%)\n",
      "Processing chunk 1069 with 100000 rows...\n",
      "Chunk 1069 processed in 0.02 seconds\n",
      "Removed 95197 duplicates from this chunk\n",
      "Progress: 106,900,000/106,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 99,890,207 (93.44%)\n",
      "Processing chunk 1070 with 100000 rows...\n",
      "Chunk 1070 processed in 0.02 seconds\n",
      "Removed 94978 duplicates from this chunk\n",
      "Progress: 107,000,000/107,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 99,985,185 (93.44%)\n",
      "Processing chunk 1071 with 100000 rows...\n",
      "Chunk 1071 processed in 0.02 seconds\n",
      "Removed 95570 duplicates from this chunk\n",
      "Progress: 107,100,000/107,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 100,080,755 (93.45%)\n",
      "Processing chunk 1072 with 100000 rows...\n",
      "Chunk 1072 processed in 0.02 seconds\n",
      "Removed 95437 duplicates from this chunk\n",
      "Progress: 107,200,000/107,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 100,176,192 (93.45%)\n",
      "Processing chunk 1073 with 100000 rows...\n",
      "Chunk 1073 processed in 0.03 seconds\n",
      "Removed 94754 duplicates from this chunk\n",
      "Progress: 107,300,000/107,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 100,270,946 (93.45%)\n",
      "Processing chunk 1074 with 100000 rows...\n",
      "Chunk 1074 processed in 0.03 seconds\n",
      "Removed 94714 duplicates from this chunk\n",
      "Progress: 107,400,000/107,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 100,365,660 (93.45%)\n",
      "Processing chunk 1075 with 100000 rows...\n",
      "Chunk 1075 processed in 0.02 seconds\n",
      "Removed 95390 duplicates from this chunk\n",
      "Progress: 107,500,000/107,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 100,461,050 (93.45%)\n",
      "Processing chunk 1076 with 100000 rows...\n",
      "Chunk 1076 processed in 0.02 seconds\n",
      "Removed 95277 duplicates from this chunk\n",
      "Progress: 107,600,000/107,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 100,556,327 (93.45%)\n",
      "Processing chunk 1077 with 100000 rows...\n",
      "Chunk 1077 processed in 0.02 seconds\n",
      "Removed 95372 duplicates from this chunk\n",
      "Progress: 107,700,000/107,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 100,651,699 (93.46%)\n",
      "Processing chunk 1078 with 100000 rows...\n",
      "Chunk 1078 processed in 0.02 seconds\n",
      "Removed 95091 duplicates from this chunk\n",
      "Progress: 107,800,000/107,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 100,746,790 (93.46%)\n",
      "Processing chunk 1079 with 100000 rows...\n",
      "Chunk 1079 processed in 0.03 seconds\n",
      "Removed 94577 duplicates from this chunk\n",
      "Progress: 107,900,000/107,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 100,841,367 (93.46%)\n",
      "Processing chunk 1080 with 100000 rows...\n",
      "Chunk 1080 processed in 0.02 seconds\n",
      "Removed 95277 duplicates from this chunk\n",
      "Progress: 108,000,000/108,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 100,936,644 (93.46%)\n",
      "Processing chunk 1081 with 100000 rows...\n",
      "Chunk 1081 processed in 0.03 seconds\n",
      "Removed 94577 duplicates from this chunk\n",
      "Progress: 108,100,000/108,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 101,031,221 (93.46%)\n",
      "Processing chunk 1082 with 100000 rows...\n",
      "Chunk 1082 processed in 0.03 seconds\n",
      "Removed 94825 duplicates from this chunk\n",
      "Progress: 108,200,000/108,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 101,126,046 (93.46%)\n",
      "Processing chunk 1083 with 100000 rows...\n",
      "Chunk 1083 processed in 0.02 seconds\n",
      "Removed 95252 duplicates from this chunk\n",
      "Progress: 108,300,000/108,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 101,221,298 (93.46%)\n",
      "Processing chunk 1084 with 100000 rows...\n",
      "Chunk 1084 processed in 0.03 seconds\n",
      "Removed 94772 duplicates from this chunk\n",
      "Progress: 108,400,000/108,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 101,316,070 (93.47%)\n",
      "Processing chunk 1085 with 100000 rows...\n",
      "Chunk 1085 processed in 0.03 seconds\n",
      "Removed 94536 duplicates from this chunk\n",
      "Progress: 108,500,000/108,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 101,410,606 (93.47%)\n",
      "Processing chunk 1086 with 100000 rows...\n",
      "Chunk 1086 processed in 0.03 seconds\n",
      "Removed 94923 duplicates from this chunk\n",
      "Progress: 108,600,000/108,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 101,505,529 (93.47%)\n",
      "Processing chunk 1087 with 100000 rows...\n",
      "Chunk 1087 processed in 0.03 seconds\n",
      "Removed 94441 duplicates from this chunk\n",
      "Progress: 108,700,000/108,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 101,599,970 (93.47%)\n",
      "Processing chunk 1088 with 100000 rows...\n",
      "Chunk 1088 processed in 0.03 seconds\n",
      "Removed 94753 duplicates from this chunk\n",
      "Progress: 108,800,000/108,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 101,694,723 (93.47%)\n",
      "Processing chunk 1089 with 100000 rows...\n",
      "Chunk 1089 processed in 0.02 seconds\n",
      "Removed 95414 duplicates from this chunk\n",
      "Progress: 108,900,000/108,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 101,790,137 (93.47%)\n",
      "Processing chunk 1090 with 100000 rows...\n",
      "Chunk 1090 processed in 0.03 seconds\n",
      "Removed 95184 duplicates from this chunk\n",
      "Progress: 109,000,000/109,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 101,885,321 (93.47%)\n",
      "Processing chunk 1091 with 100000 rows...\n",
      "Chunk 1091 processed in 0.03 seconds\n",
      "Removed 94232 duplicates from this chunk\n",
      "Progress: 109,100,000/109,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 101,979,553 (93.47%)\n",
      "Processing chunk 1092 with 100000 rows...\n",
      "Chunk 1092 processed in 0.03 seconds\n",
      "Removed 94639 duplicates from this chunk\n",
      "Progress: 109,200,000/109,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 102,074,192 (93.47%)\n",
      "Processing chunk 1093 with 100000 rows...\n",
      "Chunk 1093 processed in 0.02 seconds\n",
      "Removed 95344 duplicates from this chunk\n",
      "Progress: 109,300,000/109,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 102,169,536 (93.48%)\n",
      "Processing chunk 1094 with 100000 rows...\n",
      "Chunk 1094 processed in 0.02 seconds\n",
      "Removed 95101 duplicates from this chunk\n",
      "Progress: 109,400,000/109,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 102,264,637 (93.48%)\n",
      "Processing chunk 1095 with 100000 rows...\n",
      "Chunk 1095 processed in 0.02 seconds\n",
      "Removed 95403 duplicates from this chunk\n",
      "Progress: 109,500,000/109,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 102,360,040 (93.48%)\n",
      "Processing chunk 1096 with 100000 rows...\n",
      "Chunk 1096 processed in 0.03 seconds\n",
      "Removed 94944 duplicates from this chunk\n",
      "Progress: 109,600,000/109,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 102,454,984 (93.48%)\n",
      "Processing chunk 1097 with 100000 rows...\n",
      "Chunk 1097 processed in 0.03 seconds\n",
      "Removed 94277 duplicates from this chunk\n",
      "Progress: 109,700,000/109,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 102,549,261 (93.48%)\n",
      "Processing chunk 1098 with 100000 rows...\n",
      "Chunk 1098 processed in 0.02 seconds\n",
      "Removed 95266 duplicates from this chunk\n",
      "Progress: 109,800,000/109,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 102,644,527 (93.48%)\n",
      "Processing chunk 1099 with 100000 rows...\n",
      "Chunk 1099 processed in 0.03 seconds\n",
      "Removed 94885 duplicates from this chunk\n",
      "Progress: 109,900,000/109,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 102,739,412 (93.48%)\n",
      "Processing chunk 1100 with 100000 rows...\n",
      "Chunk 1100 processed in 0.02 seconds\n",
      "Removed 95123 duplicates from this chunk\n",
      "Progress: 110,000,000/110,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 102,834,535 (93.49%)\n",
      "Processing chunk 1101 with 100000 rows...\n",
      "Chunk 1101 processed in 0.03 seconds\n",
      "Removed 94731 duplicates from this chunk\n",
      "Progress: 110,100,000/110,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 102,929,266 (93.49%)\n",
      "Processing chunk 1102 with 100000 rows...\n",
      "Chunk 1102 processed in 0.02 seconds\n",
      "Removed 95128 duplicates from this chunk\n",
      "Progress: 110,200,000/110,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 103,024,394 (93.49%)\n",
      "Processing chunk 1103 with 100000 rows...\n",
      "Chunk 1103 processed in 0.03 seconds\n",
      "Removed 94224 duplicates from this chunk\n",
      "Progress: 110,300,000/110,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 103,118,618 (93.49%)\n",
      "Processing chunk 1104 with 100000 rows...\n",
      "Chunk 1104 processed in 0.03 seconds\n",
      "Removed 94404 duplicates from this chunk\n",
      "Progress: 110,400,000/110,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 103,213,022 (93.49%)\n",
      "Processing chunk 1105 with 100000 rows...\n",
      "Chunk 1105 processed in 0.02 seconds\n",
      "Removed 95527 duplicates from this chunk\n",
      "Progress: 110,500,000/110,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 103,308,549 (93.49%)\n",
      "Processing chunk 1106 with 100000 rows...\n",
      "Chunk 1106 processed in 0.03 seconds\n",
      "Removed 94726 duplicates from this chunk\n",
      "Progress: 110,600,000/110,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 103,403,275 (93.49%)\n",
      "Processing chunk 1107 with 100000 rows...\n",
      "Chunk 1107 processed in 0.03 seconds\n",
      "Removed 94240 duplicates from this chunk\n",
      "Progress: 110,700,000/110,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 103,497,515 (93.49%)\n",
      "Processing chunk 1108 with 100000 rows...\n",
      "Chunk 1108 processed in 0.02 seconds\n",
      "Removed 95370 duplicates from this chunk\n",
      "Progress: 110,800,000/110,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 103,592,885 (93.50%)\n",
      "Processing chunk 1109 with 100000 rows...\n",
      "Chunk 1109 processed in 0.03 seconds\n",
      "Removed 94671 duplicates from this chunk\n",
      "Progress: 110,900,000/110,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 103,687,556 (93.50%)\n",
      "Processing chunk 1110 with 100000 rows...\n",
      "Chunk 1110 processed in 0.02 seconds\n",
      "Removed 95102 duplicates from this chunk\n",
      "Progress: 111,000,000/111,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 103,782,658 (93.50%)\n",
      "Processing chunk 1111 with 100000 rows...\n",
      "Chunk 1111 processed in 0.03 seconds\n",
      "Removed 94605 duplicates from this chunk\n",
      "Progress: 111,100,000/111,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 103,877,263 (93.50%)\n",
      "Processing chunk 1112 with 100000 rows...\n",
      "Chunk 1112 processed in 0.03 seconds\n",
      "Removed 94647 duplicates from this chunk\n",
      "Progress: 111,200,000/111,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 103,971,910 (93.50%)\n",
      "Processing chunk 1113 with 100000 rows...\n",
      "Chunk 1113 processed in 0.02 seconds\n",
      "Removed 94992 duplicates from this chunk\n",
      "Progress: 111,300,000/111,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 104,066,902 (93.50%)\n",
      "Processing chunk 1114 with 100000 rows...\n",
      "Chunk 1114 processed in 0.03 seconds\n",
      "Removed 94598 duplicates from this chunk\n",
      "Progress: 111,400,000/111,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 104,161,500 (93.50%)\n",
      "Processing chunk 1115 with 100000 rows...\n",
      "Chunk 1115 processed in 0.02 seconds\n",
      "Removed 95457 duplicates from this chunk\n",
      "Progress: 111,500,000/111,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 104,256,957 (93.50%)\n",
      "Processing chunk 1116 with 100000 rows...\n",
      "Chunk 1116 processed in 0.03 seconds\n",
      "Removed 94858 duplicates from this chunk\n",
      "Progress: 111,600,000/111,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 104,351,815 (93.51%)\n",
      "Processing chunk 1117 with 100000 rows...\n",
      "Chunk 1117 processed in 0.03 seconds\n",
      "Removed 94719 duplicates from this chunk\n",
      "Progress: 111,700,000/111,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 104,446,534 (93.51%)\n",
      "Processing chunk 1118 with 100000 rows...\n",
      "Chunk 1118 processed in 0.03 seconds\n",
      "Removed 94617 duplicates from this chunk\n",
      "Progress: 111,800,000/111,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 104,541,151 (93.51%)\n",
      "Processing chunk 1119 with 100000 rows...\n",
      "Chunk 1119 processed in 0.03 seconds\n",
      "Removed 94713 duplicates from this chunk\n",
      "Progress: 111,900,000/111,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 104,635,864 (93.51%)\n",
      "Processing chunk 1120 with 100000 rows...\n",
      "Chunk 1120 processed in 0.03 seconds\n",
      "Removed 94682 duplicates from this chunk\n",
      "Progress: 112,000,000/112,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 104,730,546 (93.51%)\n",
      "Processing chunk 1121 with 100000 rows...\n",
      "Chunk 1121 processed in 0.03 seconds\n",
      "Removed 93804 duplicates from this chunk\n",
      "Progress: 112,100,000/112,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 104,824,350 (93.51%)\n",
      "Processing chunk 1122 with 100000 rows...\n",
      "Chunk 1122 processed in 0.03 seconds\n",
      "Removed 93988 duplicates from this chunk\n",
      "Progress: 112,200,000/112,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 104,918,338 (93.51%)\n",
      "Processing chunk 1123 with 100000 rows...\n",
      "Chunk 1123 processed in 0.03 seconds\n",
      "Removed 94672 duplicates from this chunk\n",
      "Progress: 112,300,000/112,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 105,013,010 (93.51%)\n",
      "Processing chunk 1124 with 100000 rows...\n",
      "Chunk 1124 processed in 0.02 seconds\n",
      "Removed 95332 duplicates from this chunk\n",
      "Progress: 112,400,000/112,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 105,108,342 (93.51%)\n",
      "Processing chunk 1125 with 100000 rows...\n",
      "Chunk 1125 processed in 0.03 seconds\n",
      "Removed 94963 duplicates from this chunk\n",
      "Progress: 112,500,000/112,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 105,203,305 (93.51%)\n",
      "Processing chunk 1126 with 100000 rows...\n",
      "Chunk 1126 processed in 0.02 seconds\n",
      "Removed 95978 duplicates from this chunk\n",
      "Progress: 112,600,000/112,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 105,299,283 (93.52%)\n",
      "Processing chunk 1127 with 100000 rows...\n",
      "Chunk 1127 processed in 0.03 seconds\n",
      "Removed 94798 duplicates from this chunk\n",
      "Progress: 112,700,000/112,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 105,394,081 (93.52%)\n",
      "Processing chunk 1128 with 100000 rows...\n",
      "Chunk 1128 processed in 0.03 seconds\n",
      "Removed 94452 duplicates from this chunk\n",
      "Progress: 112,800,000/112,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 105,488,533 (93.52%)\n",
      "Processing chunk 1129 with 100000 rows...\n",
      "Chunk 1129 processed in 0.03 seconds\n",
      "Removed 94784 duplicates from this chunk\n",
      "Progress: 112,900,000/112,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 105,583,317 (93.52%)\n",
      "Processing chunk 1130 with 100000 rows...\n",
      "Chunk 1130 processed in 0.02 seconds\n",
      "Removed 95110 duplicates from this chunk\n",
      "Progress: 113,000,000/113,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 105,678,427 (93.52%)\n",
      "Processing chunk 1131 with 100000 rows...\n",
      "Chunk 1131 processed in 0.03 seconds\n",
      "Removed 94493 duplicates from this chunk\n",
      "Progress: 113,100,000/113,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 105,772,920 (93.52%)\n",
      "Processing chunk 1132 with 100000 rows...\n",
      "Chunk 1132 processed in 0.03 seconds\n",
      "Removed 94574 duplicates from this chunk\n",
      "Progress: 113,200,000/113,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 105,867,494 (93.52%)\n",
      "Processing chunk 1133 with 100000 rows...\n",
      "Chunk 1133 processed in 0.02 seconds\n",
      "Removed 95795 duplicates from this chunk\n",
      "Progress: 113,300,000/113,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 105,963,289 (93.52%)\n",
      "Processing chunk 1134 with 100000 rows...\n",
      "Chunk 1134 processed in 0.02 seconds\n",
      "Removed 94966 duplicates from this chunk\n",
      "Progress: 113,400,000/113,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 106,058,255 (93.53%)\n",
      "Processing chunk 1135 with 100000 rows...\n",
      "Chunk 1135 processed in 0.03 seconds\n",
      "Removed 94132 duplicates from this chunk\n",
      "Progress: 113,500,000/113,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 106,152,387 (93.53%)\n",
      "Processing chunk 1136 with 100000 rows...\n",
      "Chunk 1136 processed in 0.03 seconds\n",
      "Removed 94503 duplicates from this chunk\n",
      "Progress: 113,600,000/113,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 106,246,890 (93.53%)\n",
      "Processing chunk 1137 with 100000 rows...\n",
      "Chunk 1137 processed in 0.03 seconds\n",
      "Removed 95005 duplicates from this chunk\n",
      "Progress: 113,700,000/113,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 106,341,895 (93.53%)\n",
      "Processing chunk 1138 with 100000 rows...\n",
      "Chunk 1138 processed in 0.03 seconds\n",
      "Removed 94880 duplicates from this chunk\n",
      "Progress: 113,800,000/113,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 106,436,775 (93.53%)\n",
      "Processing chunk 1139 with 100000 rows...\n",
      "Chunk 1139 processed in 0.03 seconds\n",
      "Removed 93707 duplicates from this chunk\n",
      "Progress: 113,900,000/113,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 106,530,482 (93.53%)\n",
      "Processing chunk 1140 with 100000 rows...\n",
      "Chunk 1140 processed in 0.03 seconds\n",
      "Removed 94455 duplicates from this chunk\n",
      "Progress: 114,000,000/114,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 106,624,937 (93.53%)\n",
      "Processing chunk 1141 with 100000 rows...\n",
      "Chunk 1141 processed in 0.03 seconds\n",
      "Removed 94965 duplicates from this chunk\n",
      "Progress: 114,100,000/114,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 106,719,902 (93.53%)\n",
      "Processing chunk 1142 with 100000 rows...\n",
      "Chunk 1142 processed in 0.02 seconds\n",
      "Removed 95254 duplicates from this chunk\n",
      "Progress: 114,200,000/114,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 106,815,156 (93.53%)\n",
      "Processing chunk 1143 with 100000 rows...\n",
      "Chunk 1143 processed in 0.03 seconds\n",
      "Removed 94259 duplicates from this chunk\n",
      "Progress: 114,300,000/114,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 106,909,415 (93.53%)\n",
      "Processing chunk 1144 with 100000 rows...\n",
      "Chunk 1144 processed in 0.03 seconds\n",
      "Removed 94950 duplicates from this chunk\n",
      "Progress: 114,400,000/114,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 107,004,365 (93.54%)\n",
      "Processing chunk 1145 with 100000 rows...\n",
      "Chunk 1145 processed in 0.03 seconds\n",
      "Removed 94131 duplicates from this chunk\n",
      "Progress: 114,500,000/114,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 107,098,496 (93.54%)\n",
      "Processing chunk 1146 with 100000 rows...\n",
      "Chunk 1146 processed in 0.02 seconds\n",
      "Removed 95215 duplicates from this chunk\n",
      "Progress: 114,600,000/114,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 107,193,711 (93.54%)\n",
      "Processing chunk 1147 with 100000 rows...\n",
      "Chunk 1147 processed in 0.03 seconds\n",
      "Removed 94517 duplicates from this chunk\n",
      "Progress: 114,700,000/114,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 107,288,228 (93.54%)\n",
      "Processing chunk 1148 with 100000 rows...\n",
      "Chunk 1148 processed in 0.03 seconds\n",
      "Removed 94401 duplicates from this chunk\n",
      "Progress: 114,800,000/114,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 107,382,629 (93.54%)\n",
      "Processing chunk 1149 with 100000 rows...\n",
      "Chunk 1149 processed in 0.03 seconds\n",
      "Removed 94986 duplicates from this chunk\n",
      "Progress: 114,900,000/114,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 107,477,615 (93.54%)\n",
      "Processing chunk 1150 with 100000 rows...\n",
      "Chunk 1150 processed in 0.02 seconds\n",
      "Removed 95454 duplicates from this chunk\n",
      "Progress: 115,000,000/115,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 107,573,069 (93.54%)\n",
      "Processing chunk 1151 with 100000 rows...\n",
      "Chunk 1151 processed in 0.03 seconds\n",
      "Removed 94200 duplicates from this chunk\n",
      "Progress: 115,100,000/115,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 107,667,269 (93.54%)\n",
      "Processing chunk 1152 with 100000 rows...\n",
      "Chunk 1152 processed in 0.03 seconds\n",
      "Removed 94457 duplicates from this chunk\n",
      "Progress: 115,200,000/115,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 107,761,726 (93.54%)\n",
      "Processing chunk 1153 with 100000 rows...\n",
      "Chunk 1153 processed in 0.02 seconds\n",
      "Removed 95588 duplicates from this chunk\n",
      "Progress: 115,300,000/115,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 107,857,314 (93.54%)\n",
      "Processing chunk 1154 with 100000 rows...\n",
      "Chunk 1154 processed in 0.03 seconds\n",
      "Removed 94867 duplicates from this chunk\n",
      "Progress: 115,400,000/115,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 107,952,181 (93.55%)\n",
      "Processing chunk 1155 with 100000 rows...\n",
      "Chunk 1155 processed in 0.03 seconds\n",
      "Removed 94655 duplicates from this chunk\n",
      "Progress: 115,500,000/115,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 108,046,836 (93.55%)\n",
      "Processing chunk 1156 with 100000 rows...\n",
      "Chunk 1156 processed in 0.03 seconds\n",
      "Removed 94626 duplicates from this chunk\n",
      "Progress: 115,600,000/115,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 108,141,462 (93.55%)\n",
      "Processing chunk 1157 with 100000 rows...\n",
      "Chunk 1157 processed in 0.03 seconds\n",
      "Removed 94829 duplicates from this chunk\n",
      "Progress: 115,700,000/115,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 108,236,291 (93.55%)\n",
      "Processing chunk 1158 with 100000 rows...\n",
      "Chunk 1158 processed in 0.03 seconds\n",
      "Removed 94154 duplicates from this chunk\n",
      "Progress: 115,800,000/115,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 108,330,445 (93.55%)\n",
      "Processing chunk 1159 with 100000 rows...\n",
      "Chunk 1159 processed in 0.02 seconds\n",
      "Removed 95679 duplicates from this chunk\n",
      "Progress: 115,900,000/115,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 108,426,124 (93.55%)\n",
      "Processing chunk 1160 with 100000 rows...\n",
      "Chunk 1160 processed in 0.02 seconds\n",
      "Removed 95242 duplicates from this chunk\n",
      "Progress: 116,000,000/116,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 108,521,366 (93.55%)\n",
      "Processing chunk 1161 with 100000 rows...\n",
      "Chunk 1161 processed in 0.03 seconds\n",
      "Removed 94668 duplicates from this chunk\n",
      "Progress: 116,100,000/116,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 108,616,034 (93.55%)\n",
      "Processing chunk 1162 with 100000 rows...\n",
      "Chunk 1162 processed in 0.03 seconds\n",
      "Removed 94559 duplicates from this chunk\n",
      "Progress: 116,200,000/116,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 108,710,593 (93.55%)\n",
      "Processing chunk 1163 with 100000 rows...\n",
      "Chunk 1163 processed in 0.03 seconds\n",
      "Removed 93960 duplicates from this chunk\n",
      "Progress: 116,300,000/116,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 108,804,553 (93.56%)\n",
      "Processing chunk 1164 with 100000 rows...\n",
      "Chunk 1164 processed in 0.03 seconds\n",
      "Removed 94907 duplicates from this chunk\n",
      "Progress: 116,400,000/116,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 108,899,460 (93.56%)\n",
      "Processing chunk 1165 with 100000 rows...\n",
      "Chunk 1165 processed in 0.03 seconds\n",
      "Removed 94354 duplicates from this chunk\n",
      "Progress: 116,500,000/116,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 108,993,814 (93.56%)\n",
      "Processing chunk 1166 with 100000 rows...\n",
      "Chunk 1166 processed in 0.03 seconds\n",
      "Removed 94170 duplicates from this chunk\n",
      "Progress: 116,600,000/116,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 109,087,984 (93.56%)\n",
      "Processing chunk 1167 with 100000 rows...\n",
      "Chunk 1167 processed in 0.03 seconds\n",
      "Removed 94960 duplicates from this chunk\n",
      "Progress: 116,700,000/116,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 109,182,944 (93.56%)\n",
      "Processing chunk 1168 with 100000 rows...\n",
      "Chunk 1168 processed in 0.02 seconds\n",
      "Removed 95357 duplicates from this chunk\n",
      "Progress: 116,800,000/116,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 109,278,301 (93.56%)\n",
      "Processing chunk 1169 with 100000 rows...\n",
      "Chunk 1169 processed in 0.02 seconds\n",
      "Removed 94964 duplicates from this chunk\n",
      "Progress: 116,900,000/116,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 109,373,265 (93.56%)\n",
      "Processing chunk 1170 with 100000 rows...\n",
      "Chunk 1170 processed in 0.03 seconds\n",
      "Removed 94168 duplicates from this chunk\n",
      "Progress: 117,000,000/117,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 109,467,433 (93.56%)\n",
      "Processing chunk 1171 with 100000 rows...\n",
      "Chunk 1171 processed in 0.03 seconds\n",
      "Removed 93877 duplicates from this chunk\n",
      "Progress: 117,100,000/117,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 109,561,310 (93.56%)\n",
      "Processing chunk 1172 with 100000 rows...\n",
      "Chunk 1172 processed in 0.03 seconds\n",
      "Removed 94203 duplicates from this chunk\n",
      "Progress: 117,200,000/117,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 109,655,513 (93.56%)\n",
      "Processing chunk 1173 with 100000 rows...\n",
      "Chunk 1173 processed in 0.03 seconds\n",
      "Removed 94078 duplicates from this chunk\n",
      "Progress: 117,300,000/117,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 109,749,591 (93.56%)\n",
      "Processing chunk 1174 with 100000 rows...\n",
      "Chunk 1174 processed in 0.02 seconds\n",
      "Removed 95486 duplicates from this chunk\n",
      "Progress: 117,400,000/117,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 109,845,077 (93.56%)\n",
      "Processing chunk 1175 with 100000 rows...\n",
      "Chunk 1175 processed in 0.03 seconds\n",
      "Removed 94897 duplicates from this chunk\n",
      "Progress: 117,500,000/117,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 109,939,974 (93.57%)\n",
      "Processing chunk 1176 with 100000 rows...\n",
      "Chunk 1176 processed in 0.03 seconds\n",
      "Removed 94695 duplicates from this chunk\n",
      "Progress: 117,600,000/117,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 110,034,669 (93.57%)\n",
      "Processing chunk 1177 with 100000 rows...\n",
      "Chunk 1177 processed in 0.03 seconds\n",
      "Removed 94048 duplicates from this chunk\n",
      "Progress: 117,700,000/117,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 110,128,717 (93.57%)\n",
      "Processing chunk 1178 with 100000 rows...\n",
      "Chunk 1178 processed in 0.03 seconds\n",
      "Removed 94242 duplicates from this chunk\n",
      "Progress: 117,800,000/117,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 110,222,959 (93.57%)\n",
      "Processing chunk 1179 with 100000 rows...\n",
      "Chunk 1179 processed in 0.03 seconds\n",
      "Removed 94090 duplicates from this chunk\n",
      "Progress: 117,900,000/117,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 110,317,049 (93.57%)\n",
      "Processing chunk 1180 with 100000 rows...\n",
      "Chunk 1180 processed in 0.03 seconds\n",
      "Removed 94111 duplicates from this chunk\n",
      "Progress: 118,000,000/118,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 110,411,160 (93.57%)\n",
      "Processing chunk 1181 with 100000 rows...\n",
      "Chunk 1181 processed in 0.03 seconds\n",
      "Removed 94477 duplicates from this chunk\n",
      "Progress: 118,100,000/118,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 110,505,637 (93.57%)\n",
      "Processing chunk 1182 with 100000 rows...\n",
      "Chunk 1182 processed in 0.03 seconds\n",
      "Removed 94520 duplicates from this chunk\n",
      "Progress: 118,200,000/118,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 110,600,157 (93.57%)\n",
      "Processing chunk 1183 with 100000 rows...\n",
      "Chunk 1183 processed in 0.02 seconds\n",
      "Removed 95286 duplicates from this chunk\n",
      "Progress: 118,300,000/118,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 110,695,443 (93.57%)\n",
      "Processing chunk 1184 with 100000 rows...\n",
      "Chunk 1184 processed in 0.02 seconds\n",
      "Removed 95496 duplicates from this chunk\n",
      "Progress: 118,400,000/118,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 110,790,939 (93.57%)\n",
      "Processing chunk 1185 with 100000 rows...\n",
      "Chunk 1185 processed in 0.03 seconds\n",
      "Removed 94104 duplicates from this chunk\n",
      "Progress: 118,500,000/118,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 110,885,043 (93.57%)\n",
      "Processing chunk 1186 with 100000 rows...\n",
      "Chunk 1186 processed in 0.03 seconds\n",
      "Removed 93886 duplicates from this chunk\n",
      "Progress: 118,600,000/118,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 110,978,929 (93.57%)\n",
      "Processing chunk 1187 with 100000 rows...\n",
      "Chunk 1187 processed in 0.02 seconds\n",
      "Removed 95564 duplicates from this chunk\n",
      "Progress: 118,700,000/118,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 111,074,493 (93.58%)\n",
      "Processing chunk 1188 with 100000 rows...\n",
      "Chunk 1188 processed in 0.03 seconds\n",
      "Removed 94093 duplicates from this chunk\n",
      "Progress: 118,800,000/118,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 111,168,586 (93.58%)\n",
      "Processing chunk 1189 with 100000 rows...\n",
      "Chunk 1189 processed in 0.03 seconds\n",
      "Removed 94758 duplicates from this chunk\n",
      "Progress: 118,900,000/118,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 111,263,344 (93.58%)\n",
      "Processing chunk 1190 with 100000 rows...\n",
      "Chunk 1190 processed in 0.03 seconds\n",
      "Removed 94672 duplicates from this chunk\n",
      "Progress: 119,000,000/119,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 111,358,016 (93.58%)\n",
      "Processing chunk 1191 with 100000 rows...\n",
      "Chunk 1191 processed in 0.02 seconds\n",
      "Removed 95259 duplicates from this chunk\n",
      "Progress: 119,100,000/119,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 111,453,275 (93.58%)\n",
      "Processing chunk 1192 with 100000 rows...\n",
      "Chunk 1192 processed in 0.03 seconds\n",
      "Removed 94038 duplicates from this chunk\n",
      "Progress: 119,200,000/119,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 111,547,313 (93.58%)\n",
      "Processing chunk 1193 with 100000 rows...\n",
      "Chunk 1193 processed in 0.03 seconds\n",
      "Removed 94583 duplicates from this chunk\n",
      "Progress: 119,300,000/119,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 111,641,896 (93.58%)\n",
      "Processing chunk 1194 with 100000 rows...\n",
      "Chunk 1194 processed in 0.03 seconds\n",
      "Removed 94638 duplicates from this chunk\n",
      "Progress: 119,400,000/119,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 111,736,534 (93.58%)\n",
      "Processing chunk 1195 with 100000 rows...\n",
      "Chunk 1195 processed in 0.02 seconds\n",
      "Removed 95375 duplicates from this chunk\n",
      "Progress: 119,500,000/119,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 111,831,909 (93.58%)\n",
      "Processing chunk 1196 with 100000 rows...\n",
      "Chunk 1196 processed in 0.03 seconds\n",
      "Removed 95096 duplicates from this chunk\n",
      "Progress: 119,600,000/119,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 111,927,005 (93.58%)\n",
      "Processing chunk 1197 with 100000 rows...\n",
      "Chunk 1197 processed in 0.03 seconds\n",
      "Removed 94818 duplicates from this chunk\n",
      "Progress: 119,700,000/119,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 112,021,823 (93.59%)\n",
      "Processing chunk 1198 with 100000 rows...\n",
      "Chunk 1198 processed in 0.03 seconds\n",
      "Removed 94705 duplicates from this chunk\n",
      "Progress: 119,800,000/119,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 112,116,528 (93.59%)\n",
      "Processing chunk 1199 with 100000 rows...\n",
      "Chunk 1199 processed in 0.03 seconds\n",
      "Removed 94611 duplicates from this chunk\n",
      "Progress: 119,900,000/119,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 112,211,139 (93.59%)\n",
      "Processing chunk 1200 with 100000 rows...\n",
      "Chunk 1200 processed in 0.03 seconds\n",
      "Removed 94179 duplicates from this chunk\n",
      "Progress: 120,000,000/120,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 112,305,318 (93.59%)\n",
      "Processing chunk 1201 with 100000 rows...\n",
      "Chunk 1201 processed in 0.02 seconds\n",
      "Removed 95307 duplicates from this chunk\n",
      "Progress: 120,100,000/120,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 112,400,625 (93.59%)\n",
      "Processing chunk 1202 with 100000 rows...\n",
      "Chunk 1202 processed in 0.02 seconds\n",
      "Removed 95014 duplicates from this chunk\n",
      "Progress: 120,200,000/120,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 112,495,639 (93.59%)\n",
      "Processing chunk 1203 with 100000 rows...\n",
      "Chunk 1203 processed in 0.03 seconds\n",
      "Removed 94940 duplicates from this chunk\n",
      "Progress: 120,300,000/120,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 112,590,579 (93.59%)\n",
      "Processing chunk 1204 with 100000 rows...\n",
      "Chunk 1204 processed in 0.03 seconds\n",
      "Removed 94705 duplicates from this chunk\n",
      "Progress: 120,400,000/120,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 112,685,284 (93.59%)\n",
      "Processing chunk 1205 with 100000 rows...\n",
      "Chunk 1205 processed in 0.03 seconds\n",
      "Removed 94719 duplicates from this chunk\n",
      "Progress: 120,500,000/120,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 112,780,003 (93.59%)\n",
      "Processing chunk 1206 with 100000 rows...\n",
      "Chunk 1206 processed in 0.03 seconds\n",
      "Removed 94483 duplicates from this chunk\n",
      "Progress: 120,600,000/120,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 112,874,486 (93.59%)\n",
      "Processing chunk 1207 with 100000 rows...\n",
      "Chunk 1207 processed in 0.03 seconds\n",
      "Removed 93646 duplicates from this chunk\n",
      "Progress: 120,700,000/120,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 112,968,132 (93.59%)\n",
      "Processing chunk 1208 with 100000 rows...\n",
      "Chunk 1208 processed in 0.02 seconds\n",
      "Removed 94989 duplicates from this chunk\n",
      "Progress: 120,800,000/120,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 113,063,121 (93.60%)\n",
      "Processing chunk 1209 with 100000 rows...\n",
      "Chunk 1209 processed in 0.03 seconds\n",
      "Removed 94899 duplicates from this chunk\n",
      "Progress: 120,900,000/120,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 113,158,020 (93.60%)\n",
      "Processing chunk 1210 with 100000 rows...\n",
      "Chunk 1210 processed in 0.03 seconds\n",
      "Removed 95058 duplicates from this chunk\n",
      "Progress: 121,000,000/121,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 113,253,078 (93.60%)\n",
      "Processing chunk 1211 with 100000 rows...\n",
      "Chunk 1211 processed in 0.03 seconds\n",
      "Removed 94590 duplicates from this chunk\n",
      "Progress: 121,100,000/121,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 113,347,668 (93.60%)\n",
      "Processing chunk 1212 with 100000 rows...\n",
      "Chunk 1212 processed in 0.03 seconds\n",
      "Removed 94345 duplicates from this chunk\n",
      "Progress: 121,200,000/121,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 113,442,013 (93.60%)\n",
      "Processing chunk 1213 with 100000 rows...\n",
      "Chunk 1213 processed in 0.03 seconds\n",
      "Removed 94245 duplicates from this chunk\n",
      "Progress: 121,300,000/121,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 113,536,258 (93.60%)\n",
      "Processing chunk 1214 with 100000 rows...\n",
      "Chunk 1214 processed in 0.02 seconds\n",
      "Removed 95383 duplicates from this chunk\n",
      "Progress: 121,400,000/121,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 113,631,641 (93.60%)\n",
      "Processing chunk 1215 with 100000 rows...\n",
      "Chunk 1215 processed in 0.03 seconds\n",
      "Removed 94678 duplicates from this chunk\n",
      "Progress: 121,500,000/121,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 113,726,319 (93.60%)\n",
      "Processing chunk 1216 with 100000 rows...\n",
      "Chunk 1216 processed in 0.02 seconds\n",
      "Removed 95190 duplicates from this chunk\n",
      "Progress: 121,600,000/121,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 113,821,509 (93.60%)\n",
      "Processing chunk 1217 with 100000 rows...\n",
      "Chunk 1217 processed in 0.03 seconds\n",
      "Removed 94425 duplicates from this chunk\n",
      "Progress: 121,700,000/121,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 113,915,934 (93.60%)\n",
      "Processing chunk 1218 with 100000 rows...\n",
      "Chunk 1218 processed in 0.03 seconds\n",
      "Removed 94455 duplicates from this chunk\n",
      "Progress: 121,800,000/121,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 114,010,389 (93.60%)\n",
      "Processing chunk 1219 with 100000 rows...\n",
      "Chunk 1219 processed in 0.03 seconds\n",
      "Removed 94054 duplicates from this chunk\n",
      "Progress: 121,900,000/121,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 114,104,443 (93.60%)\n",
      "Processing chunk 1220 with 100000 rows...\n",
      "Chunk 1220 processed in 0.02 seconds\n",
      "Removed 95064 duplicates from this chunk\n",
      "Progress: 122,000,000/122,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 114,199,507 (93.61%)\n",
      "Processing chunk 1221 with 100000 rows...\n",
      "Chunk 1221 processed in 0.03 seconds\n",
      "Removed 94646 duplicates from this chunk\n",
      "Progress: 122,100,000/122,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 114,294,153 (93.61%)\n",
      "Processing chunk 1222 with 100000 rows...\n",
      "Chunk 1222 processed in 0.02 seconds\n",
      "Removed 95652 duplicates from this chunk\n",
      "Progress: 122,200,000/122,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 114,389,805 (93.61%)\n",
      "Processing chunk 1223 with 100000 rows...\n",
      "Chunk 1223 processed in 0.03 seconds\n",
      "Removed 94628 duplicates from this chunk\n",
      "Progress: 122,300,000/122,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 114,484,433 (93.61%)\n",
      "Processing chunk 1224 with 100000 rows...\n",
      "Chunk 1224 processed in 0.03 seconds\n",
      "Removed 94009 duplicates from this chunk\n",
      "Progress: 122,400,000/122,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 114,578,442 (93.61%)\n",
      "Processing chunk 1225 with 100000 rows...\n",
      "Chunk 1225 processed in 0.02 seconds\n",
      "Removed 95578 duplicates from this chunk\n",
      "Progress: 122,500,000/122,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 114,674,020 (93.61%)\n",
      "Processing chunk 1226 with 100000 rows...\n",
      "Chunk 1226 processed in 0.03 seconds\n",
      "Removed 93883 duplicates from this chunk\n",
      "Progress: 122,600,000/122,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 114,767,903 (93.61%)\n",
      "Processing chunk 1227 with 100000 rows...\n",
      "Chunk 1227 processed in 0.02 seconds\n",
      "Removed 95128 duplicates from this chunk\n",
      "Progress: 122,700,000/122,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 114,863,031 (93.61%)\n",
      "Processing chunk 1228 with 100000 rows...\n",
      "Chunk 1228 processed in 0.02 seconds\n",
      "Removed 95279 duplicates from this chunk\n",
      "Progress: 122,800,000/122,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 114,958,310 (93.61%)\n",
      "Processing chunk 1229 with 100000 rows...\n",
      "Chunk 1229 processed in 0.02 seconds\n",
      "Removed 95563 duplicates from this chunk\n",
      "Progress: 122,900,000/122,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 115,053,873 (93.62%)\n",
      "Processing chunk 1230 with 100000 rows...\n",
      "Chunk 1230 processed in 0.03 seconds\n",
      "Removed 94302 duplicates from this chunk\n",
      "Progress: 123,000,000/123,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 115,148,175 (93.62%)\n",
      "Processing chunk 1231 with 100000 rows...\n",
      "Chunk 1231 processed in 0.03 seconds\n",
      "Removed 94350 duplicates from this chunk\n",
      "Progress: 123,100,000/123,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 115,242,525 (93.62%)\n",
      "Processing chunk 1232 with 100000 rows...\n",
      "Chunk 1232 processed in 0.02 seconds\n",
      "Removed 95617 duplicates from this chunk\n",
      "Progress: 123,200,000/123,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 115,338,142 (93.62%)\n",
      "Processing chunk 1233 with 100000 rows...\n",
      "Chunk 1233 processed in 0.02 seconds\n",
      "Removed 95648 duplicates from this chunk\n",
      "Progress: 123,300,000/123,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 115,433,790 (93.62%)\n",
      "Processing chunk 1234 with 100000 rows...\n",
      "Chunk 1234 processed in 0.03 seconds\n",
      "Removed 94501 duplicates from this chunk\n",
      "Progress: 123,400,000/123,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 115,528,291 (93.62%)\n",
      "Processing chunk 1235 with 100000 rows...\n",
      "Chunk 1235 processed in 0.03 seconds\n",
      "Removed 94609 duplicates from this chunk\n",
      "Progress: 123,500,000/123,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 115,622,900 (93.62%)\n",
      "Processing chunk 1236 with 100000 rows...\n",
      "Chunk 1236 processed in 0.03 seconds\n",
      "Removed 94536 duplicates from this chunk\n",
      "Progress: 123,600,000/123,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 115,717,436 (93.62%)\n",
      "Processing chunk 1237 with 100000 rows...\n",
      "Chunk 1237 processed in 0.02 seconds\n",
      "Removed 95722 duplicates from this chunk\n",
      "Progress: 123,700,000/123,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 115,813,158 (93.62%)\n",
      "Processing chunk 1238 with 100000 rows...\n",
      "Chunk 1238 processed in 0.03 seconds\n",
      "Removed 94536 duplicates from this chunk\n",
      "Progress: 123,800,000/123,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 115,907,694 (93.62%)\n",
      "Processing chunk 1239 with 100000 rows...\n",
      "Chunk 1239 processed in 0.02 seconds\n",
      "Removed 95419 duplicates from this chunk\n",
      "Progress: 123,900,000/123,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 116,003,113 (93.63%)\n",
      "Processing chunk 1240 with 100000 rows...\n",
      "Chunk 1240 processed in 0.03 seconds\n",
      "Removed 94185 duplicates from this chunk\n",
      "Progress: 124,000,000/124,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 116,097,298 (93.63%)\n",
      "Processing chunk 1241 with 100000 rows...\n",
      "Chunk 1241 processed in 0.03 seconds\n",
      "Removed 94809 duplicates from this chunk\n",
      "Progress: 124,100,000/124,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 116,192,107 (93.63%)\n",
      "Processing chunk 1242 with 100000 rows...\n",
      "Chunk 1242 processed in 0.02 seconds\n",
      "Removed 96033 duplicates from this chunk\n",
      "Progress: 124,200,000/124,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 116,288,140 (93.63%)\n",
      "Processing chunk 1243 with 100000 rows...\n",
      "Chunk 1243 processed in 0.02 seconds\n",
      "Removed 95301 duplicates from this chunk\n",
      "Progress: 124,300,000/124,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 116,383,441 (93.63%)\n",
      "Processing chunk 1244 with 100000 rows...\n",
      "Chunk 1244 processed in 0.03 seconds\n",
      "Removed 94775 duplicates from this chunk\n",
      "Progress: 124,400,000/124,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 116,478,216 (93.63%)\n",
      "Processing chunk 1245 with 100000 rows...\n",
      "Chunk 1245 processed in 0.03 seconds\n",
      "Removed 94058 duplicates from this chunk\n",
      "Progress: 124,500,000/124,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 116,572,274 (93.63%)\n",
      "Processing chunk 1246 with 100000 rows...\n",
      "Chunk 1246 processed in 0.03 seconds\n",
      "Removed 94005 duplicates from this chunk\n",
      "Progress: 124,600,000/124,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 116,666,279 (93.63%)\n",
      "Processing chunk 1247 with 100000 rows...\n",
      "Chunk 1247 processed in 0.02 seconds\n",
      "Removed 95181 duplicates from this chunk\n",
      "Progress: 124,700,000/124,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 116,761,460 (93.63%)\n",
      "Processing chunk 1248 with 100000 rows...\n",
      "Chunk 1248 processed in 0.02 seconds\n",
      "Removed 95524 duplicates from this chunk\n",
      "Progress: 124,800,000/124,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 116,856,984 (93.64%)\n",
      "Processing chunk 1249 with 100000 rows...\n",
      "Chunk 1249 processed in 0.02 seconds\n",
      "Removed 95358 duplicates from this chunk\n",
      "Progress: 124,900,000/124,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 116,952,342 (93.64%)\n",
      "Processing chunk 1250 with 100000 rows...\n",
      "Chunk 1250 processed in 0.02 seconds\n",
      "Removed 95764 duplicates from this chunk\n",
      "Progress: 125,000,000/125,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 117,048,106 (93.64%)\n",
      "Processing chunk 1251 with 100000 rows...\n",
      "Chunk 1251 processed in 0.02 seconds\n",
      "Removed 95319 duplicates from this chunk\n",
      "Progress: 125,100,000/125,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 117,143,425 (93.64%)\n",
      "Processing chunk 1252 with 100000 rows...\n",
      "Chunk 1252 processed in 0.03 seconds\n",
      "Removed 94495 duplicates from this chunk\n",
      "Progress: 125,200,000/125,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 117,237,920 (93.64%)\n",
      "Processing chunk 1253 with 100000 rows...\n",
      "Chunk 1253 processed in 0.03 seconds\n",
      "Removed 94069 duplicates from this chunk\n",
      "Progress: 125,300,000/125,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 117,331,989 (93.64%)\n",
      "Processing chunk 1254 with 100000 rows...\n",
      "Chunk 1254 processed in 0.02 seconds\n",
      "Removed 95122 duplicates from this chunk\n",
      "Progress: 125,400,000/125,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 117,427,111 (93.64%)\n",
      "Processing chunk 1255 with 100000 rows...\n",
      "Chunk 1255 processed in 0.03 seconds\n",
      "Removed 95042 duplicates from this chunk\n",
      "Progress: 125,500,000/125,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 117,522,153 (93.64%)\n",
      "Processing chunk 1256 with 100000 rows...\n",
      "Chunk 1256 processed in 0.03 seconds\n",
      "Removed 94817 duplicates from this chunk\n",
      "Progress: 125,600,000/125,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 117,616,970 (93.64%)\n",
      "Processing chunk 1257 with 100000 rows...\n",
      "Chunk 1257 processed in 0.02 seconds\n",
      "Removed 95228 duplicates from this chunk\n",
      "Progress: 125,700,000/125,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 117,712,198 (93.65%)\n",
      "Processing chunk 1258 with 100000 rows...\n",
      "Chunk 1258 processed in 0.02 seconds\n",
      "Removed 95355 duplicates from this chunk\n",
      "Progress: 125,800,000/125,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 117,807,553 (93.65%)\n",
      "Processing chunk 1259 with 100000 rows...\n",
      "Chunk 1259 processed in 0.03 seconds\n",
      "Removed 94786 duplicates from this chunk\n",
      "Progress: 125,900,000/125,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 117,902,339 (93.65%)\n",
      "Processing chunk 1260 with 100000 rows...\n",
      "Chunk 1260 processed in 0.03 seconds\n",
      "Removed 94117 duplicates from this chunk\n",
      "Progress: 126,000,000/126,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 117,996,456 (93.65%)\n",
      "Processing chunk 1261 with 100000 rows...\n",
      "Chunk 1261 processed in 0.03 seconds\n",
      "Removed 94740 duplicates from this chunk\n",
      "Progress: 126,100,000/126,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 118,091,196 (93.65%)\n",
      "Processing chunk 1262 with 100000 rows...\n",
      "Chunk 1262 processed in 0.03 seconds\n",
      "Removed 94658 duplicates from this chunk\n",
      "Progress: 126,200,000/126,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 118,185,854 (93.65%)\n",
      "Processing chunk 1263 with 100000 rows...\n",
      "Chunk 1263 processed in 0.03 seconds\n",
      "Removed 94521 duplicates from this chunk\n",
      "Progress: 126,300,000/126,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 118,280,375 (93.65%)\n",
      "Processing chunk 1264 with 100000 rows...\n",
      "Chunk 1264 processed in 0.03 seconds\n",
      "Removed 95013 duplicates from this chunk\n",
      "Progress: 126,400,000/126,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 118,375,388 (93.65%)\n",
      "Processing chunk 1265 with 100000 rows...\n",
      "Chunk 1265 processed in 0.03 seconds\n",
      "Removed 94721 duplicates from this chunk\n",
      "Progress: 126,500,000/126,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 118,470,109 (93.65%)\n",
      "Processing chunk 1266 with 100000 rows...\n",
      "Chunk 1266 processed in 0.02 seconds\n",
      "Removed 95760 duplicates from this chunk\n",
      "Progress: 126,600,000/126,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 118,565,869 (93.65%)\n",
      "Processing chunk 1267 with 100000 rows...\n",
      "Chunk 1267 processed in 0.03 seconds\n",
      "Removed 94554 duplicates from this chunk\n",
      "Progress: 126,700,000/126,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 118,660,423 (93.65%)\n",
      "Processing chunk 1268 with 100000 rows...\n",
      "Chunk 1268 processed in 0.03 seconds\n",
      "Removed 94222 duplicates from this chunk\n",
      "Progress: 126,800,000/126,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 118,754,645 (93.66%)\n",
      "Processing chunk 1269 with 100000 rows...\n",
      "Chunk 1269 processed in 0.03 seconds\n",
      "Removed 94478 duplicates from this chunk\n",
      "Progress: 126,900,000/126,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 118,849,123 (93.66%)\n",
      "Processing chunk 1270 with 100000 rows...\n",
      "Chunk 1270 processed in 0.03 seconds\n",
      "Removed 94951 duplicates from this chunk\n",
      "Progress: 127,000,000/127,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 118,944,074 (93.66%)\n",
      "Processing chunk 1271 with 100000 rows...\n",
      "Chunk 1271 processed in 0.03 seconds\n",
      "Removed 94372 duplicates from this chunk\n",
      "Progress: 127,100,000/127,100,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 119,038,446 (93.66%)\n",
      "Processing chunk 1272 with 100000 rows...\n",
      "Chunk 1272 processed in 0.03 seconds\n",
      "Removed 94551 duplicates from this chunk\n",
      "Progress: 127,200,000/127,200,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 119,132,997 (93.66%)\n",
      "Processing chunk 1273 with 100000 rows...\n",
      "Chunk 1273 processed in 0.03 seconds\n",
      "Removed 94795 duplicates from this chunk\n",
      "Progress: 127,300,000/127,300,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 119,227,792 (93.66%)\n",
      "Processing chunk 1274 with 100000 rows...\n",
      "Chunk 1274 processed in 0.03 seconds\n",
      "Removed 94966 duplicates from this chunk\n",
      "Progress: 127,400,000/127,400,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 119,322,758 (93.66%)\n",
      "Processing chunk 1275 with 100000 rows...\n",
      "Chunk 1275 processed in 0.03 seconds\n",
      "Removed 94691 duplicates from this chunk\n",
      "Progress: 127,500,000/127,500,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 119,417,449 (93.66%)\n",
      "Processing chunk 1276 with 100000 rows...\n",
      "Chunk 1276 processed in 0.02 seconds\n",
      "Removed 96042 duplicates from this chunk\n",
      "Progress: 127,600,000/127,600,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 119,513,491 (93.66%)\n",
      "Processing chunk 1277 with 100000 rows...\n",
      "Chunk 1277 processed in 0.02 seconds\n",
      "Removed 96585 duplicates from this chunk\n",
      "Progress: 127,700,000/127,700,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 119,610,076 (93.66%)\n",
      "Processing chunk 1278 with 100000 rows...\n",
      "Chunk 1278 processed in 0.02 seconds\n",
      "Removed 96773 duplicates from this chunk\n",
      "Progress: 127,800,000/127,800,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 119,706,849 (93.67%)\n",
      "Processing chunk 1279 with 100000 rows...\n",
      "Chunk 1279 processed in 0.03 seconds\n",
      "Removed 94341 duplicates from this chunk\n",
      "Progress: 127,900,000/127,900,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 119,801,190 (93.67%)\n",
      "Processing chunk 1280 with 100000 rows...\n",
      "Chunk 1280 processed in 0.03 seconds\n",
      "Removed 94138 duplicates from this chunk\n",
      "Progress: 128,000,000/128,000,000 rows processed (100.00%)\n",
      "Total duplicates removed so far: 119,895,328 (93.67%)\n",
      "Processing chunk 1281 with 17055 rows...\n",
      "Chunk 1281 processed in 0.00 seconds\n",
      "Removed 16492 duplicates from this chunk\n",
      "Progress: 128,017,055/128,017,055 rows processed (100.00%)\n",
      "Total duplicates removed so far: 119,911,820 (93.67%)\n",
      "\n",
      "Deduplication completed in 626.37 seconds\n",
      "Original file: 93854.55 MB, 128,017,055 rows\n",
      "Deduplicated file: 5991.04 MB, 8,105,235 rows\n",
      "Removed 119,911,820 duplicate rows (93.67%)\n",
      "Reduced file size by 87863.50 MB (93.62%)\n",
      "Deduplicated file saved to: /home/eadlzarabi/Desktop/datasets/Patents/merged_patent_citations_deduplicated.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "\n",
    "# File paths\n",
    "folder = \"/home/eadlzarabi/Desktop/datasets/Patents/\"\n",
    "input_file = folder + 'merged_patent_citations.csv'\n",
    "output_file = folder + 'merged_patent_citations_deduplicated.csv'\n",
    "\n",
    "# Print absolute path for verification\n",
    "print(f\"Will save deduplicated data to: {os.path.abspath(output_file)}\")\n",
    "\n",
    "# Process in smaller chunks to reduce memory usage\n",
    "chunk_size = 100000  # Increased chunk size for faster processing\n",
    "\n",
    "# Function to free memory\n",
    "def free_memory():\n",
    "    gc.collect()\n",
    "\n",
    "print(\"Starting deduplication process based on patent_id...\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Check if input file exists\n",
    "    if not os.path.exists(input_file):\n",
    "        raise FileNotFoundError(f\"Input file not found: {input_file}\")\n",
    "    \n",
    "    # Get total file size for progress tracking\n",
    "    total_size = os.path.getsize(input_file)\n",
    "    print(f\"Input file size: {total_size/1024/1024:.2f} MB\")\n",
    "    \n",
    "    # Read the CSV file in chunks to handle large files\n",
    "    # First pass: Identify all columns for proper deduplication\n",
    "    print(\"Reading file headers...\")\n",
    "    temp_df = pd.read_csv(input_file, nrows=5)\n",
    "    all_columns = temp_df.columns.tolist()\n",
    "    print(f\"Found {len(all_columns)} columns in the file\")\n",
    "    \n",
    "    # Verify patent_id column exists\n",
    "    if 'patent_id' not in all_columns:\n",
    "        raise ValueError(\"Critical column 'patent_id' not found in the file\")\n",
    "    \n",
    "    del temp_df\n",
    "    free_memory()\n",
    "    \n",
    "    # Check if output file exists and remove it\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"Removing existing output file: {output_file}\")\n",
    "        os.remove(output_file)\n",
    "    \n",
    "    # Set up for processing\n",
    "    first_chunk = True\n",
    "    processed_count = 0\n",
    "    total_rows = 0\n",
    "    unique_rows = 0\n",
    "    duplicates_removed = 0\n",
    "    \n",
    "    # Process the file in chunks\n",
    "    print(\"Processing file in chunks...\")\n",
    "    \n",
    "    # For tracking unique patent_ids across chunks\n",
    "    seen_patent_ids = set()\n",
    "    \n",
    "    # Create a reader for the merged file\n",
    "    csv_reader = pd.read_csv(\n",
    "        input_file,\n",
    "        chunksize=chunk_size,\n",
    "        engine='python',\n",
    "        on_bad_lines='skip'\n",
    "    )\n",
    "    \n",
    "    for chunk_number, chunk in enumerate(csv_reader, 1):\n",
    "        try:\n",
    "            start_chunk_time = time.time()\n",
    "            chunk_size = len(chunk)\n",
    "            total_rows += chunk_size\n",
    "            print(f\"Processing chunk {chunk_number} with {chunk_size} rows...\")\n",
    "            \n",
    "            # Fill NaN values in patent_id to avoid deduplication issues\n",
    "            if 'patent_id' in chunk.columns:\n",
    "                chunk['patent_id'] = chunk['patent_id'].fillna('NA')\n",
    "            \n",
    "            # Identify duplicates within this chunk based solely on patent_id\n",
    "            initial_chunk_size = len(chunk)\n",
    "            chunk_duplicates = chunk.duplicated(subset=['patent_id'], keep='first')\n",
    "            chunk = chunk[~chunk_duplicates]\n",
    "            duplicates_in_chunk = initial_chunk_size - len(chunk)\n",
    "            \n",
    "            # Now check against previously seen patent IDs\n",
    "            current_ids = set(chunk['patent_id'].values)\n",
    "            already_seen = current_ids.intersection(seen_patent_ids)\n",
    "            new_ids = current_ids - already_seen\n",
    "            \n",
    "            # Remove rows with already seen patent_ids\n",
    "            if already_seen:\n",
    "                chunk = chunk[chunk['patent_id'].isin(new_ids)]\n",
    "            \n",
    "            # Update the set of seen patent IDs\n",
    "            seen_patent_ids.update(new_ids)\n",
    "            \n",
    "            # Count unique rows\n",
    "            unique_rows_in_chunk = len(chunk)\n",
    "            unique_rows += unique_rows_in_chunk\n",
    "            duplicates_removed += (chunk_size - unique_rows_in_chunk)\n",
    "            \n",
    "            # Write to output file\n",
    "            if first_chunk and not chunk.empty:\n",
    "                chunk.to_csv(output_file, index=False, mode='w')\n",
    "                first_chunk = False\n",
    "            elif not chunk.empty:\n",
    "                chunk.to_csv(output_file, index=False, mode='a', header=False)\n",
    "            \n",
    "            processed_count += chunk_size\n",
    "            chunk_time = time.time() - start_chunk_time\n",
    "            print(f\"Chunk {chunk_number} processed in {chunk_time:.2f} seconds\")\n",
    "            print(f\"Removed {chunk_size - unique_rows_in_chunk} duplicates from this chunk\")\n",
    "            print(f\"Progress: {processed_count:,}/{total_rows:,} rows processed ({processed_count/total_rows*100:.2f}%)\")\n",
    "            print(f\"Total duplicates removed so far: {duplicates_removed:,} ({duplicates_removed/total_rows*100:.2f}%)\")\n",
    "            \n",
    "            # Free memory\n",
    "            del chunk\n",
    "            free_memory()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk {chunk_number}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    \n",
    "    # Final verification\n",
    "    elapsed_time = time.time() - start_time\n",
    "    if os.path.exists(output_file):\n",
    "        final_size = os.path.getsize(output_file)\n",
    "        size_reduction = total_size - final_size\n",
    "        print(f\"\\nDeduplication completed in {elapsed_time:.2f} seconds\")\n",
    "        print(f\"Original file: {total_size/1024/1024:.2f} MB, {total_rows:,} rows\")\n",
    "        print(f\"Deduplicated file: {final_size/1024/1024:.2f} MB, {unique_rows:,} rows\")\n",
    "        print(f\"Removed {duplicates_removed:,} duplicate rows ({duplicates_removed/total_rows*100:.2f}%)\")\n",
    "        print(f\"Reduced file size by {size_reduction/1024/1024:.2f} MB ({size_reduction/total_size*100:.2f}%)\")\n",
    "        print(f\"Deduplicated file saved to: {output_file}\")\n",
    "    else:\n",
    "        print(f\"Error: Output file not created at {output_file}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in processing: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
